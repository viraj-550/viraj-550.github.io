---
title: "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation"
subtitle: "A Comprehensive Study of SPXL ETF and BAC Stock for VaR Insights"
author: "Viraj Chordiya"
date: '2022-11-30'
format:
  html:
    code-fold: show
    code-tools: true
    code-line-numbers: true
    toc: True
execute:
  cache: true
  warning: false
categories: [Financial Econometrics, Portfolio Optimization, Copula, R]
bibliography: ["reference.bib"]
nocite: |
  @*
---

```{r echo=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
```

```{css, echo = FALSE}
.justify {
  text-align: justify !important
}
```

::: justify
# Introduction

In today's volatile financial landscape, effective portfolio management has become an indispensable tool for mitigating investment risks that simply cannot be overlooked. The role of a portfolio manager is paramount, as they navigate this complex terrain with the overarching objective of minimizing risks while maximizing returns for their clients.

The goal of this project is to conduct a comprehensive analysis of two distinct financial instruments: the Direxion Daily S&P 500 Bull 3X Shares (SPXL) Exchange-Traded Fund (ETF) and Bank of America Corp (NYSE: BAC) stock. Our primary goal is to craft an optimal portfolio of these two assets that not only forecasts Value-at-Risk (VaR) but also actively reduces it.

The Direxion Daily S&P 500 Bull 3X Shares ETF, a 300% leveraged vehicle based on the performance of the S&P 500 index, offers 3x amplified returns as compared to the benchmark return on a daily basis. In contrast, Bank of America Corp (BAC), a steadfast and established financial institution, has a history of providing stable returns. The fusion of these two financial assets into an optimal portfolio promises a potential of handsome returns while managing VaR. We collect historical ten year daily OHLC data for both the instruments from Yahoo Finance using `quantmod::getSymbols()` in R.

Subsequently, we perform the following analysis to construct an optimal portfolio:

-   **Exploring the Data**: We conduct a rigorous examination of the data to unveil any time-dependent patterns and potential volatility clustering, that provide insights into the behavior of these financial instruments.
-   **Time Series Modeling**: Employing a $AR(1) + GARCH(1,1)$ model, we seek to model the logarithmic returns of each product, providing us a deeper understanding of their inherent dynamics.
-   **Copula Analysis**: We fit t-distributions to the standardized errors of both models. To study the dependence structure of both the residues, we explore various copulas such as the t-Copula, Gaussian Copula, Gumbel Copula, Clayton Copula, and Frank Copula. The selection of the most suitable copula is based on minimizing the Akaike Information Criterion (AIC).
-   **Residual Analysis**: Rigorous evaluation of the standardized residues of our model is carried out, aiming to detect any potential serial correlation. This step involves the utilization of autocorrelation plots and weighted versions of the Ljung-Box test, ultimately confirming the appropriateness of the $AR(1) + GARCH(1,1)$ model for our dataset.
-   **VaR Forecasting**: Employing advanced numerical methods, we forecast Value-at-Risk for various portfolios comprising of SPXL and BAC. Our findings reveal that VaR is responsive to varying proportions of SPXL within the portfolio. Notably, we uncover that VaR tends to rise as the allocation to SPXL increases.

In essence, our project serves as a comprehensive exploration of the dynamic interplay between these financial assets, offering valuable insights and strategies for optimizing portfolio performance while diligently managing risks in an ever-evolving financial landscape.
:::

```{r warnings = FALSE, include = TRUE}
# Importing Packages

library(quantmod)
library(xts)
library(gridExtra)
library(MASS)
library(fGarch)
library(sn)
library(copula)
library(ks)
library(stargazer)
library(patchwork)
library(rugarch)
library(psych)
library(ggplot2)
library(tseries)
library(modelsummary)
```

------------------------------------------------------------------------

# Data

::: justify
We collect ten years daily OHLC data of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (NYSE: BAC). SPXL is a three times leveraged ETF mirroring the S&P 500 index. Hence, it provides a 300% of the S&P 500 index's daily return. The data is collected from Yahoo finance for period 2012/11 - 2022/11 using the `getSymbol()` function from the `quantmod` library. The table represents the summary statistics of daily adjusted closing stock price, and log returns of both the instruments.
:::

```{r}

# Assigning Date

start <- as.Date("2012-11-09")
end <- as.Date("2022-11-10")

spxl_df <- getSymbols("SPXL", from = start, to = end, src = "yahoo",
                       auto.assign = F)
bac_df <- getSymbols("BAC", from = start, to = end, src = "yahoo",
                       auto.assign = F)


daily_df <- cbind(spxl_df$SPXL.Adjusted, bac_df$BAC.Adjusted)

colnames(daily_df) <- c("SPXL", "BAC")

daily_df$SPXL_r <- diff(log(daily_df$SPXL))
daily_df$BAC_r <- diff(log(daily_df$BAC))

daily_df <- na.omit(daily_df)

```

```{r}

summary_df <- describe(daily_df, ranges = F)
colnames(summary_df) <- c("Variable", "N", "Mean", "Std", "Skew", "Kurtosis", "S.E.")
summary_df$Variable <- c("SPXL", "BAC", "SPXL Returns", "BAC Returns")
datasummary_df(summary_df,
               title = "Table 1. Summary Statistics")
```

::: justify
There are a total of 2517 observations in total. Prices and log returns of both the financial instruments are asymmetric. SPXL daily prices are extremely skewed to the right, while SPXL returns are extremely skewed to the left. Moreover, BAC daily prices are slightly skewed to the right while the returns are slightly skewed to the left. The excess kurtosis for the log returns of both assets show that returns are not normally distributed.
:::

## Time series graphs and distribution plots

```{r}

plot1 <- ggplot(spxl_df, aes(x = Index, y = SPXL.Adjusted))+
  geom_line(color = "steelblue") + 
  ggtitle("SPXL Daily Price") +   
  xlab("Date") + 
  ylab("Price($)") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  scale_x_date(date_labels = "%y-%m", date_breaks = "1 year") +
  theme_bw()

plot2 <- ggplot(bac_df, aes(x = Index, y = BAC.Adjusted))+
  geom_line(color = "darkorange2") + 
  ggtitle("BAC Daily Price") +   
  xlab("Date") + 
  ylab("Price($)") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  scale_x_date(date_labels = "%y-%m", date_breaks = "1 year") + 
  theme_bw()

plot1 / plot2
```

------------------------------------------------------------------------

## Exploratory Data Analysis

::: justify
Firstly, we check for weak stationarity by investigating the time series plots of log returns of both the financial instruments. Next, we conduct the Augmented Dickey Fuller (ADF) test to check for the presence of unit root.
:::

```{r}
# Plotting log returns

plot_spxl_r <- ggplot(daily_df, aes(x = Index, y = SPXL_r))+
  geom_line(color = "steelblue") + 
  ggtitle("SPXL Log Returns") +   
  xlab("Date") + 
  ylab("Log Returns") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  scale_x_date(date_labels = "%y-%m", date_breaks = "1 year") + 
  theme_bw()

plot_bac_r <- ggplot(daily_df, aes(x = Index, y = BAC_r))+
  geom_line(color = "darkorange2") + 
  ggtitle("BAC Log Returns") +   
  xlab("Date") + 
  ylab("Log Returns") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  scale_x_date(date_labels = "%y-%m", date_breaks = "1 year") + 
  theme_bw()

plot_spxl_r / plot_bac_r

```

```{r warning=FALSE}
adf.test(daily_df$SPXL_r)
adf.test(daily_df$BAC_r)
```

::: justify
Upon visual inspection, it looks like both the log returns are stationary. Moreover, from the ADF test, we fail to reject the null hypothesis of non-stationarity at 1% confidence.

In order to look for time dependency and volatility clustering of log returns of SPLX and BAC, From the graphs, it is evident that both series display time varying volatility. Moreover, volatility clusters together. For better analysis, we need to model the nonconstant volatility.
:::

```{r ggplot}
# ACF plots
ggacf <- function(x, ci=0.95, type="correlation", xlab="Lag", ylab=NULL,
                  ylim=NULL, main=NULL, ci.col="blue", lag.max=NULL) {

    x <- as.data.frame(x)

    x.acf <- acf(x, plot=F, lag.max=lag.max, type=type)

    ci.line <- qnorm((1 - ci) / 2) / sqrt(x.acf$n.used)

    d.acf <- data.frame(lag=x.acf$lag, acf=x.acf$acf)

    g <- ggplot(d.acf, aes(x=lag, y=acf)) +
        geom_hline(yintercept=0) +
        geom_segment(aes(xend=lag, yend=0)) +
        geom_hline(yintercept=ci.line, color=ci.col, linetype="dashed") +
        geom_hline(yintercept=-ci.line, color=ci.col, linetype="dashed") +
        theme_bw() +
        xlab("Lag") +
        ggtitle(ifelse(is.null(main), "", main)) +
        if (is.null(ylab))
            ylab(ifelse(type=="partial", "PACF", "ACF"))
        else
            ylab(ylab)

    g
}

spxl_acf <- ggacf(daily_df$SPXL_r, main = "SPXL")
spxl_acf2 <- ggacf(daily_df$SPXL_r ** 2, main = "SPXL Squared")
bac_acf <- ggacf(daily_df$BAC_r, main = "BAC")
bac_acf2 <- ggacf(daily_df$BAC_r ** 2, main = "BAC Squared")

(spxl_acf + spxl_acf2) / (bac_acf + bac_acf2)
```

::: justify
From the log return graphs, it is evident that both series display time varying volatility. Moreover, ACF plots of squared series of both stocks show significant serial correlation. For better analysis, we need to model time dependence and non-constant volatility.
:::

------------------------------------------------------------------------

# Time Series Model

## AR(1)-GARCH(1,1) Model

::: justify
In order to model autocorelation and volatility clustering, we combine an AR(1) model that has a nonconstant conditional mean but a constant conditional variance with a GARCH(1,1) model that has conditional mean and the variance of data depends on the past. Additionally, in a GARCH model, conditional standard deviation exhibits more persistent periods of low and high volatility.

The $AR(1) - GARCH(1,1)$ is model is as follows:

$$ X_t = \mu + \phi X_{t-1} + e_t, \quad e_t =  \sigma_{t}\varepsilon_{t}, \quad \sigma^2_t = \omega + \alpha e^2_{t-1} + \beta \sigma^2_{t-1} $$ $$ \tilde X_t = \tilde \mu + \tilde \phi \tilde X_{t-1} + \tilde e_t, \quad \tilde e_t =  \tilde \sigma_{t}\tilde \varepsilon_{t}, \quad \tilde \sigma^2_t = \tilde \omega + \tilde \alpha \tilde e^2_{t-1} + \tilde \beta \tilde \sigma^2_{t-1} $$

where, $X_t$ is the daily log returns of SPXL and $\tilde X_t$ is the daily log returns of BAC.
:::

```{r , echo = TRUE, eval = TRUE, results="hide"}

spxl <- as.vector(daily_df$SPXL_r)
bac <- as.vector(daily_df$BAC_r)

# Fitting an AR(1) + GARCH(1,1) Model
spxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),
                        variance.model=list(garchOrder=c(1,1)))
spxl_fit <- ugarchfit(spxl_spec, data = spxl)

bac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),
                        variance.model=list(garchOrder=c(1,1)))
bac_fit <- ugarchfit(bac_spec, data = bac)
```

## Residual Copula Modelling

::: justify
In order to work with a portfolio of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (BAC), we need to model both the series as a copula to extract the dependence structure between the two financial instruments. In the project, we particulary study the dependent structure of the standardized residuals of an $AR(1) - GARCH(1,1)$ model fitted to both the series.

First, we fit a Student t distribution to both $\varepsilon_t$ and $\tilde \varepsilon_t$ using the `fitdistr()` function from the `MASS` library.
:::

```{r warning=FALSE}
# Standardized Residuals

spxl_res <- as.vector(residuals(spxl_fit, standardize = T))
bac_res <- as.vector(residuals(bac_fit, standardize = T))

# Fitting t-distribution
spxl_res_t <- fitdistr(spxl_res, "t")
bac_res_t <- fitdistr(bac_res, "t")

```

```{r}
# Fitting t-density plots
par(mfrow = c(1,2))
n <- length(spxl_res)

x1 <- qt((1:n)/(n+1), df = 5)
x2 <- qt((1:n)/(n+1), df = 5)

qqplot(sort(spxl_res), x1, xlab="Standardized Residuals",
       ylab="t-quantiles",
       main = "SPXL Residual t-plot, DF = 5")

qqplot(sort(bac_res), x2, xlab="Standardized Residuals",
       ylab="t-quantiles",
       main = "BAC Residual t-plot, DF = 5")
```

::: justify
A t-distribution with df = 5 fits well to the SPXL and BAC residuals. The qqplot for both the instruments is a straight line except for a few outliers. Given that the total number of observations (n = 2517) is high enough, the outliers are a small fraction of the sample.

Second, we transform the residuals into marginal t - distributions and fit t-copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas. We select the best fit copula by minimizing AIC. We select t-copula as it minimizes AIC.
:::

```{r}

# Transform residues into uniform distribution

spxl_uniform <- pt(spxl_res, df = spxl_res_t$estimate[3])
bac_uniform <- pt(bac_res, df = bac_res_t$estimate[3])

U_hat <- cbind(spxl_uniform, bac_uniform)
colnames(U_hat) <- c("SPXL_U", "BAC_U")

tau <- as.numeric(cor.test(spxl_uniform, bac_uniform, 
                           method="kendall")$estimate)
omega <- sin(tau * pi/2)

```

```{r}
# Fit t-Copula
Ct <- fitCopula(copula = tCopula(dim = 2), data = U_hat,
                method = "ml", start = c(omega, 10))

# Log-Likelihood
log_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, 
                          copula = tCopula(dim = 2))
aic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))
aic_t

# Gaussian Copula
Cgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, 
                    method = "ml", start=c(omega))
log_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, 
                          copula = normalCopula(dim = 2))

aic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))
aic_gauss

# Gumbel Copula
C_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,
                      method = "ml")
log_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,
                               copula = gumbelCopula(dim = 2))
aic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))
aic_gumbel

# Clayton Copula

C_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,
                      method = "ml")
log_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,
                                copula = claytonCopula(dim = 2))
aic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))
aic_clayton

# Frank Copula

Cfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,
                    method = "ml")

log_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,
                              copula = frankCopula(dim = 2))
aic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))
aic_frank
```

------------------------------------------------------------------------

## Residual Analysis

::: justify
We conduct a residual analysis on the standardized residuals $\varepsilon_t$ and $\tilde \varepsilon$ to check the fit of the $AR(1) + GARCH(1,1)$ model.

First, we inspect the standardized residuals and squared standardized residuals for autocorrelation. The figure below plots the `acf()` function for the four series.
:::

```{r}
spxl_res_acf <- ggacf(spxl_res, main = "SPXL Std. Residuals")
spxl_res_acf2 <- ggacf(spxl_res ** 2, main = "Squared SPXL Std. Residuals")
bac_res_acf <- ggacf(bac_res, main = "BAC Std. Residuals")
bac_res_acf2 <- ggacf(bac_res ** 2, main = "Squared BAC Std. Residuals")

(spxl_res_acf + spxl_res_acf2) / (bac_res_acf + bac_res_acf2)
```

::: justify
The AR(1) + GARCH(1,1) model fits very well to SPXL and BAC. The standardized residuals and the squared standardized residuals of both the models show no significant serial correlation at 95% confidence.

Next we look at the following box tests to check autocorrelation
:::

```{r}

show(spxl_fit)

show(bac_fit)
```

```{r}
bac_fit
```

::: justify
The weighted versions of the Ljung-Box test and their p-values all indicate that the estimated $AR(1) + GARCH(1,1)$ model for the conditional mean and variance is adequate for removing serial correlation from the series and squared series.
:::

------------------------------------------------------------------------

# Applications to Portfolio Risk Management

::: justify
In this section, we apply the $AR(1) + GARCH(1,1)$ along with t-Copula residuals to forecast one-step ahead Value-at-Risk at 99% level on the following portfolio consisting of SPXL and BAC with weight $\rho$:

$$\rho X_{n + 1} + (1-\rho) \tilde X_{n+1}$$

where, $\rho = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}$

To forecast VaR, we solve the following equation:

$$0.99 = P(\rho X_{n + 1} + (1-\rho) \tilde X_{n+1} \leq x | \mathcal{F})$$ $$0.99 = P(\rho(\mu + \phi X_{n} + \sigma_{n+1}\varepsilon_{n+1}) + (1-\rho) (\tilde\mu + \tilde\phi \tilde X_{n} + \tilde \sigma_{n+1} \tilde \varepsilon_{n+1}) \leq x | \mathcal{F})$$

Initially, we forecast one step ahead $\sigma_{n+1}$ and $\tilde \sigma_{n + 1}$ using the `ugarchforecast()` function. At this point, we have estimated all the unknowns in the $AR(1) + GARCH(1,1)$ model. In order to solve the above equation numerically, we draw a random sample of size $b = 10000$ from the fitted t-Copula and make transformations to get $\varepsilon_i$ and $\tilde \varepsilon_i$, $i = 1, \dots, b$ Lastly, we calculate VaR at 99% by calculating the 99th quantile of the following sample

$$\big{\{}\rho(\mu + \phi X_{n} + \sigma_{n+1}\varepsilon_{i}) + (1-\rho) (\tilde\mu + \tilde\phi \tilde X_{n} + \tilde \sigma_{n+1} \tilde \varepsilon_{i})\big{\}}_{i = 1}^b$$ In order to see how risk depends on share of portfolio $\rho$, we conduct the empirical activity for $\rho = 0.1, \dots, 0.9$.
:::

```{r}
# Copula Simulation 

b <- 10000
rho = Ct@estimate[1]
df = Ct@estimate[2]
simulate <- rCopula(b, tCopula(dim = 2, rho, df = df))
spxl_marginal <- qt(simulate[, 1], df = spxl_res_t$estimate[3])
bac_marginal <- qt(simulate[, 2], df = bac_res_t$estimate[3])

# dataframe with transformed marginals
sim <- cbind(spxl_marginal, bac_marginal)

# Forecasting sigmas

spxl_pred <- ugarchforecast(spxl_fit, n.ahead = 1)
bac_pred <- ugarchforecast(bac_fit, n.ahead = 1)
spxl_sigma <- sigma(spxl_pred) # sigma_{t + 1}
bac_sigma <- sigma(bac_pred) # sigma_{t + 1}

# Vectorize parameters
s_mu <- rep(as.numeric(spxl_fit@fit$coef[1]), b)
s_ar <- rep(as.numeric(spxl_fit@fit$coef[2]), b)
s_last <- rep(spxl[length(spxl)], b)
s_sd <- rep(spxl_sigma, b)

b_mu <- rep(as.numeric(bac_fit@fit$coef[1]), b)
b_ar <- rep(as.numeric(bac_fit@fit$coef[2]), b)
b_last <- rep(bac[length(bac)], b)
b_sd <- rep(bac_sigma, b)


s <- s_mu + (s_ar * s_last) + (s_sd * sim[, 1])
ba <- b_mu + (b_ar * b_last) + (b_sd * sim[, 2])
pred_sample <- cbind(s, ba)
rhos <- seq(0.1, 0.9, by = 0.1)
var_sample <- data.frame()

alpha <-  0.01 # 99th percentile

portfolio_value <- 1000000

for (i in rhos){
  
  s_rho <- s * rep(i, b)
  b_rho <- ba * rep(1 - i, b)
  
  sample <-  s_rho + b_rho
  q <- as.numeric(quantile(sample, alpha))
  VaR <- - portfolio_value * q
  var_sample <- rbind(var_sample, VaR)
}

var_sample$`SPXL Portfolio Weight` <- c("p = 0.1", "p = 0.2", "p = 0.3", "p = 0.4", 
                           "p = 0.5", "p = 0.6", "p = 0.7", "p = 0.8", 
                           "p = 0.9")



colnames(var_sample) <- c("VaR ($)", "SPXL Portfolio Weight")

# Reordering columns
var_sample <- var_sample[, c("SPXL Portfolio Weight", "VaR ($)")]

datasummary_df(var_sample,
               title = "Table 2. One-Step Ahead VaR forecast for a $1 million portfolio with varying weights")
```

::: justify
The table reports the Value-at-Risk at 99% level for a portfolio value of \$1,000,000. It is evident from the table that as SPXL's proportion in the portfolio increases ($\rho$), the Value-at-Risk increases. These results are in line with expectations as Direxion Daily S&P 500 Bull 3X Shares is a 300% leveraged instrument based on the S&P 500 index. When $\rho = 0.2$, there is a 1% percent chance that the loss would be greater than `r as.numeric(var_sample[2,1])` the next day on a \$1,000,000 investment. Similarly, When $\rho = 0.9$, there is a 1% percent chance that the loss would be greater than `r as.numeric(var_sample[9,1])` the next day on a \$1,000,000 investment.
:::

# Reference