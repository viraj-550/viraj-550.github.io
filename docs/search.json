[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Viraj R. Chordiya",
    "section": "",
    "text": "I’m a senior-year honors student at Georgia State University, majoring in economics and mathematics with a deep interest in financial economics. I love working with numbers, and I am intrigued by the linkages between financial markets and global macro variables. Through my math and graduate-level economics courses, I have developed solid quantitative and programming skills.\nPreviously, I have worked as a research assistant in the Economics department at GSU, enhancing my data analysis and research skills. I was also associated with the Portfolio Management Team (PMT) at GSU as the Chief Economist. The PMT is a student-managed investment fund with an objective to outperform the S&P 500 Index on a risk-adjusted basis. This position gave me hands-on experience in analyzing macroeconomic trends and reporting them to other members."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/garch-var/index.html",
    "href": "posts/garch-var/index.html",
    "title": "Financial Econometrics Final Project",
    "section": "",
    "text": "In todays volatile world, portfolio management is crucial as it helps reduce the investment strategy risk to the extent that cannot be ignored. The goal of a portfolio manager is to reduce risks and maximize returns for their clients. The aim of this project is to analyse the Direxion Daily S&P 500 Bull 3X Shares (SPXL) ETF and Bank of America Corp (NYSE: BAC) stock and study an optimal portfolio to forecast Value-at-Risk. Direxion Daily S&P 500 Bull 3X Shares is a 300% leveraged ETF derived from the S&P 500 index and Bank of America is an evergreen stock generating steady returns. An optimum portfolio mix of these two financial assets can reduce VaR and provide handsome returns. We collect previous 10 year daily OHLC data for both the instruments from Yahoo Finance using quantmod::getSymbols() in R.\nTo this end, we perform the following analyses:\n\nExplore the data to check for time dependence and volatility clustering.\nBuild a time series model, specifically, \\(AR(1) + GARCH(1,1)\\) to model each product’s log returns.\nFit t-distributions to the standardized errors of both models. In order to study the dependence structure of both the residues, we fit several copulas like t - Copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas and choose the best one by minimizing AIC.\nWe conduct a residual analysis on the standardized residues of the model to check for any serial correlation. Using autocorrelation plots and weighted versions of Ljung-Box test, we conclude that the \\(AR(1) + GARCH(1,1)\\) model is a good fit for the data.\nLastly, we utilize numerical methods to conduct Value-at-Risk forecasting of a portfolio mix of SPXL and BAC. We find that VaR increases as SPXL’s share in the portfolio increases."
  },
  {
    "objectID": "posts/garch-var/index.html#data",
    "href": "posts/garch-var/index.html#data",
    "title": "Financial Econometrics Final Project",
    "section": "Data",
    "text": "Data\nWe collect ten years daily OHLC data of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (NYSE: BAC). SPXL is a three times leveraged ETF mirroring the S&P 500 index. Hence, it provides a 300% of the S&P 500 index’s daily return. The data is collected from Yahoo finance for period 2012/11 - 2022/11 using the getSymbol() function from the quantmod library. The table represents the summary statistics of daily adjusted closing stock price, and log returns of both the instruments.\n\n\n\nTable 1. Summary Statistics\n\n\n\n\n\n\n\n\n\nvars\n\n\nn\n\n\nmean\n\n\nsd\n\n\nskew\n\n\nkurtosis\n\n\nse\n\n\n\n\n\n\n\n\nSPXL\n\n\n1\n\n\n2,517\n\n\n42.899\n\n\n31.554\n\n\n1.240\n\n\n0.854\n\n\n0.629\n\n\n\n\nBAC\n\n\n2\n\n\n2,517\n\n\n22.488\n\n\n9.675\n\n\n0.591\n\n\n-0.556\n\n\n0.193\n\n\n\n\nSPXL_r\n\n\n3\n\n\n2,517\n\n\n0.001\n\n\n0.033\n\n\n-1.525\n\n\n20.383\n\n\n0.001\n\n\n\n\nBAC_r\n\n\n4\n\n\n2,517\n\n\n0.001\n\n\n0.019\n\n\n-0.070\n\n\n9.849\n\n\n0.0004\n\n\n\n\n\n\n\n\nThere are a total of 2517 observations in total. Prices and log returns of both the financial instruments are asymmetric. SPXL daily prices are extremely skewed to the right, while SPXL returns are extremely skewed to the left. Moreover, BAC daily prices are slightly skewed to the right while the returns are slightly skewed to the left. The excess kurtosis for the log returns of both assets show that returns are not normally distributed.\n\nTime series graphs and distribution plots"
  },
  {
    "objectID": "posts/garch-var/index.html#exploratory-data-analysis",
    "href": "posts/garch-var/index.html#exploratory-data-analysis",
    "title": "Financial Econometrics Final Project",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirstly, we check for weak stationarity by investigating the time series plots of log returns of both the financial instruments. Next, we conduct the Augmented Dickey Fuller (ADF) test to check for the presence of unit root.\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$SPXL_r\nDickey-Fuller = -13.314, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$BAC_r\nDickey-Fuller = -12.962, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nUpon visual inspection, it looks like both the log returns are stationary. Moreover, from the ADF test, we fail to reject the null hypothesis of non-stationarity at 1% confidence.\nIn order to look for time dependency and volatility clustering of log returns of SPLX and BAC, From the graphs, it is evident that both series display time varying volatility. Moreover, volatility clusters together. For better analysis, we need to model the nonconstant volatility.\n\n\n\n\n\nFrom the log return graphs, it is evident that both series display time varying volatility. Moreover, ACF plots of squared series of both stocks show significant serial correlation. For better analysis, we need to model time dependence and non-constant volatility."
  },
  {
    "objectID": "posts/garch-var/index.html#time-series-model",
    "href": "posts/garch-var/index.html#time-series-model",
    "title": "Financial Econometrics Final Project",
    "section": "Time Series Model",
    "text": "Time Series Model\n\nAR(1)-GARCH(1,1) Model\nIn order to model autocorelation and volatility clustering, we combine an AR(1) model that has a nonconstant conditional mean but a constant conditional variance with a GARCH(1,1) model that has conditional mean and the variance of data depends on the past. Additionally, in a GARCH model, conditional standard deviation exhibits more persistent periods of low and high volatility.\nThe \\(AR(1) - GARCH(1,1)\\) is model is as follows:\n\\[ X_t = \\mu + \\phi X_{t-1} + e_t, \\quad e_t =  \\sigma_{t}\\varepsilon_{t}, \\quad \\sigma^2_t = \\omega + \\alpha e^2_{t-1} + \\beta \\sigma^2_{t-1} \\] \\[ \\tilde X_t = \\tilde \\mu + \\tilde \\phi \\tilde X_{t-1} + \\tilde e_t, \\quad \\tilde e_t =  \\tilde \\sigma_{t}\\tilde \\varepsilon_{t}, \\quad \\tilde \\sigma^2_t = \\tilde \\omega + \\tilde \\alpha \\tilde e^2_{t-1} + \\tilde \\beta \\tilde \\sigma^2_{t-1} \\] where, \\(X_t\\) is the daily log returns of SPXL and \\(\\tilde X_t\\) is the daily log returns of BAC.\n\nspxl <- as.vector(daily_df$SPXL_r)\nbac <- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit <- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nbac_fit <- ugarchfit(bac_spec, data = bac)\n\n\n\nResidual Copula Modelling\nIn order to work with a portfolio of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (BAC), we need to model both the series as a copula to extract the dependence structure between the two financial instruments. In the project, we particulary study the dependent structure of the standardized residuals of an \\(AR(1) - GARCH(1,1)\\) model fitted to both the series.\nFirst, we fit a Student t distribution to both \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon_t\\) using the fitdistr() function from the MASS library.\n\n\n\n\n\n\n\n\nA t-distribution with df = 5 fits well to the SPXL and BAC residuals. The qqplot for both the instruments is a straight line except for a few outliers. Given that the total number of observations (n = 2517) is high enough, the outliers are a small fraction of the sample.\nSecond, we transform the residuals into marginal t - distributions and fit t-copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas. We select the best fit copula by minimizing AIC. We select t-copula as it minimizes AIC.\n\n\n\n\n# Fit t-Copula\nCt <- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n[1] -1516.139\n\n# Gaussian Copula\nCgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                          copula = normalCopula(dim = 2))\n\naic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n[1] -1411.647\n\n# Gumbel Copula\nC_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n[1] -1388.313\n\n# Clayton Copula\n\nC_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                      method = \"ml\")\nlog_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n[1] -1213.367\n\n# Frank Copula\n\nCfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\n[1] -1395.35"
  },
  {
    "objectID": "posts/garch-var/index.html#residual-analysis",
    "href": "posts/garch-var/index.html#residual-analysis",
    "title": "Financial Econometrics Final Project",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nWe conduct a residual analysis on the standardized residuals \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon\\) to check the fit of the \\(AR(1) + GARCH(1,1)\\) model.\nFirst, we inspect the standardized residuals and squared standardized residuals for autocorrelation. The figure below plots the acf() function for the four series.\n\n\n\n\n\nThe AR(1) + GARCH(1,1) model fits very well to SPXL and BAC. The standardized residuals and the squared standardized residuals of both the models show no significant serial correlation at 95% confidence.\nNext we look at the following box tests to check autocorrelation\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.002425    0.000386   6.2772 0.000000\nar1    -0.057589    0.022519  -2.5574 0.010546\nomega   0.000039    0.000005   7.2771 0.000000\nalpha1  0.234802    0.023531   9.9784 0.000000\nbeta1   0.735776    0.021292  34.5560 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.002425    0.000393   6.1642 0.000000\nar1    -0.057589    0.020538  -2.8040 0.005047\nomega   0.000039    0.000008   4.7664 0.000002\nalpha1  0.234802    0.035228   6.6651 0.000000\nbeta1   0.735776    0.030381  24.2180 0.000000\n\nLogLikelihood : 5725.413 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -4.5454\nBayes        -4.5338\nShibata      -4.5454\nHannan-Quinn -4.5412\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      2.049  0.1523\nLag[2*(p+q)+(p+q)-1][2]     2.124  0.1746\nLag[4*(p+q)+(p+q)-1][5]     2.964  0.4404\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.00449  0.9466\nLag[2*(p+q)+(p+q)-1][5]   0.75536  0.9121\nLag[4*(p+q)+(p+q)-1][9]   2.39145  0.8535\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3] 0.0006994 0.500 2.000  0.9789\nARCH Lag[5] 1.8924772 1.440 1.667  0.4955\nARCH Lag[7] 2.9168559 2.315 1.543  0.5294\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.4922\nIndividual Statistics:              \nmu     0.06813\nar1    0.03829\nomega  0.25543\nalpha1 0.74932\nbeta1  0.80370\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                     t-value      prob sig\nSign Bias           3.751292 0.0001799 ***\nNegative Sign Bias  0.946948 0.3437561    \nPositive Sign Bias  0.004954 0.9960476    \nJoint Effect       20.723582 0.0001201 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     132.0    6.129e-19\n2    30     157.1    1.510e-19\n3    40     178.8    8.634e-20\n4    50     187.7    4.062e-18\n\n\nElapsed time : 0.3075819 \n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000323   3.2023 0.001363\nar1     0.032520    0.021631   1.5034 0.132731\nomega   0.000018    0.000004   4.2020 0.000026\nalpha1  0.105469    0.015992   6.5953 0.000000\nbeta1   0.837323    0.026364  31.7607 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000313   3.3035 0.000955\nar1     0.032520    0.020844   1.5602 0.118709\nomega   0.000018    0.000007   2.5283 0.011462\nalpha1  0.105469    0.033066   3.1896 0.001425\nbeta1   0.837323    0.048064  17.4210 0.000000\n\nLogLikelihood : 6693.438 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.3146\nBayes        -5.3030\nShibata      -5.3146\nHannan-Quinn -5.3104\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.02462  0.8753\nLag[2*(p+q)+(p+q)-1][2]   0.56004  0.9483\nLag[4*(p+q)+(p+q)-1][5]   1.75862  0.7794\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.550  0.2131\nLag[2*(p+q)+(p+q)-1][5]     2.058  0.6047\nLag[4*(p+q)+(p+q)-1][9]     3.474  0.6789\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]   0.06871 0.500 2.000  0.7932\nARCH Lag[5]   1.27764 1.440 1.667  0.6527\nARCH Lag[7]   2.21876 2.315 1.543  0.6710\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  0.8988\nIndividual Statistics:              \nmu     0.02392\nar1    0.02143\nomega  0.24815\nalpha1 0.57061\nbeta1  0.34991\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           0.3416 0.7327015    \nNegative Sign Bias  3.0886 0.0020333 ***\nPositive Sign Bias  0.5373 0.5910849    \nJoint Effect       17.9995 0.0004399 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     77.00    6.080e-09\n2    30     89.57    4.243e-08\n3    40    105.92    4.275e-08\n4    50    114.09    4.159e-07\n\n\nElapsed time : 0.438179 \n\n\nThe weighted versions of the Ljung-Box test and their p-values all indicate that the estimated \\(AR(1) + GARCH(1,1)\\) model for the conditional mean and variance is adequate for removing serial correlation from the series and squared series."
  },
  {
    "objectID": "posts/garch-var/index.html#applications-to-portfolio-risk-management",
    "href": "posts/garch-var/index.html#applications-to-portfolio-risk-management",
    "title": "Financial Econometrics Final Project",
    "section": "Applications to Portfolio Risk Management",
    "text": "Applications to Portfolio Risk Management\nIn this section, we apply the \\(AR(1) + GARCH(1,1)\\) along with t-Copula residuals to forecast one-step ahead Value-at-Risk at 99% level on the following portfolio consisting of SPXL and BAC with weight \\(\\rho\\):\n\\[\\rho X_{n + 1} + (1-\\rho) \\tilde X_{n+1}\\]\nwhere, \\(\\rho = \\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\\}\\)\nTo forecast VaR, we solve the following equation:\n\\[0.99 = P(\\rho X_{n + 1} + (1-\\rho) \\tilde X_{n+1} \\leq x | \\mathcal{F})\\] \\[0.99 = P(\\rho(\\mu + \\phi X_{n} + \\sigma_{n+1}\\varepsilon_{n+1}) + (1-\\rho) (\\tilde\\mu + \\tilde\\phi \\tilde X_{n} + \\tilde \\sigma_{n+1} \\tilde \\varepsilon_{n+1}) \\leq x | \\mathcal{F})\\]\nInitially, we forecast one step ahead \\(\\sigma_{n+1}\\) and \\(\\tilde \\sigma_{n + 1}\\) using the ugarchforecast() function. At this point, we have estimated all the unknowns in the \\(AR(1) + GARCH(1,1)\\) model. In order to solve the above equation numerically, we draw a random sample of size \\(b = 10000\\) from the fitted t-Copula and make transformations to get \\(\\varepsilon_i\\) and \\(\\tilde \\varepsilon_i\\), \\(i = 1, \\dots, b\\) Lastly, we calculate VaR at 99% by calculating the 99th quantile of the following sample\n\\[\\big{\\{}\\rho(\\mu + \\phi X_{n} + \\sigma_{n+1}\\varepsilon_{i}) + (1-\\rho) (\\tilde\\mu + \\tilde\\phi \\tilde X_{n} + \\tilde \\sigma_{n+1} \\tilde \\varepsilon_{i})\\big{\\}}_{i = 1}^b\\] In order to see how risk depends on share of portfolio \\(\\rho\\), we conduct the empirical activity for \\(\\rho = 0.1, \\dots, 0.9\\).\n\n# Copula Simulation \n\nb <- 10000\nrho = Ct@estimate[1]\ndf = Ct@estimate[2]\nsimulate <- rCopula(b, tCopula(dim = 2, rho, df = df))\nspxl_marginal <- qt(simulate[, 1], df = spxl_res_t$estimate[3])\nbac_marginal <- qt(simulate[, 2], df = bac_res_t$estimate[3])\n\n# dataframe with transformed marginals\nsim <- cbind(spxl_marginal, bac_marginal)\n\n# Forecasting sigmas\n\nspxl_pred <- ugarchforecast(spxl_fit, n.ahead = 1)\nbac_pred <- ugarchforecast(bac_fit, n.ahead = 1)\nspxl_sigma <- sigma(spxl_pred) # sigma_{t + 1}\nbac_sigma <- sigma(bac_pred) # sigma_{t + 1}\n\n# Vectorize parameters\ns_mu <- rep(as.numeric(spxl_fit@fit$coef[1]), b)\ns_ar <- rep(as.numeric(spxl_fit@fit$coef[2]), b)\ns_last <- rep(spxl[length(spxl)], b)\ns_sd <- rep(spxl_sigma, b)\n\nb_mu <- rep(as.numeric(bac_fit@fit$coef[1]), b)\nb_ar <- rep(as.numeric(bac_fit@fit$coef[2]), b)\nb_last <- rep(bac[length(bac)], b)\nb_sd <- rep(bac_sigma, b)\n\n\ns <- s_mu + (s_ar * s_last) + (s_sd * sim[, 1])\nba <- b_mu + (b_ar * b_last) + (b_sd * sim[, 2])\npred_sample <- cbind(s, ba)\nrhos <- seq(0.1, 0.9, by = 0.1)\nvar_sample <- data.frame()\n\nalpha <-  0.01 # 99th percentile\n\nportfolio_value <- 1000000\n\nfor (i in rhos){\n  \n  s_rho <- s * rep(i, b)\n  b_rho <- ba * rep(1 - i, b)\n  \n  sample <-  s_rho + b_rho\n  q <- as.numeric(quantile(sample, alpha))\n  VaR <- - portfolio_value * q\n  var_sample <- rbind(var_sample, VaR)\n}\ncolnames(var_sample) <- c(\"VaR ($)\")\nrow.names(var_sample) <- c(\"p = 0.1\", \"p = 0.2\", \"p = 0.3\", \"p = 0.4\", \n                           \"p = 0.5\", \"p = 0.6\", \"p = 0.7\", \"p = 0.8\", \n                           \"p = 0.9\")\nvar_sample\n\n          VaR ($)\np = 0.1  56868.64\np = 0.2  64760.15\np = 0.3  71810.30\np = 0.4  80322.54\np = 0.5  89308.00\np = 0.6  98992.82\np = 0.7 108932.23\np = 0.8 118802.90\np = 0.9 128930.28\n\n\nThe table reports the Value-at-Risk at 99% level for a portfolio value of $1,000,000. It is evident from the table that as SPXL’s proportion in the portfolio increases (\\(\\rho\\)), the Value-at-Risk increases. These results are in line with expectations as Direxion Daily S&P 500 Bull 3X Shares is a 300% leveraged instrument based on the S&P 500 index. When \\(\\rho = 0.2\\), there is a 1% percent chance that the loss would be greater than 6.4760151^{4} the next day on a $1,000,000 investment. Similarly, When \\(\\rho = 0.9\\), there is a 1% percent chance that the loss would be greater than 1.2893028^{5} the next day on a $1,000,000 investment."
  },
  {
    "objectID": "posts/garch-var/index.html#appendix",
    "href": "posts/garch-var/index.html#appendix",
    "title": "Financial Econometrics Final Project",
    "section": "Appendix",
    "text": "Appendix\nR code:\n\n# Importing Packages\n\nlibrary(quantmod)\nlibrary(xts)\nlibrary(gridExtra)\nlibrary(MASS)\nlibrary(fGarch)\nlibrary(sn)\nlibrary(copula)\nlibrary(ks)\nlibrary(stargazer)\nlibrary(patchwork)\nlibrary(rugarch)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(tseries)\n\n\n# Assigning Date\n\nstart <- as.Date(\"2012-11-09\")\nend <- as.Date(\"2022-11-10\")\n\nspxl_df <- getSymbols(\"SPXL\", from = start, to = end, src = \"yahoo\",\n                      auto.assign = F)\nbac_df <- getSymbols(\"BAC\", from = start, to = end, src = \"yahoo\",\n                     auto.assign = F)\n\n\ndaily_df <- cbind(spxl_df$SPXL.Adjusted, bac_df$BAC.Adjusted)\n\ncolnames(daily_df) <- c(\"SPXL\", \"BAC\")\n\ndaily_df$SPXL_r <- diff(log(daily_df$SPXL))\ndaily_df$BAC_r <- diff(log(daily_df$BAC))\n\ndaily_df <- na.omit(daily_df)\n\nstargazer(describe(daily_df, ranges = F), summary = F, type = \"html\", title = \"Table 1. Summary Statistics\")\nplot1 <- ggplot(spxl_df, aes(x = Index, y = SPXL.Adjusted))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot2 <- ggplot(bac_df, aes(x = Index, y = BAC.Adjusted))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\nplot1 / plot2\n\n# Plotting log returns\n\nplot_spxl_r <- ggplot(daily_df, aes(x = Index, y = SPXL_r))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot_bac_r <- ggplot(daily_df, aes(x = Index, y = BAC_r))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot_spxl_r / plot_bac_r\n\nadf.test(daily_df$SPXL_r)\nadf.test(daily_df$BAC_r)\n\n# ACF plots\nggacf <- function(x, ci=0.95, type=\"correlation\", xlab=\"Lag\", ylab=NULL,\n                  ylim=NULL, main=NULL, ci.col=\"blue\", lag.max=NULL) {\n  \n  x <- as.data.frame(x)\n  \n  x.acf <- acf(x, plot=F, lag.max=lag.max, type=type)\n  \n  ci.line <- qnorm((1 - ci) / 2) / sqrt(x.acf$n.used)\n  \n  d.acf <- data.frame(lag=x.acf$lag, acf=x.acf$acf)\n  \n  g <- ggplot(d.acf, aes(x=lag, y=acf)) +\n    geom_hline(yintercept=0) +\n    geom_segment(aes(xend=lag, yend=0)) +\n    geom_hline(yintercept=ci.line, color=ci.col, linetype=\"dashed\") +\n    geom_hline(yintercept=-ci.line, color=ci.col, linetype=\"dashed\") +\n    theme_bw() +\n    xlab(\"Lag\") +\n    ggtitle(ifelse(is.null(main), \"\", main)) +\n    if (is.null(ylab))\n      ylab(ifelse(type==\"partial\", \"PACF\", \"ACF\"))\n  else\n    ylab(ylab)\n  \n  g\n}\n\nspxl_acf <- ggacf(daily_df$SPXL_r, main = \"SPXL\")\nspxl_acf2 <- ggacf(daily_df$SPXL_r ** 2, main = \"SPXL Squared\")\nbac_acf <- ggacf(daily_df$BAC_r, main = \"BAC\")\nbac_acf2 <- ggacf(daily_df$BAC_r ** 2, main = \"BAC Squared\")\n\n(spxl_acf + spxl_acf2) / (bac_acf + bac_acf2)\n\nspxl <- as.vector(daily_df$SPXL_r)\nbac <- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit <- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                       variance.model=list(garchOrder=c(1,1)))\nbac_fit <- ugarchfit(bac_spec, data = bac)\n\n# Standardized Residuals\n\nspxl_res <- as.vector(residuals(spxl_fit, standardize = T))\nbac_res <- as.vector(residuals(bac_fit, standardize = T))\n\n# Fitting t-distribution\nspxl_res_t <- fitdistr(spxl_res, \"t\")\nbac_res_t <- fitdistr(bac_res, \"t\")\n\n# Fitting t-density plots\npar(mfrow = c(1,2))\nn <- length(spxl_res)\n\nx1 <- qt((1:n)/(n+1), df = 5)\nx2 <- qt((1:n)/(n+1), df = 5)\n\nqqplot(sort(spxl_res), x1, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"SPXL Residual t-plot, DF = 5\")\n\nqqplot(sort(bac_res), x2, xlab=\"Standardized Residuals\",\n       ylab=\"tquantiles\",\n       main = \"BAC Residual t-plot, DF = 5\")\n\n# Transform residues into uniform distribution\n\nspxl_uniform <- pt(spxl_res, df = spxl_res_t$estimate[3])\nbac_uniform <- pt(bac_res, df = bac_res_t$estimate[3])\n\nU_hat <- cbind(spxl_uniform, bac_uniform)\ncolnames(U_hat) <- c(\"SPXL_U\", \"BAC_U\")\n\ntau <- as.numeric(cor.test(spxl_uniform, bac_uniform, \n                           method=\"kendall\")$estimate)\nomega <- sin(tau * pi/2)\n\n\n# Fit t-Copula\nCt <- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n# Gaussian Copula\nCgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                              copula = normalCopula(dim = 2))\n\naic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n# Gumbel Copula\nC_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n# Clayton Copula\n\nC_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                       method = \"ml\")\nlog_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n# Frank Copula\n\nCfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\nspxl_res_acf <- ggacf(spxl_res, main = \"SPXL Std. Residuals\")\nspxl_res_acf2 <- ggacf(spxl_res ** 2, main = \"Squared SPXL Std. Residuals\")\nbac_res_acf <- ggacf(bac_res, main = \"BAC Std. Residuals\")\nbac_res_acf2 <- ggacf(bac_res ** 2, main = \"Squared BAC Std. Residuals\")\n\n(spxl_res_acf + spxl_res_acf2) / (bac_res_acf + bac_res_acf2)\n\nshow(spxl_fit)\n\nshow(bac_fit)\n\n# Copula Simulation \n\nb <- 10000\nrho = Ct@estimate[1]\ndf = Ct@estimate[2]\nsimulate <- rCopula(b, tCopula(dim = 2, rho, df = df))\nspxl_marginal <- qt(simulate[, 1], df = spxl_res_t$estimate[3])\nbac_marginal <- qt(simulate[, 2], df = bac_res_t$estimate[3])\n\n# dataframe with transformed marginals\nsim <- cbind(spxl_marginal, bac_marginal)\n\n# Forecasting sigmas\n\nspxl_pred <- ugarchforecast(spxl_fit, n.ahead = 1)\nbac_pred <- ugarchforecast(bac_fit, n.ahead = 1)\nspxl_sigma <- sigma(spxl_pred) # sigma_{t + 1}\nbac_sigma <- sigma(bac_pred) # sigma_{t + 1}\n\n# Vectorize parameters\ns_mu <- rep(as.numeric(spxl_fit@fit$coef[1]), b)\ns_ar <- rep(as.numeric(spxl_fit@fit$coef[2]), b)\ns_last <- rep(spxl[length(spxl)], b)\ns_sd <- rep(spxl_sigma, b)\n\nb_mu <- rep(as.numeric(bac_fit@fit$coef[1]), b)\nb_ar <- rep(as.numeric(bac_fit@fit$coef[2]), b)\nb_last <- rep(bac[length(bac)], b)\nb_sd <- rep(bac_sigma, b)\n\n\ns <- s_mu + (s_ar * s_last) + (s_sd * sim[, 1])\nba <- b_mu + (b_ar * b_last) + (b_sd * sim[, 2])\npred_sample <- cbind(s, ba)\nrhos <- seq(0.1, 0.9, by = 0.1)\nvar_sample <- data.frame()\n\nalpha <-  0.01 # 99th percentile\n\nportfolio_value <- 1000000\n\nfor (i in rhos){\n  \n  s_rho <- s * rep(i, b)\n  b_rho <- ba * rep(1 - i, b)\n  \n  sample <-  s_rho + b_rho\n  q <- as.numeric(quantile(sample, alpha))\n  VaR <- - portfolio_value * q\n  var_sample <- rbind(var_sample, VaR)\n}\ncolnames(var_sample) <- c(\"VaR ($)\")\nrow.names(var_sample) <- c(\"p = 0.1\", \"p = 0.2\", \"p = 0.3\", \"p = 0.4\", \n                           \"p = 0.5\", \"p = 0.6\", \"p = 0.7\", \"p = 0.8\", \n                           \"p = 0.9\")\nvar_sample"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a senior-year honors student at Georgia State University, majoring in economics and mathematics with a deep interest in financial economics. I love working with numbers, and I am intrigued by the linkages between financial markets and global macro variables. Through my math and graduate-level economics courses, I have developed solid quantitative and programming skills.\nPreviously, I have worked as a research assistant in the Economics department at GSU, enhancing my data analysis and research skills. I was also associated with the Portfolio Management Team (PMT) at GSU as the Chief Economist. The PMT is a student-managed investment fund with an objective to outperform the S&P 500 Index on a risk-adjusted basis. This position gave me hands-on experience in analyzing macroeconomic trends and reporting them to other members."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\n\nApr 8, 2023\n\n\nHarlow Malloc\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nTristan O’Malley\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Econometrics Final Project\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nViraj Chordiya\n\n\n21 min\n\n\n\n\n\n\nNo matching items"
  }
]