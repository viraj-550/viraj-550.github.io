[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nApr 8, 2023\n\n\nHarlow Malloc\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nTristan O’Malley\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html#recent-research",
    "href": "index.html#recent-research",
    "title": "Viraj R. Chordiya",
    "section": "Recent Research",
    "text": "Recent Research"
  },
  {
    "objectID": "index_backup.html#recent-research",
    "href": "index_backup.html#recent-research",
    "title": "Viraj R. Chordiya",
    "section": "Recent Research",
    "text": "Recent Research\n\n\n\n\n\n\n\n\n\n\nThin Markets, Thick Premiums\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nView all research →"
  },
  {
    "objectID": "index_backup.html#recent-research-1",
    "href": "index_backup.html#recent-research-1",
    "title": "Viraj R. Chordiya",
    "section": "Recent Research",
    "text": "Recent Research\n\n\n\n\nView all research →"
  },
  {
    "objectID": "projects/smartcard/index.html#program-theory-and-impact-theory-graph",
    "href": "projects/smartcard/index.html#program-theory-and-impact-theory-graph",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Program Theory and Impact Theory Graph",
    "text": "Program Theory and Impact Theory Graph\n\nThe program theory is that biometric authentication and secure payment systems can improve governance by reducing leakages in the delivery systems and reducing payment delays without adversely affecting program access.\nThe theory, based on previous research (World Bank 2003; Pritchett 2010, Reinikka and Svensson 2004; Programme Evaluation Organisation 2005; Niehaus and Sukhtankar 2013b) assumes that corruption and leakages are prevalent in government services in developing countries, leading to ineﬀiciencies in assistance delivery systems. By implementing biometric authentication and secure payment systems, the government can ensure that the correct individuals receive the correct payments and services. The Smartcards intervention has a two-fold effect. First, it changes the institutions responsible for managing the funds by eliminating a few intermediaries and moves the point of payment closer to the recipient. Second, it introduces biometric authentication which facilities transparency in the system by providing a secure, tamperproof record of who has received services and payments. This can help to reduce corruption and increase accountability.\nSimilar to Muralidharan et al. 2016, we consider two main dimensions of impact: payment logistics, and leakages. We also consider multiple scenarios surrounding each dimension. First, the payment logistics could improve or worsen. It could naturally improve the system by moving the point of payment closer to the village, or the program could slow down the process due to technical problems like malfunctioning authentication devices or absence of technical staff. Improving payment logistics will affect the wait times of beneficiaries to access the funds. Secondly, the program could reduce or fail to reduce leakages. Theoretically, smartcards should reduce payments to ghost recipients and forbid oﬀicials to collect payments in the name of real beneficiaries as the beneficiaries must be present to pass the biometric test. But there could be technical loopholes in the payment architecture that could disprove the claim of an absolute secure system. Moreover, the assistance programs’ access could improve or suffer. In case of NREGS, local oﬀicers decide projects and generate employment. Eliminating opportunities to seek rent may disincentivize oﬀicials and they might reduce access. On the other hand, the NREGS funds would directly go towards to the development projects that were originally intended thus creating rural assets and generating more employment.\n\n\n\nSmartcard Program Impact Theory Graph\n\n\n\nLogic model\n\n\n\nSmartcard Program Logic Model\n\n\n\nInputs:\n\n\nFederal and state assistance funds: These funds are required to finance the implementation of the Smartcard intervention as well as NGREGS transfer payments.\nBiometric authentication technology: This technology is required to verify the identity of beneficiaries and ensure that payments are disbursed to the correct individuals.\nBanks: Banks are required to help manage the payment system and ensure that payments are securely and efficiently disbursed to beneficiaries.\nSecure payment systems: Secure payment systems are required to ensure that payments are not lost or stolen during the disbursement process.\nSmartcards: Smartcards are required to store the payment information of beneficiaries and facilitate the payment process.\nTechnical staff: Trained staff are required to collect and manage beneficiary information, conduct stress tests, and implement the payment system effectively and efficiently.\nSurvey support staff: Survey staff are required to conduct randomized promotion and conduct weekly surveys to collect information on variables of interest.\n\n\nActivities:\n\n\nTraining staff for data collection and management of payment systems effectively and efficiently\nCollecting beneficiary personal and biometric information to verify individuals.\nConducting stress tests on the digital payment system to ensure its smooth functioning.\nImplementation of secure payment systems to disburse payments to the correct individuals:\nConducting door-to-door random promotions and provide information about the program.\nConducting weekly surveys to collect data.\n\n\nOutputs:\n\n\nNumber of people using Smartcard technology:\nPercentage change in average wait times at payment centers.\nAmount of money credited for every beneficiary and payment type.\nAmount of money saved: The amount of money saved is a measure of the program’s efficiency and cost-effectiveness.\nChange in leakages and corruption in the system:\n\n\nOutcomes:\n\n\nReduced wait times: The Smartcard intervention is designed to reduce wait times at payment centers and payment delays which can save time and reduce the burden on beneficiaries.\nIncreased earnings for the beneficiaries: The Smartcard intervention is designed to improve the delivery of funds for welfare programs, which can increase the earnings of beneficiaries.\nIncreased efficiency: The Smartcard program will deliver reduce leakage, ensuring beneficiaries receive more funds in less time without allocating more budget to NREGS.\nImproved trust in government services: The Smartcard intervention is designed to improve the delivery of funds for welfare programs and ensure transparency in the payment system, which can improve trust in government.\n\n\n\n\nOutcome and Causation\n\nMain outcome\nThe goal of this paper is to evaluate the smartcard program’s effect on governance. Good governance is critical to achieving the goals of development. Moreover, strong governance ensures the efficiency and smoothness of governmental processes in achieving goals directed towards public welfare. It means that the government can deliver what they promised to the public in a fair and efficient way. We expect the smartcard intervention to reduce leakages, corruption and wait times for the beneficiaries. This will lead to improved efficiency, increased earnings and greater trust in government welfare measures.\n\n\nMeasurement\nOutcome: Better Governance\n\nImproved wait times\nReduction in corruption and abuse of power\nReduction in leakages in the payment system\nBetter implementation of governmental plans or improved effectiveness\nIncreased transparency\nBudget management\n\nWe think that the most important attributes of better governance are reduced wait times to access promised goods or services, reduction in leakages, and reduction in corruption and abuse of power. These aspects can be best quantified by observational and survey data. Leakages and delays in payments, crucial aspects of governance, should be the focus of the evaluation study. Corruption is also an important aspect of good governance, but it is our opinion that study participants might not be completely transparent about corruption related experiences. Budget management is subject to political ideology and economic situation. On the other hand, transparency can be diﬀicult to measure.\nAttribute 1: Reduction in Leakages\n\nMeasurable definition: Change in money value allocated and the money value actually received per person. Money value here is the rupee (or dollar) value actually received by the beneficiary. In other projects, money value can be the dollar value of the services received.\nIdeal measurement: Difference between the actual official amount of money allocated and the actual money received by the beneficiary.\nFeasible measurement: Difference between the muster-roll recorded amount of money allocated to the beneficiary and the money received by beneficiary based on surveys. Officials can manipulate muster rolls to under record work and pocket the difference. In an ideal world, we cannot know the true allocated value.\nMeasurement of program effect: If the promotion of the smartcard program is randomized, the leakage amount will reflect the local average treatment effect of the program on the compliers.\n\nAttribute 2: Reduction in Wait Time\n\nMeasurable definition: Number of days between completing and work and receiving the payment. Moreover, number of minutes spent at the point of service (PoS) to collect the amount.\nIdeal measurement: In an ideal setting, conduct a randomized trial on the entire population. Post treatment, survey all the people and measure the wait times people in each group experienced before receiving their full promised amount of money.\nFeasible measurement: Conduct baseline surveys before rolling out the smartcard program and randomizing promotions and conducting endline surveys after the intervention to record the wait times of people.\nMeasurement of program effect: Similar leakages, if the promotion is random, wait times would be the local average treatment effect of the smartcard program.\n\nAttribute 3: Reduction in Corruption and Abuse of Power\n\nMeasurable definition: Change in number of corruption incidences and average money value of corruption.\nIdeal measurement: In an ideal world, conduct a randomized trial on the entire population and measure the corruption incidences and per capita corruption amount for both the groups. The difference in these two parameters would be the program effect.\nFeasible measurement: Conduct a small-scale randomized promotion of the smartcard program and measure the number of corruption incidences and average corruption value.\nMeasurement of program effect: As the randomized promotion is a valid instrument, the feasible measures would reflect the local average treatment effect of the smartcard program."
  },
  {
    "objectID": "projects/smartcard/index.html#logic-model",
    "href": "projects/smartcard/index.html#logic-model",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Logic model",
    "text": "Logic model\n\n\n\nSmartcard Program Logic Model\n\n\n\nInputs:\n\n\nFederal and state assistance funds: These funds are required to finance the implementation of the Smartcard intervention as well as NGREGS transfer payments.\nBiometric authentication technology: This technology is required to verify the identity of beneficiaries and ensure that payments are disbursed to the correct individuals.\nBanks: Banks are required to help manage the payment system and ensure that payments are securely and efficiently disbursed to beneficiaries.\nSecure payment systems: Secure payment systems are required to ensure that payments are not lost or stolen during the disbursement process.\nSmartcards: Smartcards are required to store the payment information of beneficiaries and facilitate the payment process.\nTechnical staff: Trained staff are required to collect and manage beneficiary information, conduct stress tests, and implement the payment system effectively and efficiently.\nSurvey support staff: Survey staff are required to conduct randomized promotion and conduct weekly surveys to collect information on variables of interest.\n\n\nActivities:\n\n\nTraining staff for data collection and management of payment systems effectively and efficiently\nCollecting beneficiary personal and biometric information to verify individuals.\nConducting stress tests on the digital payment system to ensure its smooth functioning.\nImplementation of secure payment systems to disburse payments to the correct individuals:\nConducting door-to-door random promotions and provide information about the program.\nConducting weekly surveys to collect data.\n\n\nOutputs:\n\n\nNumber of people using Smartcard technology:\nPercentage change in average wait times at payment centers.\nAmount of money credited for every beneficiary and payment type.\nAmount of money saved: The amount of money saved is a measure of the program’s efficiency and cost-effectiveness.\nChange in leakages and corruption in the system:\n\n\nOutcomes:\n\n\nReduced wait times: The Smartcard intervention is designed to reduce wait times at payment centers and payment delays which can save time and reduce the burden on beneficiaries.\nIncreased earnings for the beneficiaries: The Smartcard intervention is designed to improve the delivery of funds for welfare programs, which can increase the earnings of beneficiaries.\nIncreased efficiency: The Smartcard program will deliver reduce leakage, ensuring beneficiaries receive more funds in less time without allocating more budget to NREGS.\nImproved trust in government services: The Smartcard intervention is designed to improve the delivery of funds for welfare programs and ensure transparency in the payment system, which can improve trust in government."
  },
  {
    "objectID": "projects/smartcard/index.html#main-outcome",
    "href": "projects/smartcard/index.html#main-outcome",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Main outcome",
    "text": "Main outcome\nThe goal of this paper is to evaluate the smartcard program’s effect on governance. Good governance is critical to achieving the goals of development. Moreover, strong governance ensures the efficiency and smoothness of governmental processes in achieving goals directed towards public welfare. It means that the government can deliver what they promised to the public in a fair and efficient way. We expect the smartcard intervention to reduce leakages, corruption and wait times for the beneficiaries. This will lead to improved efficiency, increased earnings and greater trust in government welfare measures."
  },
  {
    "objectID": "projects/smartcard/index.html#measurement",
    "href": "projects/smartcard/index.html#measurement",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Measurement",
    "text": "Measurement\nOutcome: Better Governance\n\nImproved wait times\nReduction in corruption and abuse of power\nReduction in leakages in the payment system\nBetter implementation of governmental plans or improved effectiveness\nIncreased transparency\nBudget management\n\nWe think that the most important attributes of better governance are reduced wait times to access promised goods or services, reduction in leakages, and reduction in corruption and abuse of power. These aspects can be best quantified by observational and survey data. Leakages and delays in payments, crucial aspects of governance, should be the focus of the evaluation study. Corruption is also an important aspect of good governance, but it is our opinion that study participants might not be completely transparent about corruption related experiences. Budget management is subject to political ideology and economic situation. On the other hand, transparency can be diﬀicult to measure.\nAttribute 1: Reduction in Leakages\n\nMeasurable definition: Change in money value allocated and the money value actually received per person. Money value here is the rupee (or dollar) value actually received by the beneficiary. In other projects, money value can be the dollar value of the services received.\nIdeal measurement: Difference between the actual official amount of money allocated and the actual money received by the beneficiary.\nFeasible measurement: Difference between the muster-roll recorded amount of money allocated to the beneficiary and the money received by beneficiary based on surveys. Officials can manipulate muster rolls to under record work and pocket the difference. In an ideal world, we cannot know the true allocated value.\nMeasurement of program effect: If the promotion of the smartcard program is randomized, the leakage amount will reflect the local average treatment effect of the program on the compliers.\n\nAttribute 2: Reduction in Wait Time\n\nMeasurable definition: Number of days between completing and work and receiving the payment. Moreover, number of minutes spent at the point of service (PoS) to collect the amount.\nIdeal measurement: In an ideal setting, conduct a randomized trial on the entire population. Post treatment, survey all the people and measure the wait times people in each group experienced before receiving their full promised amount of money.\nFeasible measurement: Conduct baseline surveys before rolling out the smartcard program and randomizing promotions and conducting endline surveys after the intervention to record the wait times of people.\nMeasurement of program effect: Similar leakages, if the promotion is random, wait times would be the local average treatment effect of the smartcard program.\n\nAttribute 3: Reduction in Corruption and Abuse of Power\n\nMeasurable definition: Change in number of corruption incidences and average money value of corruption.\nIdeal measurement: In an ideal world, conduct a randomized trial on the entire population and measure the corruption incidences and per capita corruption amount for both the groups. The difference in these two parameters would be the program effect.\nFeasible measurement: Conduct a small-scale randomized promotion of the smartcard program and measure the number of corruption incidences and average corruption value.\nMeasurement of program effect: As the randomized promotion is a valid instrument, the feasible measures would reflect the local average treatment effect of the smartcard program."
  },
  {
    "objectID": "projects/smartcard/index.html#identification-strategy",
    "href": "projects/smartcard/index.html#identification-strategy",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Identification Strategy",
    "text": "Identification Strategy\n\nSimilar to Muralidharan et al. 2016, we assert causal claims that the Smartcard intervention will lead to reduced leakages, wait times, and corruption for beneficiary households. In earlier leakage-prone systems, the funds were released by the government and flowed through multiple intermediaries to reach the beneficiary. But in the new Smartcard system, the worker can directly receive funds with just a single third-party processing all the transactions. This leads to fewer leakages in the system as people cannot game it by withdrawing money in the name of ghost recipients. Moreover, oﬀicers have fewer chances to engage in corruption as funds are digitally transferred to the end user’s bank account. Further, the Smartcard intervention will improve payment logistics by reducing wait times and delays in payment collection. Smartcard holders will not be bothered by cash shortages at the PoS as money would be directly deposited into their accounts. Finally, the biometric cards would aid in quicker verifications at the PoS and reduce wait times in queues.\nA host of household-level characteristics influence enrollment into the Smartcard program, reduction in leakages, wait times and corruption events. Some of the observable covariates include location, family size, household literacy rate, religion, assets, and others. Controlling for these covariates is a possible solution, but it does not paint the full causal picture. The NREGS program has a universal access and hence it is fair to provide universal access to the Smartcard program that facilitates the NREGS welfare payments. In this scenario, we suspect that households that enroll into the Smartcard intervention are fundamentally different than the households that do not enroll. For instance, poor households with fewer job opportunities or literate households would be more likely to enroll into the program. There exists a host of household and geographic characteristics that influence enrollment into the program and the outcomes. The universal nature of the program leads to self-selection bias. Additionally, the unobserved household and governmental characteristics lead to omitted variable bias. To tackle these challenges, we introduce the instrumental variable design by using randomized promotion as an instrument. Randomized promotion design closes all the backdoors and provides a local average treatment effect for the compliers that enroll in the Smartcard program after receiving the promotion.\nThe instrumental variable approach isolates the causal effect of the program on the outcome by using the variation in the treatment induced by the instrument. By comparing the outcomes of households who received treatment through the instrument with those who did not receive treatment through the instrument, we can estimate the causal effect of the program on the outcomes of choice.\nSpecifically, the randomized promotion ensures that the distribution of observable and unobservable characteristics between the treatment and control groups is balanced, reducing the potential for selection bias. Furthermore, the instrumental variable approach addresses endogeneity by focusing on the exogenous variation in the randomized promotion, rather than the endogenous relationship between the program and outcomes, thus isolating the causal effect of the program on the outcomes of interest. This approach allows us to obtain an unbiased estimate of the causal effect of the program, even in the presence of selection bias and endogeneity.\nOur identification strategy takes the form of two-staged least squares (2SLS) instrumental variable. In the first stage, we estimate the effect of randomized promotion on the endogenous smartcard enrollment. Hence, we estimate:\n\\[\n\\text{Smartcard}= \\beta_{0} + \\beta \\text{Promotion} + \\gamma X + u\n\\] where \\(\\text{Smartcard}\\) is a binary indicator of whether a household enrolled into the program, and \\(X\\) is a vector of covariates.\nThe general form of the second stage least squares is as follows: \\[\n\\text{Outcome}_i =  \\beta_{0_i} + \\beta_i \\hat{\\text{Smartcard}} + \\gamma_i X + \\epsilon_i\\]\nWhere \\[\\text{Outcome }_i = \\{\\text{Leakage}, \\text{Queue Wait Time }, \\text{Payment Delay}, \\text{Corruption} \\}\\]\nOverall, the instrumental variable approach using randomized promotion provides a rigorous and credible method for estimating the causal effects of the smartcard program on the outcomes of interest, while also addressing potential issues of selection bias and endogeneity.\n\nA DAG for the outcome is as follows:\n\n\nCode\n# Outcome_i DAG\n\niv_dag &lt;- dagitty('dag {\nbb=\"-1.129,-1.666,1.129,1.666\"\n\"# Children\" [pos=\"-0.100,-0.416\"]\n\"# Female Members\" [pos=\"-0.295,-0.335\"]\n\"Household Has Farm\" [pos=\"0.258,-0.469\"]\n\"Household Head (HH) Age\" [pos=\"0.440,1.076\"]\n\"Household Head (HH) Gender\" [pos=\"0.690,0.902\"]\n\"Household Literacy Rate\" [pos=\"0.805,-0.106\"]\n\"Random Promotion (IV)\" [pos=\"-1.005,0.524\"]\nHindu [pos=\"-0.187,0.991\"]\nOutcome_i [outcome,pos=\"0.896,0.466\"]\nLocation [pos=\"-0.483,-0.189\"]\nMuslim [pos=\"0.130,1.169\"]\nSmartcard [exposure,pos=\"-0.689,0.514\"]\n\"# Children\" -&gt; Outcome_i\n\"# Children\" -&gt; Smartcard\n\"# Female Members\" -&gt; Outcome_i\n\"# Female Members\" -&gt; Smartcard\n\"Household Has Farm\" -&gt; Smartcard\n\"Household Head (HH) Age\" -&gt; Outcome_i\n\"Household Head (HH) Age\" -&gt; Smartcard\n\"Household Head (HH) Gender\" -&gt; Outcome_i\n\"Household Head (HH) Gender\" -&gt; Smartcard\n\"Household Literacy Rate\" -&gt; Outcome_i\n\"Household Literacy Rate\" -&gt; Smartcard\n\"Random Promotion (IV)\" -&gt; Smartcard\nHindu -&gt; Outcome_i\nHindu -&gt; Smartcard\nLocation -&gt; Outcome_i\nLocation -&gt; Smartcard\nMuslim -&gt; Outcome_i\nMuslim -&gt; Smartcard\nSmartcard -&gt; Outcome_i\n}\n\n')\n\nggdag_status(iv_dag,  text = FALSE, use_labels = \"name\") +\n  guides(fill = \"none\", color = \"none\") +\n  theme_dag()"
  },
  {
    "objectID": "projects/smartcard/index.html#hypotheses",
    "href": "projects/smartcard/index.html#hypotheses",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Hypotheses",
    "text": "Hypotheses\nWe formulate the following hypothesis and test it in the subsequent sections:\n\\(\\textbf{H1}\\): The Smartcard program causes leakages to decrease.\n\\(\\textbf{H2}\\): The Smartcard program causes payment delays (in number of days to receive payment) to decrease.\n\\(\\textbf{H3}\\): The Smartcard program causes waiting time in queue (in minutes) to decrease.\n\\(\\textbf{H4}\\): The Smartcard program reduces incidences of corruption."
  },
  {
    "objectID": "projects/smartcard/index.html#data",
    "href": "projects/smartcard/index.html#data",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Data",
    "text": "Data\n\nWe propose to use a mix of administrative and survey data to estimate the causal link between the program and the outcomes of interest. We plan to conduct weekly survey for a 4-week period to measure the household level weekly payments received, time spent (in minutes) at the PoS, days between work is completed and payment is received, and corruption incidences. We will compare the weekly payments received to the official disbursement data to estimate the household level leakage in the system, the amount lost in inefficiencies or corruptions. Alongside, we will collect data on household characteristics like number of children, number of females in the household, household literacy rate, religion, assets like farmland, and household head age and gender.\n\n\n\nCode\nset.seed(1234)\n\nn_household &lt;- 100000\nn_panchayats &lt;- 500\niv_data &lt;- tibble(\n  household_ID = factor(1:n_household),\n  panchayat_ID = factor(sample(1:n_panchayats, n_household, replace = TRUE)),\n  state_ID = factor(sample(1:3, n_household, replace = TRUE))) %&gt;%\n  mutate(\n  off_week_1 = rlnormTrunc(n_household, meanlog = log(500), \n                              sdlog =0.6, min = 0, max = 4000),\n  off_week_2 = rlnormTrunc(n_household, meanlog = log(450), \n                              sdlog =0.6, min = 0, max = 4000),\n  off_week_3 = rlnormTrunc(n_household, meanlog = log(450), \n                              sdlog =0.6, min = 0, max = 4000),\n  off_week_4 = rlnormTrunc(n_household, meanlog = log(400), \n                              sdlog =0.6, min = 0, max = 4000),\n  week_1 = rlnormTrunc(n_household, meanlog = log(450), \n                          sdlog =0.6, min = 0, max = 3000),\n  week_2 = rlnormTrunc(n_household, meanlog = log(400), \n                          sdlog =0.6, min = 0, max = 3400),\n  week_3 = rlnormTrunc(n_household, meanlog = log(375), \n                          sdlog =0.6, min = 0, max = 3000),\n  week_4 = rlnormTrunc(n_household, meanlog = log(350),\n                          sdlog =0.6, min = 0, max = 3000)) %&gt;% \n  \n  mutate(off_total_amount = off_week_1 + off_week_2 + off_week_3 + off_week_4 +\n           rnorm(n_household, 0, 10),\n         total_amount = week_1 + week_2 + week_3 + week_4 + rnorm(n_household, 0, 10),\n         leakage_base = off_total_amount - total_amount) \n  \npromotion_panchayats &lt;- sample(1:n_panchayats, round(n_panchayats / 2), replace = FALSE)\n\niv_data &lt;- iv_data %&gt;% \n  # Randomized Promotion\n  mutate(promotion = ifelse(panchayat_ID %in% promotion_panchayats, TRUE, FALSE)) %&gt;% \n  # Potential Confounders\n  mutate(location = panchayat_ID,\n         num_members = round(rtruncnorm(n_household, a = 0, b = 20, \n                                  mean = 4, sd = 1)),\n         farm = rbinom(n_household, 1, 0.1),\n         children = round(rtruncnorm(n_household, a = 0, b = 10, \n                                  mean = 2, sd = 1)),\n         household_literacy_rate = rtruncnorm(n_household, a = 0, b = 1, \n                                              mean = 0.5, sd = 0.2),\n         female_member = round(rtruncnorm(n_household, a = 0, b = 10, \n                                  mean = 1, sd = 1)),\n         hindu = rbinom(n_household, 1, 0.8),\n         muslim = ifelse(hindu == 1, 0, rbinom(1, 1, 0.1)),\n         other_religion = ifelse(hindu == 1 | muslim == 1, 0, 1),\n         hh_age = round(rtruncnorm(n_household, a = 18, b = 80, \n                                  mean = 35, sd = 10)),\n         hh_male = rbinom(n_household, 1, 0.9)\n         ) %&gt;% \n  mutate(promotion_effect = promotion * 6,\n         # Agricultural Locations\n         farm_effect = -0.09 * farm,\n         children_effect = 0.03 * children,\n         literacy_effect = - 0.01 * household_literacy_rate,\n         female_effect = 0.02 * female_member,\n         hindu_effect = 0.03 * hindu,\n         muslim_effect = ifelse(location %in% c(2, 6, 7), 0.005 * muslim, -0.01 * muslim),\n         other_effect = - 0.02 * other_religion,\n         hh_age_effect = 0.02 * hh_age,\n         hh_male_effect = 0.04 * hh_male,\n         smartcard_score = 0 + promotion_effect + \n           farm_effect + children_effect +\n           literacy_effect + female_effect + hindu_effect + \n           muslim_effect + other_effect + hh_age_effect + hh_male_effect +\n           rnorm(n_household, 0, 0.2),\n         smartcard_probability = scales::rescale(smartcard_score, to = c(0.1, 1)),\n         smartcard = rbinom(n_household, 1, smartcard_probability)) %&gt;% \n  mutate(smartcard_leakage_effect = - 60 * smartcard)\n\niv_data &lt;- iv_data %&gt;% \n  mutate(leakage = leakage_base + smartcard_leakage_effect + + (100 * children_effect) +\n           + (100 * farm_effect) +\n           (100 *literacy_effect) + (100 * female_effect) + \n           (100 * hindu_effect) + (100 * muslim_effect) + (100 * other_effect) + \n           (100 * hh_age_effect) + (hh_male_effect) +\n           rnorm(n_household, 0, 20)) %&gt;% \n  mutate(leakage_difference = leakage - leakage_base) %&gt;% \n  mutate(leakage_percetage = rescale(leakage/off_total_amount, to = c(0, 1)))\n\n\n# Time to Payment\n\niv_data &lt;- iv_data %&gt;% \n  mutate(\n    \n    week_1_ttp = ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                          round(rnorm(n_household, 0, 5)),\n    week_2_ttp = ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                          round(rnorm(n_household, 0, 5)),\n    week_3_ttp =ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                            round(rnorm(n_household, 0, 5)),\n    week_4_ttp = ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                          round(rnorm(n_household, 0, 5)),\n    avg_ttp = (week_1_ttp + week_2_ttp + week_3_ttp + week_4_ttp)/ 4\n  )\n\niv_data &lt;- iv_data%&gt;% \n  mutate(\n    week_1_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    week_2_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    week_3_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    week_4_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    avg_dbp = (week_1_dbp+ week_2_dbp+ week_3_dbp+ week_4_dbp) / 4\n  )\n\n# Corruption\n\n# Did you pay bribe? Yes/No\n\niv_data &lt;- iv_data %&gt;% \n  mutate(\n    smartcard_bribe_effect = -0.2 * smartcard,\n    bribe_score = rbeta(n_household, 4, 6),\n    bribe_probability = scales::rescale(bribe_score + smartcard_bribe_effect + \n                                  rnorm(n_household, mean = 0, sd = 0.01), to = c(0, 1)),\n    week_1_bribe = rbinom(n_household, 1, bribe_probability),\n    week_2_bribe = rbinom(n_household, 1, bribe_probability),\n    week_3_bribe = rbinom(n_household, 1, bribe_probability),\n    week_4_bribe = rbinom(n_household, 1, bribe_probability),\n    bribe_incidence = (week_1_bribe + week_2_bribe +week_3_bribe + week_4_bribe)\n  )\n\n\niv_data_csv &lt;- iv_data %&gt;% \n  dplyr::select(household_ID, panchayat_ID, state_ID, promotion, num_members, farm, children, female_member, household_literacy_rate, hh_age, hh_male, hindu, muslim, other_religion,  off_week_1, off_week_2, off_week_3, off_week_4, off_total_amount, week_1, week_2, week_3, week_4, total_amount, leakage, week_1_ttp, week_2_ttp,week_3_ttp,week_4_ttp, avg_ttp, week_1_dbp,\n                week_2_dbp, week_3_dbp, week_4_dbp, avg_dbp,\n                week_1_bribe, week_2_bribe, week_3_bribe, week_4_bribe, bribe_incidence)\n\nwrite_csv(iv_data_csv, \"./data/smartcard_data.csv\")"
  },
  {
    "objectID": "projects/smartcard/index.html#sls-estimation",
    "href": "projects/smartcard/index.html#sls-estimation",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "2SLS Estimation",
    "text": "2SLS Estimation\n\n\nCode\nleakage_iv_c &lt;- iv_robust(leakage ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male| \n                        promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                        data = iv_data,\n                        diagnostics = TRUE)\n\nttp_iv_c &lt;- iv_robust(avg_ttp ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male\n                      | promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male , \n                      data = iv_data,\n                      diagnostics = TRUE)\n\ndbp_iv_c &lt;- iv_robust(avg_dbp ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male| \n                        promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                      data = iv_data,\n                      diagnostics = TRUE)\n\nbribe_iv_c &lt;- iv_robust(bribe_incidence ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male| \n                          promotion+ farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                        data = iv_data,\n                        diagnostics = TRUE)\n\n\n\n\nCode\n# Robustness Checks\n\nendo_row &lt;- tibble(\"Wu-Hausman Test\", leakage_iv_c$diagnostic_endogeneity_test[\"value\"], ttp_iv_c$diagnostic_endogeneity_test[\"value\"], dbp_iv_c$diagnostic_endogeneity_test[\"value\"], bribe_iv_c$diagnostic_endogeneity_test[\"value\"])\n\n\nrelevance_row &lt;- tibble(\"Weak Instruments Test\", leakage_iv_c$diagnostic_first_stage_fstatistic[\"value\"], ttp_iv_c$diagnostic_first_stage_fstatistic[\"value\"], dbp_iv_c$diagnostic_first_stage_fstatistic[\"value\"], bribe_iv_c$diagnostic_first_stage_fstatistic[\"value\"])\n\ncolnames(relevance_row) &lt;- c(\"Test\", \"a\", \"b\", \"c\", \"d\")\ncolnames(endo_row) &lt;- c(\"Test\", \"a\", \"b\", \"c\", \"d\")\ndiagnostics_tibble &lt;- bind_rows(data.frame(relevance_row), data.frame(endo_row))\n\n\n# Final Table with Robustness Checks\nmodelsummary(list(\"Leakage\" = leakage_iv_c, \"Wait Time (Queue)\" = ttp_iv_c, \n                  \"Payment Delay\" = dbp_iv_c, \"Corruption\" = bribe_iv_c),\n             stars = TRUE,\n             vcov = vcov,\n             metrics = \"all\",\n             add_rows = diagnostics_tibble,\n             title = \"Instrumental Variables Estimation\")\n\n\n\n\nInstrumental Variables Estimation\n\n\n\nLeakage\nWait Time (Queue)\nPayment Delay\nCorruption\n\n\n\n\nsmartcard\n−45.308***\n−10.123***\n−2.995***\n−0.725***\n\n\n\n(9.303)\n(0.075)\n(0.010)\n(0.011)\n\n\nfarm\n−2.225\n0.038\n−0.012\n0.026*\n\n\n\n(9.970)\n(0.081)\n(0.011)\n(0.011)\n\n\nchildren\n6.175*\n−0.006\n−0.002\n0.001\n\n\n\n(2.981)\n(0.024)\n(0.003)\n(0.003)\n\n\nhousehold_literacy_rate\n−15.608\n−0.054\n−0.023\n−0.007\n\n\n\n(15.490)\n(0.125)\n(0.017)\n(0.018)\n\n\nfemale_member\n1.402\n0.006\n0.004\n0.006\n\n\n\n(3.432)\n(0.028)\n(0.004)\n(0.004)\n\n\nhindu\n242.714***\n69.991***\n13.006***\n2.118***\n\n\n\n(18.852)\n(0.153)\n(0.020)\n(0.022)\n\n\nother_religion\n235.559***\n70.036***\n12.994***\n2.119***\n\n\n\n(19.722)\n(0.160)\n(0.021)\n(0.023)\n\n\nhh_age\n2.249***\n0.001\n0.000\n0.000\n\n\n\n(0.325)\n(0.003)\n(0.000)\n(0.000)\n\n\nhh_male\n20.088*\n0.084\n0.002\n0.002\n\n\n\n(9.869)\n(0.080)\n(0.011)\n(0.011)\n\n\nNum.Obs.\n100000\n100000\n100000\n100000\n\n\nR2\n0.001\n0.307\n0.686\n0.103\n\n\nR2 Adj.\n0.001\n0.307\n0.686\n0.103\n\n\nAIC\n1651825.0\n688573.5\n285975.8\n298953.8\n\n\nBIC\n1651920.1\n688668.6\n286071.0\n299048.9\n\n\nRMSE\n934.57\n7.57\n1.01\n1.08\n\n\nStd.Errors\nCustom\nCustom\nCustom\nCustom\n\n\nWeak Instruments Test\n24846.035\n24846.035\n24846.035\n24846.035\n\n\nWu-Hausman Test\n1.489\n0.341\n0.268\n0.968\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the smartcard intervention causes a decrease in leakages by 45.3 rupees for compliers. Moreover, wait time in queue is reduced by 10 minutes. Complier households received payment 3 days earlier, on average, as compared to non-compliers. Lastly, corruption decreased by 0.725 incidences. All estimates are statistically significant at 1% level. These results reinforce our expectations that the smartcard intervention would negatively affect leakage, wait times and corruption faced by the rural workers."
  },
  {
    "objectID": "projects/smartcard/index.html#robustness-checks",
    "href": "projects/smartcard/index.html#robustness-checks",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Robustness Checks",
    "text": "Robustness Checks\n\nInstrument Validity\n\nBased on the first stage model, we see that there is a strong relationship between the instrument (randomized promotion) and Smartcard enrollment. The randomized promotion is positively correlated with Smartcard enrollment. Moreover, the joint F-statistic of the model is 8608 with a p-value less than 0.01. We can reject the hypothesis that the instrument is weak. We can conclude that the instrument is relevant. Since the promotion panchayats are selected through a randomized process, the instrument is not correlated with any observable or unobservable characteristics of households or location. The instrument satisfies exogeneity. Lastly, there is a clear causal mechanism between the randomized promotion and the outcomes of interest. A promotion can only cause leakages, wait times, and corruption to increase or decrease only if a household enrolls into the Smartcard program after receiving the promotion. Hence, the instrument satisfies exclusion.\n\n\n\nCode\nrelevance_check &lt;- lm(smartcard ~  promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                      data = iv_data)\nmodelsummary(list(\"First Stage IV\" = relevance_check),\n             stars = T)\n\n\n\n\n\n\n\n First Stage IV\n\n\n\n\n(Intercept)\n0.118***\n\n\n\n(0.008)\n\n\npromotionTRUE\n0.635***\n\n\n\n(0.002)\n\n\nfarm\n−0.007\n\n\n\n(0.004)\n\n\nchildren\n0.003*\n\n\n\n(0.001)\n\n\nhousehold_literacy_rate\n0.002\n\n\n\n(0.006)\n\n\nfemale_member\n0.003+\n\n\n\n(0.001)\n\n\nhindu\n0.004\n\n\n\n(0.003)\n\n\nhh_age\n0.002***\n\n\n\n(0.000)\n\n\nhh_male\n0.005\n\n\n\n(0.004)\n\n\nNum.Obs.\n100000\n\n\nR2\n0.408\n\n\nR2 Adj.\n0.408\n\n\nAIC\n92273.1\n\n\nBIC\n92368.2\n\n\nLog.Lik.\n−46126.530\n\n\nRMSE\n0.38\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n Back to top"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThin Markets, Thick Premiums\n\n\n\n\n\n\n\n\nJul 4, 2025\n\n\nViraj Chordiya\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "research/insurance-alm/index.html",
    "href": "research/insurance-alm/index.html",
    "title": "Thin Markets, Thick Premiums",
    "section": "",
    "text": "This research examines the relationship between market liquidity and insurance premium pricing in specialized insurance markets. Using a comprehensive dataset of insurance transactions, I analyze how thin markets contribute to higher premiums and explore the implications for market efficiency and regulatory policy."
  },
  {
    "objectID": "research/insurance-alm/index.html#abstract",
    "href": "research/insurance-alm/index.html#abstract",
    "title": "Thin Markets, Thick Premiums",
    "section": "",
    "text": "This research examines the relationship between market liquidity and insurance premium pricing in specialized insurance markets. Using a comprehensive dataset of insurance transactions, I analyze how thin markets contribute to higher premiums and explore the implications for market efficiency and regulatory policy."
  },
  {
    "objectID": "research/insurance-alm/index.html#quick-links",
    "href": "research/insurance-alm/index.html#quick-links",
    "title": "Thin Markets, Thick Premiums",
    "section": "Quick Links",
    "text": "Quick Links\n\n📄 Paper (PDF) 📊 SSRN 💻 Data & Code"
  },
  {
    "objectID": "research/insurance-alm/index.html#methodology",
    "href": "research/insurance-alm/index.html#methodology",
    "title": "Thin Markets, Thick Premiums",
    "section": "Methodology",
    "text": "Methodology\nThis is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "LAYOUT_OPTIONS.html",
    "href": "LAYOUT_OPTIONS.html",
    "title": "Layout Improvement Options",
    "section": "",
    "text": "The current custom layout works but could be refined. Here are three options to improve the research card layout and overall design.\n\n\nWhat it does: Keep the current approach but fix proportion and spacing issues\n\n\n\nImprove research card proportions\nBetter responsive behavior\nFix any overflow/spacing issues\nEnhanced visual hierarchy\n\n\n\n\n\nUpdate custom_theme.scss with refined grid and spacing\nOptimize image sizing and text flow\nBetter mobile responsiveness\n\n\n\n\n\n✅ Minimal changes needed\n✅ Maintains current design concept\n✅ Addresses specific layout issues\n\n\n\n\n\n⚠️ Still requires manual SCSS maintenance\n\n\n\n\n\n\nWhat it does: Replace custom layout with proven Jolla template\n\n\n\nChange index.qmd to use Jolla template\nRemove custom hero/research sections\n\nUse Jolla’s built-in about and listing features\nKeep static link injection system\n\n\n\n\n# index.qmd\n---\ntitle: \"Viraj R. Chordiya\"\nabout:\n  template: jolla\n  image: viraj.jpeg\n  image-shape: round\n  links:\n    - icon: linkedin\n      text: LinkedIn\n      href: https://linkedin.com/in/virajchordiya\n# ... rest stays the same\n---\n\n\n\n\n✅ Proven, well-tested layout\n✅ Better responsive design out of the box\n✅ No custom CSS maintenance needed\n✅ Cleaner research card display\n\n\n\n\n\n⚠️ Less customization control\n⚠️ Need to adapt to Jolla’s structure\n\n\n\n\n\n\nWhat it does: Build a modern CSS Grid layout from scratch\n\n\n\nReplace flexbox with CSS Grid\nModern responsive design patterns\nBetter control over breakpoints\nProfessional academic layout\n\n\n\n\n\nCreate new CSS Grid system in SCSS\nUpdate HTML structure for grid\nModern responsive patterns (grid-template-areas)\nClean separation of concerns\n\n\n\n\n\n✅ Maximum control and flexibility\n✅ Modern responsive design\n✅ Professional appearance\n✅ Future-proof approach\n\n\n\n\n\n⚠️ More complex to implement\n⚠️ Requires more CSS knowledge\n\n\n\n\n\n\nStart with Option 1 (Refined Custom Layout) because: 1. Quick to implement and test 2. Addresses immediate issues 3. Can always upgrade to Options 2 or 3 later 4. Maintains your current visual design\nIf Option 1 doesn’t fully solve the layout issues, then Option 2 (Jolla) is the safest choice for a professional academic website.\n\n\n\nBased on the current layout, here are the specific problems to fix:\n\nResearch Card Proportions: Images vs text balance\nResponsive Behavior: Mobile layout improvements\n\nSpacing Consistency: Margins and padding\nVisual Hierarchy: Better separation between sections\nCross-browser Compatibility: Ensure consistent rendering\n\nLet me know which option you’d like to pursue!"
  },
  {
    "objectID": "LAYOUT_OPTIONS.html#option-1-refined-custom-layout-recommended",
    "href": "LAYOUT_OPTIONS.html#option-1-refined-custom-layout-recommended",
    "title": "Layout Improvement Options",
    "section": "",
    "text": "What it does: Keep the current approach but fix proportion and spacing issues\n\n\n\nImprove research card proportions\nBetter responsive behavior\nFix any overflow/spacing issues\nEnhanced visual hierarchy\n\n\n\n\n\nUpdate custom_theme.scss with refined grid and spacing\nOptimize image sizing and text flow\nBetter mobile responsiveness\n\n\n\n\n\n✅ Minimal changes needed\n✅ Maintains current design concept\n✅ Addresses specific layout issues\n\n\n\n\n\n⚠️ Still requires manual SCSS maintenance"
  },
  {
    "objectID": "LAYOUT_OPTIONS.html#option-2-switch-to-quarto-jolla-template",
    "href": "LAYOUT_OPTIONS.html#option-2-switch-to-quarto-jolla-template",
    "title": "Layout Improvement Options",
    "section": "",
    "text": "What it does: Replace custom layout with proven Jolla template\n\n\n\nChange index.qmd to use Jolla template\nRemove custom hero/research sections\n\nUse Jolla’s built-in about and listing features\nKeep static link injection system\n\n\n\n\n# index.qmd\n---\ntitle: \"Viraj R. Chordiya\"\nabout:\n  template: jolla\n  image: viraj.jpeg\n  image-shape: round\n  links:\n    - icon: linkedin\n      text: LinkedIn\n      href: https://linkedin.com/in/virajchordiya\n# ... rest stays the same\n---\n\n\n\n\n✅ Proven, well-tested layout\n✅ Better responsive design out of the box\n✅ No custom CSS maintenance needed\n✅ Cleaner research card display\n\n\n\n\n\n⚠️ Less customization control\n⚠️ Need to adapt to Jolla’s structure"
  },
  {
    "objectID": "LAYOUT_OPTIONS.html#option-3-css-grid-based-custom-layout",
    "href": "LAYOUT_OPTIONS.html#option-3-css-grid-based-custom-layout",
    "title": "Layout Improvement Options",
    "section": "",
    "text": "What it does: Build a modern CSS Grid layout from scratch\n\n\n\nReplace flexbox with CSS Grid\nModern responsive design patterns\nBetter control over breakpoints\nProfessional academic layout\n\n\n\n\n\nCreate new CSS Grid system in SCSS\nUpdate HTML structure for grid\nModern responsive patterns (grid-template-areas)\nClean separation of concerns\n\n\n\n\n\n✅ Maximum control and flexibility\n✅ Modern responsive design\n✅ Professional appearance\n✅ Future-proof approach\n\n\n\n\n\n⚠️ More complex to implement\n⚠️ Requires more CSS knowledge"
  },
  {
    "objectID": "LAYOUT_OPTIONS.html#recommendation",
    "href": "LAYOUT_OPTIONS.html#recommendation",
    "title": "Layout Improvement Options",
    "section": "",
    "text": "Start with Option 1 (Refined Custom Layout) because: 1. Quick to implement and test 2. Addresses immediate issues 3. Can always upgrade to Options 2 or 3 later 4. Maintains your current visual design\nIf Option 1 doesn’t fully solve the layout issues, then Option 2 (Jolla) is the safest choice for a professional academic website."
  },
  {
    "objectID": "LAYOUT_OPTIONS.html#current-issues-to-address",
    "href": "LAYOUT_OPTIONS.html#current-issues-to-address",
    "title": "Layout Improvement Options",
    "section": "",
    "text": "Based on the current layout, here are the specific problems to fix:\n\nResearch Card Proportions: Images vs text balance\nResponsive Behavior: Mobile layout improvements\n\nSpacing Consistency: Margins and padding\nVisual Hierarchy: Better separation between sections\nCross-browser Compatibility: Ensure consistent rendering\n\nLet me know which option you’d like to pursue!"
  },
  {
    "objectID": "projects/garch-var/index.html",
    "href": "projects/garch-var/index.html",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "",
    "text": "Code\n# Importing Packages\n\nlibrary(quantmod)\nlibrary(xts)\nlibrary(gridExtra)\nlibrary(MASS)\nlibrary(fGarch)\nlibrary(sn)\nlibrary(copula)\nlibrary(ks)\nlibrary(stargazer)\nlibrary(patchwork)\nlibrary(rugarch)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(tseries)\nlibrary(modelsummary)"
  },
  {
    "objectID": "projects/garch-var/index.html#time-series-graphs-and-distribution-plots",
    "href": "projects/garch-var/index.html#time-series-graphs-and-distribution-plots",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Time series graphs and distribution plots",
    "text": "Time series graphs and distribution plots\n\n\nCode\nplot1 &lt;- ggplot(spxl_df, aes(x = Index, y = SPXL.Adjusted))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") +\n  theme_bw()\n\nplot2 &lt;- ggplot(bac_df, aes(x = Index, y = BAC.Adjusted))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") + \n  theme_bw()\n\nplot1 / plot2"
  },
  {
    "objectID": "projects/garch-var/index.html#exploratory-data-analysis",
    "href": "projects/garch-var/index.html#exploratory-data-analysis",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nFirstly, we check for weak stationarity by investigating the time series plots of log returns of both the financial instruments. Next, we conduct the Augmented Dickey Fuller (ADF) test to check for the presence of unit root.\n\n\n\nCode\n# Plotting log returns\n\nplot_spxl_r &lt;- ggplot(daily_df, aes(x = Index, y = SPXL_r))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") + \n  theme_bw()\n\nplot_bac_r &lt;- ggplot(daily_df, aes(x = Index, y = BAC_r))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") + \n  theme_bw()\n\nplot_spxl_r / plot_bac_r\n\n\n\n\n\n\n\n\n\n\n\nCode\nadf.test(daily_df$SPXL_r)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$SPXL_r\nDickey-Fuller = -13.314, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\nadf.test(daily_df$BAC_r)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$BAC_r\nDickey-Fuller = -12.962, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\nUpon visual inspection, it looks like both the log returns are stationary. Moreover, from the ADF test, we fail to reject the null hypothesis of non-stationarity at 1% confidence.\nIn order to look for time dependency and volatility clustering of log returns of SPLX and BAC, From the graphs, it is evident that both series display time varying volatility. Moreover, volatility clusters together. For better analysis, we need to model the nonconstant volatility.\n\n\n\nCode\n# ACF plots\nggacf &lt;- function(x, ci=0.95, type=\"correlation\", xlab=\"Lag\", ylab=NULL,\n                  ylim=NULL, main=NULL, ci.col=\"blue\", lag.max=NULL) {\n\n    x &lt;- as.data.frame(x)\n\n    x.acf &lt;- acf(x, plot=F, lag.max=lag.max, type=type)\n\n    ci.line &lt;- qnorm((1 - ci) / 2) / sqrt(x.acf$n.used)\n\n    d.acf &lt;- data.frame(lag=x.acf$lag, acf=x.acf$acf)\n\n    g &lt;- ggplot(d.acf, aes(x=lag, y=acf)) +\n        geom_hline(yintercept=0) +\n        geom_segment(aes(xend=lag, yend=0)) +\n        geom_hline(yintercept=ci.line, color=ci.col, linetype=\"dashed\") +\n        geom_hline(yintercept=-ci.line, color=ci.col, linetype=\"dashed\") +\n        theme_bw() +\n        xlab(\"Lag\") +\n        ggtitle(ifelse(is.null(main), \"\", main)) +\n        if (is.null(ylab))\n            ylab(ifelse(type==\"partial\", \"PACF\", \"ACF\"))\n        else\n            ylab(ylab)\n\n    g\n}\n\nspxl_acf &lt;- ggacf(daily_df$SPXL_r, main = \"SPXL\")\nspxl_acf2 &lt;- ggacf(daily_df$SPXL_r ** 2, main = \"SPXL Squared\")\nbac_acf &lt;- ggacf(daily_df$BAC_r, main = \"BAC\")\nbac_acf2 &lt;- ggacf(daily_df$BAC_r ** 2, main = \"BAC Squared\")\n\n(spxl_acf + spxl_acf2) / (bac_acf + bac_acf2)\n\n\n\n\n\n\n\n\n\n\nFrom the log return graphs, it is evident that both series display time varying volatility. Moreover, ACF plots of squared series of both stocks show significant serial correlation. For better analysis, we need to model time dependence and non-constant volatility."
  },
  {
    "objectID": "projects/garch-var/index.html#ar1-garch11-model",
    "href": "projects/garch-var/index.html#ar1-garch11-model",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "AR(1)-GARCH(1,1) Model",
    "text": "AR(1)-GARCH(1,1) Model\n\nIn order to model autocorelation and volatility clustering, we combine an AR(1) model that has a nonconstant conditional mean but a constant conditional variance with a GARCH(1,1) model that has conditional mean and the variance of data depends on the past. Additionally, in a GARCH model, conditional standard deviation exhibits more persistent periods of low and high volatility.\nThe \\(AR(1) - GARCH(1,1)\\) is model is as follows:\n\\[ X_t = \\mu + \\phi X_{t-1} + e_t, \\quad e_t =  \\sigma_{t}\\varepsilon_{t}, \\quad \\sigma^2_t = \\omega + \\alpha e^2_{t-1} + \\beta \\sigma^2_{t-1} \\] \\[ \\tilde X_t = \\tilde \\mu + \\tilde \\phi \\tilde X_{t-1} + \\tilde e_t, \\quad \\tilde e_t =  \\tilde \\sigma_{t}\\tilde \\varepsilon_{t}, \\quad \\tilde \\sigma^2_t = \\tilde \\omega + \\tilde \\alpha \\tilde e^2_{t-1} + \\tilde \\beta \\tilde \\sigma^2_{t-1} \\]\nwhere, \\(X_t\\) is the daily log returns of SPXL and \\(\\tilde X_t\\) is the daily log returns of BAC.\n\n\n\nCode\nspxl &lt;- as.vector(daily_df$SPXL_r)\nbac &lt;- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec &lt;- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit &lt;- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec &lt;- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nbac_fit &lt;- ugarchfit(bac_spec, data = bac)"
  },
  {
    "objectID": "projects/garch-var/index.html#residual-copula-modelling",
    "href": "projects/garch-var/index.html#residual-copula-modelling",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Residual Copula Modelling",
    "text": "Residual Copula Modelling\n\nIn order to work with a portfolio of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (BAC), we need to model both the series as a copula to extract the dependence structure between the two financial instruments. In the project, we particulary study the dependent structure of the standardized residuals of an \\(AR(1) - GARCH(1,1)\\) model fitted to both the series.\nFirst, we fit a Student t distribution to both \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon_t\\) using the fitdistr() function from the MASS library.\n\n\n\nCode\n# Standardized Residuals\n\nspxl_res &lt;- as.vector(residuals(spxl_fit, standardize = T))\nbac_res &lt;- as.vector(residuals(bac_fit, standardize = T))\n\n# Fitting t-distribution\nspxl_res_t &lt;- fitdistr(spxl_res, \"t\")\nbac_res_t &lt;- fitdistr(bac_res, \"t\")\n\n\n\n\nCode\n# Fitting t-density plots\npar(mfrow = c(1,2))\nn &lt;- length(spxl_res)\n\nx1 &lt;- qt((1:n)/(n+1), df = 5)\nx2 &lt;- qt((1:n)/(n+1), df = 5)\n\nqqplot(sort(spxl_res), x1, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"SPXL Residual t-plot, DF = 5\")\n\nqqplot(sort(bac_res), x2, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"BAC Residual t-plot, DF = 5\")\n\n\n\n\n\n\n\n\n\n\nA t-distribution with df = 5 fits well to the SPXL and BAC residuals. The qqplot for both the instruments is a straight line except for a few outliers. Given that the total number of observations (n = 2517) is high enough, the outliers are a small fraction of the sample.\nSecond, we transform the residuals into marginal t - distributions and fit t-copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas. We select the best fit copula by minimizing AIC. We select t-copula as it minimizes AIC.\n\n\n\nCode\n# Transform residues into uniform distribution\n\nspxl_uniform &lt;- pt(spxl_res, df = spxl_res_t$estimate[3])\nbac_uniform &lt;- pt(bac_res, df = bac_res_t$estimate[3])\n\nU_hat &lt;- cbind(spxl_uniform, bac_uniform)\ncolnames(U_hat) &lt;- c(\"SPXL_U\", \"BAC_U\")\n\ntau &lt;- as.numeric(cor.test(spxl_uniform, bac_uniform, \n                           method=\"kendall\")$estimate)\nomega &lt;- sin(tau * pi/2)\n\n\n\n\nCode\n# Fit t-Copula\nCt &lt;- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t &lt;- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t &lt;- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n\n[1] -1516.131\n\n\nCode\n# Gaussian Copula\nCgauss &lt;- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss &lt;- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                          copula = normalCopula(dim = 2))\n\naic_gauss &lt;- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n\n[1] -1411.64\n\n\nCode\n# Gumbel Copula\nC_gumbel &lt;- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel &lt;- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel &lt;- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n\n[1] -1388.308\n\n\nCode\n# Clayton Copula\n\nC_clayton &lt;- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                      method = \"ml\")\nlog_lik_clayton &lt;- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton &lt;- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n\n[1] -1213.358\n\n\nCode\n# Frank Copula\n\nCfrank &lt;- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank &lt;- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank &lt;- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\n\n[1] -1395.342"
  },
  {
    "objectID": "projects/garch-var/index.html#residual-analysis",
    "href": "projects/garch-var/index.html#residual-analysis",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\nWe conduct a residual analysis on the standardized residuals \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon\\) to check the fit of the \\(AR(1) + GARCH(1,1)\\) model.\nFirst, we inspect the standardized residuals and squared standardized residuals for autocorrelation. The figure below plots the acf() function for the four series.\n\n\n\nCode\nspxl_res_acf &lt;- ggacf(spxl_res, main = \"SPXL Std. Residuals\")\nspxl_res_acf2 &lt;- ggacf(spxl_res ** 2, main = \"Squared SPXL Std. Residuals\")\nbac_res_acf &lt;- ggacf(bac_res, main = \"BAC Std. Residuals\")\nbac_res_acf2 &lt;- ggacf(bac_res ** 2, main = \"Squared BAC Std. Residuals\")\n\n(spxl_res_acf + spxl_res_acf2) / (bac_res_acf + bac_res_acf2)\n\n\n\n\n\n\n\n\n\n\nThe AR(1) + GARCH(1,1) model fits very well to SPXL and BAC. The standardized residuals and the squared standardized residuals of both the models show no significant serial correlation at 95% confidence.\nNext we look at the following box tests to check autocorrelation\n\n\n\nCode\nshow(spxl_fit)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.002425    0.000386   6.2774 0.000000\nar1    -0.057599    0.022519  -2.5578 0.010532\nomega   0.000039    0.000005   7.2772 0.000000\nalpha1  0.234792    0.023529   9.9789 0.000000\nbeta1   0.735780    0.021291  34.5576 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.002425    0.000393   6.1644 0.000000\nar1    -0.057599    0.020538  -2.8045 0.005039\nomega   0.000039    0.000008   4.7666 0.000002\nalpha1  0.234792    0.035225   6.6656 0.000000\nbeta1   0.735780    0.030380  24.2194 0.000000\n\nLogLikelihood : 5725.413 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -4.5454\nBayes        -4.5338\nShibata      -4.5454\nHannan-Quinn -4.5412\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      2.051  0.1521\nLag[2*(p+q)+(p+q)-1][2]     2.125  0.1742\nLag[4*(p+q)+(p+q)-1][5]     2.965  0.4401\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                   0.004507  0.9465\nLag[2*(p+q)+(p+q)-1][5]  0.755387  0.9121\nLag[4*(p+q)+(p+q)-1][9]  2.391474  0.8535\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3] 0.0007028 0.500 2.000  0.9789\nARCH Lag[5] 1.8924935 1.440 1.667  0.4955\nARCH Lag[7] 2.9168770 2.315 1.543  0.5294\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.4917\nIndividual Statistics:              \nmu     0.06813\nar1    0.03829\nomega  0.25524\nalpha1 0.74886\nbeta1  0.80326\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                     t-value      prob sig\nSign Bias           3.751262 0.0001799 ***\nNegative Sign Bias  0.946854 0.3438041    \nPositive Sign Bias  0.004948 0.9960528    \nJoint Effect       20.723476 0.0001202 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     132.0    6.129e-19\n2    30     157.1    1.510e-19\n3    40     178.8    8.634e-20\n4    50     187.7    4.062e-18\n\n\nElapsed time : 0.519062 \n\n\nCode\nshow(bac_fit)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.001034    0.000323   3.2023 0.001363\nar1     0.032518    0.021631   1.5033 0.132758\nomega   0.000018    0.000004   4.2020 0.000026\nalpha1  0.105468    0.015991   6.5953 0.000000\nbeta1   0.837325    0.026363  31.7610 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.001034    0.000313   3.3035 0.000955\nar1     0.032518    0.020844   1.5601 0.118735\nomega   0.000018    0.000007   2.5283 0.011463\nalpha1  0.105468    0.033066   3.1896 0.001425\nbeta1   0.837325    0.048064  17.4211 0.000000\n\nLogLikelihood : 6693.436 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.3146\nBayes        -5.3030\nShibata      -5.3146\nHannan-Quinn -5.3104\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.02465  0.8753\nLag[2*(p+q)+(p+q)-1][2]   0.56006  0.9483\nLag[4*(p+q)+(p+q)-1][5]   1.75866  0.7794\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.550  0.2131\nLag[2*(p+q)+(p+q)-1][5]     2.058  0.6047\nLag[4*(p+q)+(p+q)-1][9]     3.474  0.6789\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.0687 0.500 2.000  0.7932\nARCH Lag[5]    1.2776 1.440 1.667  0.6527\nARCH Lag[7]    2.2188 2.315 1.543  0.6710\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  0.8988\nIndividual Statistics:              \nmu     0.02392\nar1    0.02143\nomega  0.24815\nalpha1 0.57062\nbeta1  0.34991\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           0.3415 0.7327322    \nNegative Sign Bias  3.0886 0.0020331 ***\nPositive Sign Bias  0.5373 0.5910826    \nJoint Effect       17.9996 0.0004399 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     77.00    6.080e-09\n2    30     90.07    3.554e-08\n3    40    105.92    4.275e-08\n4    50    114.09    4.159e-07\n\n\nElapsed time : 0.2202091 \n\n\n\n\nCode\nbac_fit\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.001034    0.000323   3.2023 0.001363\nar1     0.032518    0.021631   1.5033 0.132758\nomega   0.000018    0.000004   4.2020 0.000026\nalpha1  0.105468    0.015991   6.5953 0.000000\nbeta1   0.837325    0.026363  31.7610 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(&gt;|t|)\nmu      0.001034    0.000313   3.3035 0.000955\nar1     0.032518    0.020844   1.5601 0.118735\nomega   0.000018    0.000007   2.5283 0.011463\nalpha1  0.105468    0.033066   3.1896 0.001425\nbeta1   0.837325    0.048064  17.4211 0.000000\n\nLogLikelihood : 6693.436 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.3146\nBayes        -5.3030\nShibata      -5.3146\nHannan-Quinn -5.3104\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.02465  0.8753\nLag[2*(p+q)+(p+q)-1][2]   0.56006  0.9483\nLag[4*(p+q)+(p+q)-1][5]   1.75866  0.7794\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.550  0.2131\nLag[2*(p+q)+(p+q)-1][5]     2.058  0.6047\nLag[4*(p+q)+(p+q)-1][9]     3.474  0.6789\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.0687 0.500 2.000  0.7932\nARCH Lag[5]    1.2776 1.440 1.667  0.6527\nARCH Lag[7]    2.2188 2.315 1.543  0.6710\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  0.8988\nIndividual Statistics:              \nmu     0.02392\nar1    0.02143\nomega  0.24815\nalpha1 0.57062\nbeta1  0.34991\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           0.3415 0.7327322    \nNegative Sign Bias  3.0886 0.0020331 ***\nPositive Sign Bias  0.5373 0.5910826    \nJoint Effect       17.9996 0.0004399 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     77.00    6.080e-09\n2    30     90.07    3.554e-08\n3    40    105.92    4.275e-08\n4    50    114.09    4.159e-07\n\n\nElapsed time : 0.2202091 \n\n\n\nThe weighted versions of the Ljung-Box test and their p-values all indicate that the estimated \\(AR(1) + GARCH(1,1)\\) model for the conditional mean and variance is adequate for removing serial correlation from the series and squared series."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nImpact Evaulation of the Smartcard Program using Synthetic Data\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nViraj R. Chordiya\n\n\n34 min\n\n\n\n\n\n\n\n\n\n\n\n\nBalancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation\n\n\nA Comprehensive Study of SPXL ETF and BAC Stock for VaR Insights\n\n\n\n\n\nNov 30, 2022\n\n\nViraj Chordiya\n\n\n16 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index_clean.html#recent-research",
    "href": "index_clean.html#recent-research",
    "title": "Viraj R. Chordiya",
    "section": "Recent Research",
    "text": "Recent Research\n\n\n\n\n\n\n\n\n\n\nThin Markets, Thick Premiums\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nView all research →"
  }
]