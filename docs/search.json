[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Viraj R. Chordiya",
    "section": "",
    "text": "I’m a senior-year honors student at Georgia State University, majoring in economics and mathematics with a deep interest in financial economics. I love working with numbers, and I am intrigued by the linkages between financial markets and global macro variables. Through my math and graduate-level economics courses, I have developed solid quantitative and programming skills.\nPreviously, I have worked as a research assistant in the Economics department at GSU, enhancing my data analysis and research skills. I was also associated with the Portfolio Management Team (PMT) at GSU as the Chief Economist. The PMT is a student-managed investment fund with an objective to outperform the S&P 500 Index on a risk-adjusted basis. This position gave me hands-on experience in analyzing macroeconomic trends and reporting them to other members."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/garch-var/index.html",
    "href": "posts/garch-var/index.html",
    "title": "Financial Econometrics Final Project",
    "section": "",
    "text": "In todays volatile world, portfolio management is crucial as it helps reduce the investment strategy risk to the extent that cannot be ignored. The goal of a portfolio manager is to reduce risks and maximize returns for their clients. The aim of this project is to analyse the Direxion Daily S&P 500 Bull 3X Shares (SPXL) ETF and Bank of America Corp (NYSE: BAC) stock and study an optimal portfolio to forecast Value-at-Risk. Direxion Daily S&P 500 Bull 3X Shares is a 300% leveraged ETF derived from the S&P 500 index and Bank of America is an evergreen stock generating steady returns. An optimum portfolio mix of these two financial assets can reduce VaR and provide handsome returns. We collect previous 10 year daily OHLC data for both the instruments from Yahoo Finance using quantmod::getSymbols() in R.\nTo this end, we perform the following analyses:\n\nExplore the data to check for time dependence and volatility clustering.\nBuild a time series model, specifically, \\(AR(1) + GARCH(1,1)\\) to model each product’s log returns.\nFit t-distributions to the standardized errors of both models. In order to study the dependence structure of both the residues, we fit several copulas like t - Copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas and choose the best one by minimizing AIC.\nWe conduct a residual analysis on the standardized residues of the model to check for any serial correlation. Using autocorrelation plots and weighted versions of Ljung-Box test, we conclude that the \\(AR(1) + GARCH(1,1)\\) model is a good fit for the data.\nLastly, we utilize numerical methods to conduct Value-at-Risk forecasting of a portfolio mix of SPXL and BAC. We find that VaR increases as SPXL’s share in the portfolio increases."
  },
  {
    "objectID": "posts/garch-var/index.html#data",
    "href": "posts/garch-var/index.html#data",
    "title": "Financial Econometrics Final Project",
    "section": "Data",
    "text": "Data\nWe collect ten years daily OHLC data of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (NYSE: BAC). SPXL is a three times leveraged ETF mirroring the S&P 500 index. Hence, it provides a 300% of the S&P 500 index’s daily return. The data is collected from Yahoo finance for period 2012/11 - 2022/11 using the getSymbol() function from the quantmod library. The table represents the summary statistics of daily adjusted closing stock price, and log returns of both the instruments.\n\n\n\nTable 1. Summary Statistics\n\n\n\n\n\n\n\n\n\nvars\n\n\nn\n\n\nmean\n\n\nsd\n\n\nskew\n\n\nkurtosis\n\n\nse\n\n\n\n\n\n\n\n\nSPXL\n\n\n1\n\n\n2,517\n\n\n42.899\n\n\n31.554\n\n\n1.240\n\n\n0.854\n\n\n0.629\n\n\n\n\nBAC\n\n\n2\n\n\n2,517\n\n\n22.488\n\n\n9.675\n\n\n0.591\n\n\n-0.556\n\n\n0.193\n\n\n\n\nSPXL_r\n\n\n3\n\n\n2,517\n\n\n0.001\n\n\n0.033\n\n\n-1.525\n\n\n20.383\n\n\n0.001\n\n\n\n\nBAC_r\n\n\n4\n\n\n2,517\n\n\n0.001\n\n\n0.019\n\n\n-0.070\n\n\n9.849\n\n\n0.0004\n\n\n\n\n\n\n\n\nThere are a total of 2517 observations in total. Prices and log returns of both the financial instruments are asymmetric. SPXL daily prices are extremely skewed to the right, while SPXL returns are extremely skewed to the left. Moreover, BAC daily prices are slightly skewed to the right while the returns are slightly skewed to the left. The excess kurtosis for the log returns of both assets show that returns are not normally distributed.\n\nTime series graphs and distribution plots"
  },
  {
    "objectID": "posts/garch-var/index.html#exploratory-data-analysis",
    "href": "posts/garch-var/index.html#exploratory-data-analysis",
    "title": "Financial Econometrics Final Project",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirstly, we check for weak stationarity by investigating the time series plots of log returns of both the financial instruments. Next, we conduct the Augmented Dickey Fuller (ADF) test to check for the presence of unit root.\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$SPXL_r\nDickey-Fuller = -13.314, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$BAC_r\nDickey-Fuller = -12.962, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nUpon visual inspection, it looks like both the log returns are stationary. Moreover, from the ADF test, we fail to reject the null hypothesis of non-stationarity at 1% confidence.\nIn order to look for time dependency and volatility clustering of log returns of SPLX and BAC, From the graphs, it is evident that both series display time varying volatility. Moreover, volatility clusters together. For better analysis, we need to model the nonconstant volatility.\n\n\n\n\n\nFrom the log return graphs, it is evident that both series display time varying volatility. Moreover, ACF plots of squared series of both stocks show significant serial correlation. For better analysis, we need to model time dependence and non-constant volatility."
  },
  {
    "objectID": "posts/garch-var/index.html#time-series-model",
    "href": "posts/garch-var/index.html#time-series-model",
    "title": "Financial Econometrics Final Project",
    "section": "Time Series Model",
    "text": "Time Series Model\n\nAR(1)-GARCH(1,1) Model\nIn order to model autocorelation and volatility clustering, we combine an AR(1) model that has a nonconstant conditional mean but a constant conditional variance with a GARCH(1,1) model that has conditional mean and the variance of data depends on the past. Additionally, in a GARCH model, conditional standard deviation exhibits more persistent periods of low and high volatility.\nThe \\(AR(1) - GARCH(1,1)\\) is model is as follows:\n\\[ X_t = \\mu + \\phi X_{t-1} + e_t, \\quad e_t =  \\sigma_{t}\\varepsilon_{t}, \\quad \\sigma^2_t = \\omega + \\alpha e^2_{t-1} + \\beta \\sigma^2_{t-1} \\] \\[ \\tilde X_t = \\tilde \\mu + \\tilde \\phi \\tilde X_{t-1} + \\tilde e_t, \\quad \\tilde e_t =  \\tilde \\sigma_{t}\\tilde \\varepsilon_{t}, \\quad \\tilde \\sigma^2_t = \\tilde \\omega + \\tilde \\alpha \\tilde e^2_{t-1} + \\tilde \\beta \\tilde \\sigma^2_{t-1} \\] where, \\(X_t\\) is the daily log returns of SPXL and \\(\\tilde X_t\\) is the daily log returns of BAC.\n\nspxl <- as.vector(daily_df$SPXL_r)\nbac <- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit <- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nbac_fit <- ugarchfit(bac_spec, data = bac)\n\n\n\nResidual Copula Modelling\nIn order to work with a portfolio of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (BAC), we need to model both the series as a copula to extract the dependence structure between the two financial instruments. In the project, we particulary study the dependent structure of the standardized residuals of an \\(AR(1) - GARCH(1,1)\\) model fitted to both the series.\nFirst, we fit a Student t distribution to both \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon_t\\) using the fitdistr() function from the MASS library.\n\n\n\n\n\n\n\n\nA t-distribution with df = 5 fits well to the SPXL and BAC residuals. The qqplot for both the instruments is a straight line except for a few outliers. Given that the total number of observations (n = 2517) is high enough, the outliers are a small fraction of the sample.\nSecond, we transform the residuals into marginal t - distributions and fit t-copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas. We select the best fit copula by minimizing AIC. We select t-copula as it minimizes AIC.\n\n\n\n\n# Fit t-Copula\nCt <- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n[1] -1516.139\n\n# Gaussian Copula\nCgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                          copula = normalCopula(dim = 2))\n\naic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n[1] -1411.647\n\n# Gumbel Copula\nC_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n[1] -1388.313\n\n# Clayton Copula\n\nC_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                      method = \"ml\")\nlog_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n[1] -1213.367\n\n# Frank Copula\n\nCfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\n[1] -1395.35"
  },
  {
    "objectID": "posts/garch-var/index.html#residual-analysis",
    "href": "posts/garch-var/index.html#residual-analysis",
    "title": "Financial Econometrics Final Project",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nWe conduct a residual analysis on the standardized residuals \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon\\) to check the fit of the \\(AR(1) + GARCH(1,1)\\) model.\nFirst, we inspect the standardized residuals and squared standardized residuals for autocorrelation. The figure below plots the acf() function for the four series.\n\n\n\n\n\nThe AR(1) + GARCH(1,1) model fits very well to SPXL and BAC. The standardized residuals and the squared standardized residuals of both the models show no significant serial correlation at 95% confidence.\nNext we look at the following box tests to check autocorrelation\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.002425    0.000386   6.2772 0.000000\nar1    -0.057589    0.022519  -2.5574 0.010546\nomega   0.000039    0.000005   7.2771 0.000000\nalpha1  0.234802    0.023531   9.9784 0.000000\nbeta1   0.735776    0.021292  34.5560 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.002425    0.000393   6.1642 0.000000\nar1    -0.057589    0.020538  -2.8040 0.005047\nomega   0.000039    0.000008   4.7664 0.000002\nalpha1  0.234802    0.035228   6.6651 0.000000\nbeta1   0.735776    0.030381  24.2180 0.000000\n\nLogLikelihood : 5725.413 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -4.5454\nBayes        -4.5338\nShibata      -4.5454\nHannan-Quinn -4.5412\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      2.049  0.1523\nLag[2*(p+q)+(p+q)-1][2]     2.124  0.1746\nLag[4*(p+q)+(p+q)-1][5]     2.964  0.4404\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.00449  0.9466\nLag[2*(p+q)+(p+q)-1][5]   0.75536  0.9121\nLag[4*(p+q)+(p+q)-1][9]   2.39145  0.8535\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3] 0.0006994 0.500 2.000  0.9789\nARCH Lag[5] 1.8924772 1.440 1.667  0.4955\nARCH Lag[7] 2.9168559 2.315 1.543  0.5294\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.4922\nIndividual Statistics:              \nmu     0.06813\nar1    0.03829\nomega  0.25543\nalpha1 0.74932\nbeta1  0.80370\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                     t-value      prob sig\nSign Bias           3.751292 0.0001799 ***\nNegative Sign Bias  0.946948 0.3437561    \nPositive Sign Bias  0.004954 0.9960476    \nJoint Effect       20.723582 0.0001201 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     132.0    6.129e-19\n2    30     157.1    1.510e-19\n3    40     178.8    8.634e-20\n4    50     187.7    4.062e-18\n\n\nElapsed time : 0.3075819 \n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000323   3.2023 0.001363\nar1     0.032520    0.021631   1.5034 0.132731\nomega   0.000018    0.000004   4.2020 0.000026\nalpha1  0.105469    0.015992   6.5953 0.000000\nbeta1   0.837323    0.026364  31.7607 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000313   3.3035 0.000955\nar1     0.032520    0.020844   1.5602 0.118709\nomega   0.000018    0.000007   2.5283 0.011462\nalpha1  0.105469    0.033066   3.1896 0.001425\nbeta1   0.837323    0.048064  17.4210 0.000000\n\nLogLikelihood : 6693.438 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.3146\nBayes        -5.3030\nShibata      -5.3146\nHannan-Quinn -5.3104\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.02462  0.8753\nLag[2*(p+q)+(p+q)-1][2]   0.56004  0.9483\nLag[4*(p+q)+(p+q)-1][5]   1.75862  0.7794\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.550  0.2131\nLag[2*(p+q)+(p+q)-1][5]     2.058  0.6047\nLag[4*(p+q)+(p+q)-1][9]     3.474  0.6789\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]   0.06871 0.500 2.000  0.7932\nARCH Lag[5]   1.27764 1.440 1.667  0.6527\nARCH Lag[7]   2.21876 2.315 1.543  0.6710\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  0.8988\nIndividual Statistics:              \nmu     0.02392\nar1    0.02143\nomega  0.24815\nalpha1 0.57061\nbeta1  0.34991\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           0.3416 0.7327015    \nNegative Sign Bias  3.0886 0.0020333 ***\nPositive Sign Bias  0.5373 0.5910849    \nJoint Effect       17.9995 0.0004399 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     77.00    6.080e-09\n2    30     89.57    4.243e-08\n3    40    105.92    4.275e-08\n4    50    114.09    4.159e-07\n\n\nElapsed time : 0.438179 \n\n\nThe weighted versions of the Ljung-Box test and their p-values all indicate that the estimated \\(AR(1) + GARCH(1,1)\\) model for the conditional mean and variance is adequate for removing serial correlation from the series and squared series."
  },
  {
    "objectID": "posts/garch-var/index.html#applications-to-portfolio-risk-management",
    "href": "posts/garch-var/index.html#applications-to-portfolio-risk-management",
    "title": "Financial Econometrics Final Project",
    "section": "Applications to Portfolio Risk Management",
    "text": "Applications to Portfolio Risk Management\nIn this section, we apply the \\(AR(1) + GARCH(1,1)\\) along with t-Copula residuals to forecast one-step ahead Value-at-Risk at 99% level on the following portfolio consisting of SPXL and BAC with weight \\(\\rho\\):\n\\[\\rho X_{n + 1} + (1-\\rho) \\tilde X_{n+1}\\]\nwhere, \\(\\rho = \\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\\}\\)\nTo forecast VaR, we solve the following equation:\n\\[0.99 = P(\\rho X_{n + 1} + (1-\\rho) \\tilde X_{n+1} \\leq x | \\mathcal{F})\\] \\[0.99 = P(\\rho(\\mu + \\phi X_{n} + \\sigma_{n+1}\\varepsilon_{n+1}) + (1-\\rho) (\\tilde\\mu + \\tilde\\phi \\tilde X_{n} + \\tilde \\sigma_{n+1} \\tilde \\varepsilon_{n+1}) \\leq x | \\mathcal{F})\\]\nInitially, we forecast one step ahead \\(\\sigma_{n+1}\\) and \\(\\tilde \\sigma_{n + 1}\\) using the ugarchforecast() function. At this point, we have estimated all the unknowns in the \\(AR(1) + GARCH(1,1)\\) model. In order to solve the above equation numerically, we draw a random sample of size \\(b = 10000\\) from the fitted t-Copula and make transformations to get \\(\\varepsilon_i\\) and \\(\\tilde \\varepsilon_i\\), \\(i = 1, \\dots, b\\) Lastly, we calculate VaR at 99% by calculating the 99th quantile of the following sample\n\\[\\big{\\{}\\rho(\\mu + \\phi X_{n} + \\sigma_{n+1}\\varepsilon_{i}) + (1-\\rho) (\\tilde\\mu + \\tilde\\phi \\tilde X_{n} + \\tilde \\sigma_{n+1} \\tilde \\varepsilon_{i})\\big{\\}}_{i = 1}^b\\] In order to see how risk depends on share of portfolio \\(\\rho\\), we conduct the empirical activity for \\(\\rho = 0.1, \\dots, 0.9\\).\n\n# Copula Simulation \n\nb <- 10000\nrho = Ct@estimate[1]\ndf = Ct@estimate[2]\nsimulate <- rCopula(b, tCopula(dim = 2, rho, df = df))\nspxl_marginal <- qt(simulate[, 1], df = spxl_res_t$estimate[3])\nbac_marginal <- qt(simulate[, 2], df = bac_res_t$estimate[3])\n\n# dataframe with transformed marginals\nsim <- cbind(spxl_marginal, bac_marginal)\n\n# Forecasting sigmas\n\nspxl_pred <- ugarchforecast(spxl_fit, n.ahead = 1)\nbac_pred <- ugarchforecast(bac_fit, n.ahead = 1)\nspxl_sigma <- sigma(spxl_pred) # sigma_{t + 1}\nbac_sigma <- sigma(bac_pred) # sigma_{t + 1}\n\n# Vectorize parameters\ns_mu <- rep(as.numeric(spxl_fit@fit$coef[1]), b)\ns_ar <- rep(as.numeric(spxl_fit@fit$coef[2]), b)\ns_last <- rep(spxl[length(spxl)], b)\ns_sd <- rep(spxl_sigma, b)\n\nb_mu <- rep(as.numeric(bac_fit@fit$coef[1]), b)\nb_ar <- rep(as.numeric(bac_fit@fit$coef[2]), b)\nb_last <- rep(bac[length(bac)], b)\nb_sd <- rep(bac_sigma, b)\n\n\ns <- s_mu + (s_ar * s_last) + (s_sd * sim[, 1])\nba <- b_mu + (b_ar * b_last) + (b_sd * sim[, 2])\npred_sample <- cbind(s, ba)\nrhos <- seq(0.1, 0.9, by = 0.1)\nvar_sample <- data.frame()\n\nalpha <-  0.01 # 99th percentile\n\nportfolio_value <- 1000000\n\nfor (i in rhos){\n  \n  s_rho <- s * rep(i, b)\n  b_rho <- ba * rep(1 - i, b)\n  \n  sample <-  s_rho + b_rho\n  q <- as.numeric(quantile(sample, alpha))\n  VaR <- - portfolio_value * q\n  var_sample <- rbind(var_sample, VaR)\n}\ncolnames(var_sample) <- c(\"VaR ($)\")\nrow.names(var_sample) <- c(\"p = 0.1\", \"p = 0.2\", \"p = 0.3\", \"p = 0.4\", \n                           \"p = 0.5\", \"p = 0.6\", \"p = 0.7\", \"p = 0.8\", \n                           \"p = 0.9\")\nvar_sample\n\n          VaR ($)\np = 0.1  56868.64\np = 0.2  64760.15\np = 0.3  71810.30\np = 0.4  80322.54\np = 0.5  89308.00\np = 0.6  98992.82\np = 0.7 108932.23\np = 0.8 118802.90\np = 0.9 128930.28\n\n\nThe table reports the Value-at-Risk at 99% level for a portfolio value of $1,000,000. It is evident from the table that as SPXL’s proportion in the portfolio increases (\\(\\rho\\)), the Value-at-Risk increases. These results are in line with expectations as Direxion Daily S&P 500 Bull 3X Shares is a 300% leveraged instrument based on the S&P 500 index. When \\(\\rho = 0.2\\), there is a 1% percent chance that the loss would be greater than 6.4760151^{4} the next day on a $1,000,000 investment. Similarly, When \\(\\rho = 0.9\\), there is a 1% percent chance that the loss would be greater than 1.2893028^{5} the next day on a $1,000,000 investment."
  },
  {
    "objectID": "posts/garch-var/index.html#appendix",
    "href": "posts/garch-var/index.html#appendix",
    "title": "Financial Econometrics Final Project",
    "section": "Appendix",
    "text": "Appendix\nR code:\n\n# Importing Packages\n\nlibrary(quantmod)\nlibrary(xts)\nlibrary(gridExtra)\nlibrary(MASS)\nlibrary(fGarch)\nlibrary(sn)\nlibrary(copula)\nlibrary(ks)\nlibrary(stargazer)\nlibrary(patchwork)\nlibrary(rugarch)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(tseries)\n\n\n# Assigning Date\n\nstart <- as.Date(\"2012-11-09\")\nend <- as.Date(\"2022-11-10\")\n\nspxl_df <- getSymbols(\"SPXL\", from = start, to = end, src = \"yahoo\",\n                      auto.assign = F)\nbac_df <- getSymbols(\"BAC\", from = start, to = end, src = \"yahoo\",\n                     auto.assign = F)\n\n\ndaily_df <- cbind(spxl_df$SPXL.Adjusted, bac_df$BAC.Adjusted)\n\ncolnames(daily_df) <- c(\"SPXL\", \"BAC\")\n\ndaily_df$SPXL_r <- diff(log(daily_df$SPXL))\ndaily_df$BAC_r <- diff(log(daily_df$BAC))\n\ndaily_df <- na.omit(daily_df)\n\nstargazer(describe(daily_df, ranges = F), summary = F, type = \"html\", title = \"Table 1. Summary Statistics\")\nplot1 <- ggplot(spxl_df, aes(x = Index, y = SPXL.Adjusted))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot2 <- ggplot(bac_df, aes(x = Index, y = BAC.Adjusted))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\nplot1 / plot2\n\n# Plotting log returns\n\nplot_spxl_r <- ggplot(daily_df, aes(x = Index, y = SPXL_r))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot_bac_r <- ggplot(daily_df, aes(x = Index, y = BAC_r))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot_spxl_r / plot_bac_r\n\nadf.test(daily_df$SPXL_r)\nadf.test(daily_df$BAC_r)\n\n# ACF plots\nggacf <- function(x, ci=0.95, type=\"correlation\", xlab=\"Lag\", ylab=NULL,\n                  ylim=NULL, main=NULL, ci.col=\"blue\", lag.max=NULL) {\n  \n  x <- as.data.frame(x)\n  \n  x.acf <- acf(x, plot=F, lag.max=lag.max, type=type)\n  \n  ci.line <- qnorm((1 - ci) / 2) / sqrt(x.acf$n.used)\n  \n  d.acf <- data.frame(lag=x.acf$lag, acf=x.acf$acf)\n  \n  g <- ggplot(d.acf, aes(x=lag, y=acf)) +\n    geom_hline(yintercept=0) +\n    geom_segment(aes(xend=lag, yend=0)) +\n    geom_hline(yintercept=ci.line, color=ci.col, linetype=\"dashed\") +\n    geom_hline(yintercept=-ci.line, color=ci.col, linetype=\"dashed\") +\n    theme_bw() +\n    xlab(\"Lag\") +\n    ggtitle(ifelse(is.null(main), \"\", main)) +\n    if (is.null(ylab))\n      ylab(ifelse(type==\"partial\", \"PACF\", \"ACF\"))\n  else\n    ylab(ylab)\n  \n  g\n}\n\nspxl_acf <- ggacf(daily_df$SPXL_r, main = \"SPXL\")\nspxl_acf2 <- ggacf(daily_df$SPXL_r ** 2, main = \"SPXL Squared\")\nbac_acf <- ggacf(daily_df$BAC_r, main = \"BAC\")\nbac_acf2 <- ggacf(daily_df$BAC_r ** 2, main = \"BAC Squared\")\n\n(spxl_acf + spxl_acf2) / (bac_acf + bac_acf2)\n\nspxl <- as.vector(daily_df$SPXL_r)\nbac <- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit <- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                       variance.model=list(garchOrder=c(1,1)))\nbac_fit <- ugarchfit(bac_spec, data = bac)\n\n# Standardized Residuals\n\nspxl_res <- as.vector(residuals(spxl_fit, standardize = T))\nbac_res <- as.vector(residuals(bac_fit, standardize = T))\n\n# Fitting t-distribution\nspxl_res_t <- fitdistr(spxl_res, \"t\")\nbac_res_t <- fitdistr(bac_res, \"t\")\n\n# Fitting t-density plots\npar(mfrow = c(1,2))\nn <- length(spxl_res)\n\nx1 <- qt((1:n)/(n+1), df = 5)\nx2 <- qt((1:n)/(n+1), df = 5)\n\nqqplot(sort(spxl_res), x1, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"SPXL Residual t-plot, DF = 5\")\n\nqqplot(sort(bac_res), x2, xlab=\"Standardized Residuals\",\n       ylab=\"tquantiles\",\n       main = \"BAC Residual t-plot, DF = 5\")\n\n# Transform residues into uniform distribution\n\nspxl_uniform <- pt(spxl_res, df = spxl_res_t$estimate[3])\nbac_uniform <- pt(bac_res, df = bac_res_t$estimate[3])\n\nU_hat <- cbind(spxl_uniform, bac_uniform)\ncolnames(U_hat) <- c(\"SPXL_U\", \"BAC_U\")\n\ntau <- as.numeric(cor.test(spxl_uniform, bac_uniform, \n                           method=\"kendall\")$estimate)\nomega <- sin(tau * pi/2)\n\n\n# Fit t-Copula\nCt <- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n# Gaussian Copula\nCgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                              copula = normalCopula(dim = 2))\n\naic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n# Gumbel Copula\nC_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n# Clayton Copula\n\nC_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                       method = \"ml\")\nlog_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n# Frank Copula\n\nCfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\nspxl_res_acf <- ggacf(spxl_res, main = \"SPXL Std. Residuals\")\nspxl_res_acf2 <- ggacf(spxl_res ** 2, main = \"Squared SPXL Std. Residuals\")\nbac_res_acf <- ggacf(bac_res, main = \"BAC Std. Residuals\")\nbac_res_acf2 <- ggacf(bac_res ** 2, main = \"Squared BAC Std. Residuals\")\n\n(spxl_res_acf + spxl_res_acf2) / (bac_res_acf + bac_res_acf2)\n\nshow(spxl_fit)\n\nshow(bac_fit)\n\n# Copula Simulation \n\nb <- 10000\nrho = Ct@estimate[1]\ndf = Ct@estimate[2]\nsimulate <- rCopula(b, tCopula(dim = 2, rho, df = df))\nspxl_marginal <- qt(simulate[, 1], df = spxl_res_t$estimate[3])\nbac_marginal <- qt(simulate[, 2], df = bac_res_t$estimate[3])\n\n# dataframe with transformed marginals\nsim <- cbind(spxl_marginal, bac_marginal)\n\n# Forecasting sigmas\n\nspxl_pred <- ugarchforecast(spxl_fit, n.ahead = 1)\nbac_pred <- ugarchforecast(bac_fit, n.ahead = 1)\nspxl_sigma <- sigma(spxl_pred) # sigma_{t + 1}\nbac_sigma <- sigma(bac_pred) # sigma_{t + 1}\n\n# Vectorize parameters\ns_mu <- rep(as.numeric(spxl_fit@fit$coef[1]), b)\ns_ar <- rep(as.numeric(spxl_fit@fit$coef[2]), b)\ns_last <- rep(spxl[length(spxl)], b)\ns_sd <- rep(spxl_sigma, b)\n\nb_mu <- rep(as.numeric(bac_fit@fit$coef[1]), b)\nb_ar <- rep(as.numeric(bac_fit@fit$coef[2]), b)\nb_last <- rep(bac[length(bac)], b)\nb_sd <- rep(bac_sigma, b)\n\n\ns <- s_mu + (s_ar * s_last) + (s_sd * sim[, 1])\nba <- b_mu + (b_ar * b_last) + (b_sd * sim[, 2])\npred_sample <- cbind(s, ba)\nrhos <- seq(0.1, 0.9, by = 0.1)\nvar_sample <- data.frame()\n\nalpha <-  0.01 # 99th percentile\n\nportfolio_value <- 1000000\n\nfor (i in rhos){\n  \n  s_rho <- s * rep(i, b)\n  b_rho <- ba * rep(1 - i, b)\n  \n  sample <-  s_rho + b_rho\n  q <- as.numeric(quantile(sample, alpha))\n  VaR <- - portfolio_value * q\n  var_sample <- rbind(var_sample, VaR)\n}\ncolnames(var_sample) <- c(\"VaR ($)\")\nrow.names(var_sample) <- c(\"p = 0.1\", \"p = 0.2\", \"p = 0.3\", \"p = 0.4\", \n                           \"p = 0.5\", \"p = 0.6\", \"p = 0.7\", \"p = 0.8\", \n                           \"p = 0.9\")\nvar_sample"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a senior-year honors student at Georgia State University, majoring in economics and mathematics with a deep interest in financial economics. I love working with numbers, and I am intrigued by the linkages between financial markets and global macro variables. Through my math and graduate-level economics courses, I have developed solid quantitative and programming skills.\nPreviously, I have worked as a research assistant in the Economics department at GSU, enhancing my data analysis and research skills. I was also associated with the Portfolio Management Team (PMT) at GSU as the Chief Economist. The PMT is a student-managed investment fund with an objective to outperform the S&P 500 Index on a risk-adjusted basis. This position gave me hands-on experience in analyzing macroeconomic trends and reporting them to other members."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\n\nApr 8, 2023\n\n\nHarlow Malloc\n\n\n0 min\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nTristan O’Malley\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Website"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nImpact Evaulation of the Smartcard Program using Synthetic Data\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nViraj R. Chordiya\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBalancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation\n\n\nA Comprehensive Study of SPXL ETF and BAC Stock for VaR Insights\n\n\n\n\n\n\nNov 30, 2022\n\n\nViraj Chordiya\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/garch-var/index.html",
    "href": "projects/garch-var/index.html",
    "title": "Financial Econometrics Final Project",
    "section": "",
    "text": "Code\nknitr::opts_chunk$set(fig.width=12, fig.height=8)"
  },
  {
    "objectID": "projects/garch-var/index.html#data",
    "href": "projects/garch-var/index.html#data",
    "title": "Balancing Risk and Reward: Copula-GARCH framework for Portfolio Optimiziation",
    "section": "Data",
    "text": "Data\nWe collect ten years daily OHLC data of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (NYSE: BAC). SPXL is a three times leveraged ETF mirroring the S&P 500 index. Hence, it provides a 300% of the S&P 500 index’s daily return. The data is collected from Yahoo finance for period 2012/11 - 2022/11 using the getSymbol() function from the quantmod library. The table represents the summary statistics of daily adjusted closing stock price, and log returns of both the instruments.\n\n\nCode\n# Assigning Date\n\nstart <- as.Date(\"2012-11-09\")\nend <- as.Date(\"2022-11-10\")\n\nspxl_df <- getSymbols(\"SPXL\", from = start, to = end, src = \"yahoo\",\n                       auto.assign = F)\nbac_df <- getSymbols(\"BAC\", from = start, to = end, src = \"yahoo\",\n                       auto.assign = F)\n\n\ndaily_df <- cbind(spxl_df$SPXL.Adjusted, bac_df$BAC.Adjusted)\n\ncolnames(daily_df) <- c(\"SPXL\", \"BAC\")\n\ndaily_df$SPXL_r <- diff(log(daily_df$SPXL))\ndaily_df$BAC_r <- diff(log(daily_df$BAC))\n\ndaily_df <- na.omit(daily_df)\n\n\n\n\nCode\nsummary_df <- describe(daily_df, ranges = F)\ncolnames(summary_df) <- c(\"Variable\", \"N\", \"Mean\", \"Std\", \"Skew\", \"Kurtosis\", \"S.E.\")\nsummary_df$Variable <- c(\"SPXL\", \"BAC\", \"SPXL Returns\", \"BAC Returns\")\ndatasummary_df(summary_df,\n               title = \"Table 1. Summary Statistics\")\n\n\n\n\nTable 1. Summary Statistics\n \n  \n     Variable \n    N \n    Mean \n    Std \n    Skew \n    Kurtosis \n    S.E. \n  \n \n\n  \n    SPXL \n    2517.00 \n    42.77 \n    31.46 \n    1.24 \n    0.85 \n    0.63 \n  \n  \n    BAC \n    2517.00 \n    22.13 \n    9.52 \n    0.59 \n    -0.56 \n    0.19 \n  \n  \n    SPXL Returns \n    2517.00 \n    0.00 \n    0.03 \n    -1.52 \n    20.38 \n    0.00 \n  \n  \n    BAC Returns \n    2517.00 \n    0.00 \n    0.02 \n    -0.07 \n    9.85 \n    0.00 \n  \n\n\n\n\n\nThere are a total of 2517 observations in total. Prices and log returns of both the financial instruments are asymmetric. SPXL daily prices are extremely skewed to the right, while SPXL returns are extremely skewed to the left. Moreover, BAC daily prices are slightly skewed to the right while the returns are slightly skewed to the left. The excess kurtosis for the log returns of both assets show that returns are not normally distributed.\n\nTime series graphs and distribution plots\n\n\nCode\nplot1 <- ggplot(spxl_df, aes(x = Index, y = SPXL.Adjusted))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") +\n  theme_bw()\n\nplot2 <- ggplot(bac_df, aes(x = Index, y = BAC.Adjusted))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") + \n  theme_bw()\n\nplot1 / plot2"
  },
  {
    "objectID": "projects/garch-var/index.html#exploratory-data-analysis",
    "href": "projects/garch-var/index.html#exploratory-data-analysis",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirstly, we check for weak stationarity by investigating the time series plots of log returns of both the financial instruments. Next, we conduct the Augmented Dickey Fuller (ADF) test to check for the presence of unit root.\n\n\nCode\n# Plotting log returns\n\nplot_spxl_r <- ggplot(daily_df, aes(x = Index, y = SPXL_r))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") + \n  theme_bw()\n\nplot_bac_r <- ggplot(daily_df, aes(x = Index, y = BAC_r))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") + \n  theme_bw()\n\nplot_spxl_r / plot_bac_r\n\n\n\n\n\n\n\nCode\nadf.test(daily_df$SPXL_r)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$SPXL_r\nDickey-Fuller = -13.314, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nCode\nadf.test(daily_df$BAC_r)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  daily_df$BAC_r\nDickey-Fuller = -12.962, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nUpon visual inspection, it looks like both the log returns are stationary. Moreover, from the ADF test, we fail to reject the null hypothesis of non-stationarity at 1% confidence.\nIn order to look for time dependency and volatility clustering of log returns of SPLX and BAC, From the graphs, it is evident that both series display time varying volatility. Moreover, volatility clusters together. For better analysis, we need to model the nonconstant volatility.\n\n\nCode\n# ACF plots\nggacf <- function(x, ci=0.95, type=\"correlation\", xlab=\"Lag\", ylab=NULL,\n                  ylim=NULL, main=NULL, ci.col=\"blue\", lag.max=NULL) {\n\n    x <- as.data.frame(x)\n\n    x.acf <- acf(x, plot=F, lag.max=lag.max, type=type)\n\n    ci.line <- qnorm((1 - ci) / 2) / sqrt(x.acf$n.used)\n\n    d.acf <- data.frame(lag=x.acf$lag, acf=x.acf$acf)\n\n    g <- ggplot(d.acf, aes(x=lag, y=acf)) +\n        geom_hline(yintercept=0) +\n        geom_segment(aes(xend=lag, yend=0)) +\n        geom_hline(yintercept=ci.line, color=ci.col, linetype=\"dashed\") +\n        geom_hline(yintercept=-ci.line, color=ci.col, linetype=\"dashed\") +\n        theme_bw() +\n        xlab(\"Lag\") +\n        ggtitle(ifelse(is.null(main), \"\", main)) +\n        if (is.null(ylab))\n            ylab(ifelse(type==\"partial\", \"PACF\", \"ACF\"))\n        else\n            ylab(ylab)\n\n    g\n}\n\nspxl_acf <- ggacf(daily_df$SPXL_r, main = \"SPXL\")\nspxl_acf2 <- ggacf(daily_df$SPXL_r ** 2, main = \"SPXL Squared\")\nbac_acf <- ggacf(daily_df$BAC_r, main = \"BAC\")\nbac_acf2 <- ggacf(daily_df$BAC_r ** 2, main = \"BAC Squared\")\n\n(spxl_acf + spxl_acf2) / (bac_acf + bac_acf2)\n\n\n\n\n\nFrom the log return graphs, it is evident that both series display time varying volatility. Moreover, ACF plots of squared series of both stocks show significant serial correlation. For better analysis, we need to model time dependence and non-constant volatility."
  },
  {
    "objectID": "projects/garch-var/index.html#time-series-model",
    "href": "projects/garch-var/index.html#time-series-model",
    "title": "Balancing Risk and Reward: Copula-GARCH framework for Portfolio Optimiziation",
    "section": "Time Series Model",
    "text": "Time Series Model\n\nAR(1)-GARCH(1,1) Model\nIn order to model autocorelation and volatility clustering, we combine an AR(1) model that has a nonconstant conditional mean but a constant conditional variance with a GARCH(1,1) model that has conditional mean and the variance of data depends on the past. Additionally, in a GARCH model, conditional standard deviation exhibits more persistent periods of low and high volatility.\nThe \\(AR(1) - GARCH(1,1)\\) is model is as follows:\n\\[ X_t = \\mu + \\phi X_{t-1} + e_t, \\quad e_t =  \\sigma_{t}\\varepsilon_{t}, \\quad \\sigma^2_t = \\omega + \\alpha e^2_{t-1} + \\beta \\sigma^2_{t-1} \\] \\[ \\tilde X_t = \\tilde \\mu + \\tilde \\phi \\tilde X_{t-1} + \\tilde e_t, \\quad \\tilde e_t =  \\tilde \\sigma_{t}\\tilde \\varepsilon_{t}, \\quad \\tilde \\sigma^2_t = \\tilde \\omega + \\tilde \\alpha \\tilde e^2_{t-1} + \\tilde \\beta \\tilde \\sigma^2_{t-1} \\]\nwhere, \\(X_t\\) is the daily log returns of SPXL and \\(\\tilde X_t\\) is the daily log returns of BAC.\n\n\nCode\nspxl <- as.vector(daily_df$SPXL_r)\nbac <- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit <- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nbac_fit <- ugarchfit(bac_spec, data = bac)\n\n\n\n\nResidual Copula Modelling\nIn order to work with a portfolio of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (BAC), we need to model both the series as a copula to extract the dependence structure between the two financial instruments. In the project, we particulary study the dependent structure of the standardized residuals of an \\(AR(1) - GARCH(1,1)\\) model fitted to both the series.\nFirst, we fit a Student t distribution to both \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon_t\\) using the fitdistr() function from the MASS library.\n\n\nCode\n# Standardized Residuals\n\nspxl_res <- as.vector(residuals(spxl_fit, standardize = T))\nbac_res <- as.vector(residuals(bac_fit, standardize = T))\n\n# Fitting t-distribution\nspxl_res_t <- fitdistr(spxl_res, \"t\")\nbac_res_t <- fitdistr(bac_res, \"t\")\n\n\n\n\nCode\n# Fitting t-density plots\npar(mfrow = c(1,2))\nn <- length(spxl_res)\n\nx1 <- qt((1:n)/(n+1), df = 5)\nx2 <- qt((1:n)/(n+1), df = 5)\n\nqqplot(sort(spxl_res), x1, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"SPXL Residual t-plot, DF = 5\")\n\nqqplot(sort(bac_res), x2, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"BAC Residual t-plot, DF = 5\")\n\n\n\n\n\nA t-distribution with df = 5 fits well to the SPXL and BAC residuals. The qqplot for both the instruments is a straight line except for a few outliers. Given that the total number of observations (n = 2517) is high enough, the outliers are a small fraction of the sample.\nSecond, we transform the residuals into marginal t - distributions and fit t-copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas. We select the best fit copula by minimizing AIC. We select t-copula as it minimizes AIC.\n\n\nCode\n# Transform residues into uniform distribution\n\nspxl_uniform <- pt(spxl_res, df = spxl_res_t$estimate[3])\nbac_uniform <- pt(bac_res, df = bac_res_t$estimate[3])\n\nU_hat <- cbind(spxl_uniform, bac_uniform)\ncolnames(U_hat) <- c(\"SPXL_U\", \"BAC_U\")\n\ntau <- as.numeric(cor.test(spxl_uniform, bac_uniform, \n                           method=\"kendall\")$estimate)\nomega <- sin(tau * pi/2)\n\n\n\n\nCode\n# Fit t-Copula\nCt <- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n\n[1] -1516.14\n\n\nCode\n# Gaussian Copula\nCgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                          copula = normalCopula(dim = 2))\n\naic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n\n[1] -1411.647\n\n\nCode\n# Gumbel Copula\nC_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n\n[1] -1388.313\n\n\nCode\n# Clayton Copula\n\nC_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                      method = \"ml\")\nlog_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n\n[1] -1213.368\n\n\nCode\n# Frank Copula\n\nCfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\n\n[1] -1395.351"
  },
  {
    "objectID": "projects/garch-var/index.html#residual-analysis",
    "href": "projects/garch-var/index.html#residual-analysis",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Residual Analysis",
    "text": "Residual Analysis\nWe conduct a residual analysis on the standardized residuals \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon\\) to check the fit of the \\(AR(1) + GARCH(1,1)\\) model.\nFirst, we inspect the standardized residuals and squared standardized residuals for autocorrelation. The figure below plots the acf() function for the four series.\n\n\nCode\nspxl_res_acf <- ggacf(spxl_res, main = \"SPXL Std. Residuals\")\nspxl_res_acf2 <- ggacf(spxl_res ** 2, main = \"Squared SPXL Std. Residuals\")\nbac_res_acf <- ggacf(bac_res, main = \"BAC Std. Residuals\")\nbac_res_acf2 <- ggacf(bac_res ** 2, main = \"Squared BAC Std. Residuals\")\n\n(spxl_res_acf + spxl_res_acf2) / (bac_res_acf + bac_res_acf2)\n\n\n\n\n\nThe AR(1) + GARCH(1,1) model fits very well to SPXL and BAC. The standardized residuals and the squared standardized residuals of both the models show no significant serial correlation at 95% confidence.\nNext we look at the following box tests to check autocorrelation\n\n\nCode\nshow(spxl_fit)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.002425    0.000386   6.2774 0.000000\nar1    -0.057599    0.022519  -2.5578 0.010532\nomega   0.000039    0.000005   7.2772 0.000000\nalpha1  0.234792    0.023529   9.9789 0.000000\nbeta1   0.735780    0.021291  34.5576 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.002425    0.000393   6.1644 0.000000\nar1    -0.057599    0.020538  -2.8045 0.005039\nomega   0.000039    0.000008   4.7666 0.000002\nalpha1  0.234792    0.035225   6.6656 0.000000\nbeta1   0.735780    0.030380  24.2194 0.000000\n\nLogLikelihood : 5725.413 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -4.5454\nBayes        -4.5338\nShibata      -4.5454\nHannan-Quinn -4.5412\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      2.051  0.1521\nLag[2*(p+q)+(p+q)-1][2]     2.125  0.1742\nLag[4*(p+q)+(p+q)-1][5]     2.965  0.4401\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                   0.004507  0.9465\nLag[2*(p+q)+(p+q)-1][5]  0.755387  0.9121\nLag[4*(p+q)+(p+q)-1][9]  2.391474  0.8535\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3] 0.0007028 0.500 2.000  0.9789\nARCH Lag[5] 1.8924935 1.440 1.667  0.4955\nARCH Lag[7] 2.9168770 2.315 1.543  0.5294\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  1.4917\nIndividual Statistics:              \nmu     0.06813\nar1    0.03829\nomega  0.25524\nalpha1 0.74886\nbeta1  0.80326\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                     t-value      prob sig\nSign Bias           3.751262 0.0001799 ***\nNegative Sign Bias  0.946854 0.3438041    \nPositive Sign Bias  0.004948 0.9960528    \nJoint Effect       20.723476 0.0001202 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     132.0    6.129e-19\n2    30     157.1    1.510e-19\n3    40     178.8    8.634e-20\n4    50     187.7    4.062e-18\n\n\nElapsed time : 0.519062 \n\n\nCode\nshow(bac_fit)\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000323   3.2023 0.001363\nar1     0.032518    0.021631   1.5033 0.132758\nomega   0.000018    0.000004   4.2020 0.000026\nalpha1  0.105468    0.015991   6.5953 0.000000\nbeta1   0.837325    0.026363  31.7610 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000313   3.3035 0.000955\nar1     0.032518    0.020844   1.5601 0.118735\nomega   0.000018    0.000007   2.5283 0.011463\nalpha1  0.105468    0.033066   3.1896 0.001425\nbeta1   0.837325    0.048064  17.4211 0.000000\n\nLogLikelihood : 6693.436 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.3146\nBayes        -5.3030\nShibata      -5.3146\nHannan-Quinn -5.3104\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.02465  0.8753\nLag[2*(p+q)+(p+q)-1][2]   0.56006  0.9483\nLag[4*(p+q)+(p+q)-1][5]   1.75866  0.7794\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.550  0.2131\nLag[2*(p+q)+(p+q)-1][5]     2.058  0.6047\nLag[4*(p+q)+(p+q)-1][9]     3.474  0.6789\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.0687 0.500 2.000  0.7932\nARCH Lag[5]    1.2776 1.440 1.667  0.6527\nARCH Lag[7]    2.2188 2.315 1.543  0.6710\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  0.8988\nIndividual Statistics:              \nmu     0.02392\nar1    0.02143\nomega  0.24815\nalpha1 0.57062\nbeta1  0.34991\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           0.3415 0.7327322    \nNegative Sign Bias  3.0886 0.0020331 ***\nPositive Sign Bias  0.5373 0.5910826    \nJoint Effect       17.9996 0.0004399 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     77.00    6.080e-09\n2    30     90.07    3.554e-08\n3    40    105.92    4.275e-08\n4    50    114.09    4.159e-07\n\n\nElapsed time : 0.2202091 \n\n\n\n\nCode\nbac_fit\n\n\n\n*---------------------------------*\n*          GARCH Model Fit        *\n*---------------------------------*\n\nConditional Variance Dynamics   \n-----------------------------------\nGARCH Model : sGARCH(1,1)\nMean Model  : ARFIMA(1,0,0)\nDistribution    : norm \n\nOptimal Parameters\n------------------------------------\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000323   3.2023 0.001363\nar1     0.032518    0.021631   1.5033 0.132758\nomega   0.000018    0.000004   4.2020 0.000026\nalpha1  0.105468    0.015991   6.5953 0.000000\nbeta1   0.837325    0.026363  31.7610 0.000000\n\nRobust Standard Errors:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      0.001034    0.000313   3.3035 0.000955\nar1     0.032518    0.020844   1.5601 0.118735\nomega   0.000018    0.000007   2.5283 0.011463\nalpha1  0.105468    0.033066   3.1896 0.001425\nbeta1   0.837325    0.048064  17.4211 0.000000\n\nLogLikelihood : 6693.436 \n\nInformation Criteria\n------------------------------------\n                    \nAkaike       -5.3146\nBayes        -5.3030\nShibata      -5.3146\nHannan-Quinn -5.3104\n\nWeighted Ljung-Box Test on Standardized Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                    0.02465  0.8753\nLag[2*(p+q)+(p+q)-1][2]   0.56006  0.9483\nLag[4*(p+q)+(p+q)-1][5]   1.75866  0.7794\nd.o.f=1\nH0 : No serial correlation\n\nWeighted Ljung-Box Test on Standardized Squared Residuals\n------------------------------------\n                        statistic p-value\nLag[1]                      1.550  0.2131\nLag[2*(p+q)+(p+q)-1][5]     2.058  0.6047\nLag[4*(p+q)+(p+q)-1][9]     3.474  0.6789\nd.o.f=2\n\nWeighted ARCH LM Tests\n------------------------------------\n            Statistic Shape Scale P-Value\nARCH Lag[3]    0.0687 0.500 2.000  0.7932\nARCH Lag[5]    1.2776 1.440 1.667  0.6527\nARCH Lag[7]    2.2188 2.315 1.543  0.6710\n\nNyblom stability test\n------------------------------------\nJoint Statistic:  0.8988\nIndividual Statistics:              \nmu     0.02392\nar1    0.02143\nomega  0.24815\nalpha1 0.57062\nbeta1  0.34991\n\nAsymptotic Critical Values (10% 5% 1%)\nJoint Statistic:         1.28 1.47 1.88\nIndividual Statistic:    0.35 0.47 0.75\n\nSign Bias Test\n------------------------------------\n                   t-value      prob sig\nSign Bias           0.3415 0.7327322    \nNegative Sign Bias  3.0886 0.0020331 ***\nPositive Sign Bias  0.5373 0.5910826    \nJoint Effect       17.9996 0.0004399 ***\n\n\nAdjusted Pearson Goodness-of-Fit Test:\n------------------------------------\n  group statistic p-value(g-1)\n1    20     77.00    6.080e-09\n2    30     90.07    3.554e-08\n3    40    105.92    4.275e-08\n4    50    114.09    4.159e-07\n\n\nElapsed time : 0.2202091 \n\n\nThe weighted versions of the Ljung-Box test and their p-values all indicate that the estimated \\(AR(1) + GARCH(1,1)\\) model for the conditional mean and variance is adequate for removing serial correlation from the series and squared series."
  },
  {
    "objectID": "projects/garch-var/index.html#applications-to-portfolio-risk-management",
    "href": "projects/garch-var/index.html#applications-to-portfolio-risk-management",
    "title": "Balancing Risk and Reward: Copula-GARCH framework for Portfolio Optimiziation",
    "section": "Applications to Portfolio Risk Management",
    "text": "Applications to Portfolio Risk Management\nIn this section, we apply the \\(AR(1) + GARCH(1,1)\\) along with t-Copula residuals to forecast one-step ahead Value-at-Risk at 99% level on the following portfolio consisting of SPXL and BAC with weight \\(\\rho\\):\n\\[\\rho X_{n + 1} + (1-\\rho) \\tilde X_{n+1}\\]\nwhere, \\(\\rho = \\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\\}\\)\nTo forecast VaR, we solve the following equation:\n\\[0.99 = P(\\rho X_{n + 1} + (1-\\rho) \\tilde X_{n+1} \\leq x | \\mathcal{F})\\] \\[0.99 = P(\\rho(\\mu + \\phi X_{n} + \\sigma_{n+1}\\varepsilon_{n+1}) + (1-\\rho) (\\tilde\\mu + \\tilde\\phi \\tilde X_{n} + \\tilde \\sigma_{n+1} \\tilde \\varepsilon_{n+1}) \\leq x | \\mathcal{F})\\]\nInitially, we forecast one step ahead \\(\\sigma_{n+1}\\) and \\(\\tilde \\sigma_{n + 1}\\) using the ugarchforecast() function. At this point, we have estimated all the unknowns in the \\(AR(1) + GARCH(1,1)\\) model. In order to solve the above equation numerically, we draw a random sample of size \\(b = 10000\\) from the fitted t-Copula and make transformations to get \\(\\varepsilon_i\\) and \\(\\tilde \\varepsilon_i\\), \\(i = 1, \\dots, b\\) Lastly, we calculate VaR at 99% by calculating the 99th quantile of the following sample\n\\[\\big{\\{}\\rho(\\mu + \\phi X_{n} + \\sigma_{n+1}\\varepsilon_{i}) + (1-\\rho) (\\tilde\\mu + \\tilde\\phi \\tilde X_{n} + \\tilde \\sigma_{n+1} \\tilde \\varepsilon_{i})\\big{\\}}_{i = 1}^b\\] In order to see how risk depends on share of portfolio \\(\\rho\\), we conduct the empirical activity for \\(\\rho = 0.1, \\dots, 0.9\\).\n\n\nCode\n# Copula Simulation \n\nb <- 10000\nrho = Ct@estimate[1]\ndf = Ct@estimate[2]\nsimulate <- rCopula(b, tCopula(dim = 2, rho, df = df))\nspxl_marginal <- qt(simulate[, 1], df = spxl_res_t$estimate[3])\nbac_marginal <- qt(simulate[, 2], df = bac_res_t$estimate[3])\n\n# dataframe with transformed marginals\nsim <- cbind(spxl_marginal, bac_marginal)\n\n# Forecasting sigmas\n\nspxl_pred <- ugarchforecast(spxl_fit, n.ahead = 1)\nbac_pred <- ugarchforecast(bac_fit, n.ahead = 1)\nspxl_sigma <- sigma(spxl_pred) # sigma_{t + 1}\nbac_sigma <- sigma(bac_pred) # sigma_{t + 1}\n\n# Vectorize parameters\ns_mu <- rep(as.numeric(spxl_fit@fit$coef[1]), b)\ns_ar <- rep(as.numeric(spxl_fit@fit$coef[2]), b)\ns_last <- rep(spxl[length(spxl)], b)\ns_sd <- rep(spxl_sigma, b)\n\nb_mu <- rep(as.numeric(bac_fit@fit$coef[1]), b)\nb_ar <- rep(as.numeric(bac_fit@fit$coef[2]), b)\nb_last <- rep(bac[length(bac)], b)\nb_sd <- rep(bac_sigma, b)\n\n\ns <- s_mu + (s_ar * s_last) + (s_sd * sim[, 1])\nba <- b_mu + (b_ar * b_last) + (b_sd * sim[, 2])\npred_sample <- cbind(s, ba)\nrhos <- seq(0.1, 0.9, by = 0.1)\nvar_sample <- data.frame()\n\nalpha <-  0.01 # 99th percentile\n\nportfolio_value <- 1000000\n\nfor (i in rhos){\n  \n  s_rho <- s * rep(i, b)\n  b_rho <- ba * rep(1 - i, b)\n  \n  sample <-  s_rho + b_rho\n  q <- as.numeric(quantile(sample, alpha))\n  VaR <- - portfolio_value * q\n  var_sample <- rbind(var_sample, VaR)\n}\n\nvar_sample$`SPXL Portfolio Weight` <- c(\"p = 0.1\", \"p = 0.2\", \"p = 0.3\", \"p = 0.4\", \n                           \"p = 0.5\", \"p = 0.6\", \"p = 0.7\", \"p = 0.8\", \n                           \"p = 0.9\")\n\n\n\ncolnames(var_sample) <- c(\"VaR ($)\", \"SPXL Portfolio Weight\")\n\n# Reordering columns\nvar_sample <- var_sample[, c(\"SPXL Portfolio Weight\", \"VaR ($)\")]\n\ndatasummary_df(var_sample,\n               title = \"Table 2. One-Step Ahead VaR forecast for a $1 million portfolio with varying weights\")\n\n\n\n\nTable 2. One-Step Ahead VaR forecast for a $1 million portfolio with varying weights\n \n  \n     SPXL Portfolio Weight \n     VaR ($) \n  \n \n\n\n  \n    p = 0.1 \n    57392.09 \n  \n  \n    p = 0.2 \n    64754.87 \n  \n  \n    p = 0.3 \n    73878.49 \n  \n  \n    p = 0.4 \n    83306.74 \n  \n  \n    p = 0.5 \n    92833.22 \n  \n  \n    p = 0.6 \n    101688.88 \n  \n  \n    p = 0.7 \n    111384.48 \n  \n  \n    p = 0.8 \n    121083.05 \n  \n  \n    p = 0.9 \n    130927.88 \n  \n\n\n\n\n\nThe table reports the Value-at-Risk at 99% level for a portfolio value of $1,000,000. It is evident from the table that as SPXL’s proportion in the portfolio increases (\\(\\rho\\)), the Value-at-Risk increases. These results are in line with expectations as Direxion Daily S&P 500 Bull 3X Shares is a 300% leveraged instrument based on the S&P 500 index. When \\(\\rho = 0.2\\), there is a 1% percent chance that the loss would be greater than NA the next day on a $1,000,000 investment. Similarly, When \\(\\rho = 0.9\\), there is a 1% percent chance that the loss would be greater than NA the next day on a $1,000,000 investment."
  },
  {
    "objectID": "projects/garch-var/index.html#appendix",
    "href": "projects/garch-var/index.html#appendix",
    "title": "Financial Econometrics Final Project",
    "section": "Appendix",
    "text": "Appendix\nR code:\n\n# Importing Packages\n\nlibrary(quantmod)\nlibrary(xts)\nlibrary(gridExtra)\nlibrary(MASS)\nlibrary(fGarch)\nlibrary(sn)\nlibrary(copula)\nlibrary(ks)\nlibrary(stargazer)\nlibrary(patchwork)\nlibrary(rugarch)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(tseries)\n\n\n# Assigning Date\n\nstart <- as.Date(\"2012-11-09\")\nend <- as.Date(\"2022-11-10\")\n\nspxl_df <- getSymbols(\"SPXL\", from = start, to = end, src = \"yahoo\",\n                      auto.assign = F)\nbac_df <- getSymbols(\"BAC\", from = start, to = end, src = \"yahoo\",\n                     auto.assign = F)\n\n\ndaily_df <- cbind(spxl_df$SPXL.Adjusted, bac_df$BAC.Adjusted)\n\ncolnames(daily_df) <- c(\"SPXL\", \"BAC\")\n\ndaily_df$SPXL_r <- diff(log(daily_df$SPXL))\ndaily_df$BAC_r <- diff(log(daily_df$BAC))\n\ndaily_df <- na.omit(daily_df)\n\nstargazer(describe(daily_df, ranges = F), summary = F, type = \"html\", title = \"Table 1. Summary Statistics\")\nplot1 <- ggplot(spxl_df, aes(x = Index, y = SPXL.Adjusted))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot2 <- ggplot(bac_df, aes(x = Index, y = BAC.Adjusted))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\nplot1 / plot2\n\n# Plotting log returns\n\nplot_spxl_r <- ggplot(daily_df, aes(x = Index, y = SPXL_r))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot_bac_r <- ggplot(daily_df, aes(x = Index, y = BAC_r))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Log Returns\") +   \n  xlab(\"Date\") + \n  ylab(\"Log Returns\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\")\n\nplot_spxl_r / plot_bac_r\n\nadf.test(daily_df$SPXL_r)\nadf.test(daily_df$BAC_r)\n\n# ACF plots\nggacf <- function(x, ci=0.95, type=\"correlation\", xlab=\"Lag\", ylab=NULL,\n                  ylim=NULL, main=NULL, ci.col=\"blue\", lag.max=NULL) {\n  \n  x <- as.data.frame(x)\n  \n  x.acf <- acf(x, plot=F, lag.max=lag.max, type=type)\n  \n  ci.line <- qnorm((1 - ci) / 2) / sqrt(x.acf$n.used)\n  \n  d.acf <- data.frame(lag=x.acf$lag, acf=x.acf$acf)\n  \n  g <- ggplot(d.acf, aes(x=lag, y=acf)) +\n    geom_hline(yintercept=0) +\n    geom_segment(aes(xend=lag, yend=0)) +\n    geom_hline(yintercept=ci.line, color=ci.col, linetype=\"dashed\") +\n    geom_hline(yintercept=-ci.line, color=ci.col, linetype=\"dashed\") +\n    theme_bw() +\n    xlab(\"Lag\") +\n    ggtitle(ifelse(is.null(main), \"\", main)) +\n    if (is.null(ylab))\n      ylab(ifelse(type==\"partial\", \"PACF\", \"ACF\"))\n  else\n    ylab(ylab)\n  \n  g\n}\n\nspxl_acf <- ggacf(daily_df$SPXL_r, main = \"SPXL\")\nspxl_acf2 <- ggacf(daily_df$SPXL_r ** 2, main = \"SPXL Squared\")\nbac_acf <- ggacf(daily_df$BAC_r, main = \"BAC\")\nbac_acf2 <- ggacf(daily_df$BAC_r ** 2, main = \"BAC Squared\")\n\n(spxl_acf + spxl_acf2) / (bac_acf + bac_acf2)\n\nspxl <- as.vector(daily_df$SPXL_r)\nbac <- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit <- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                       variance.model=list(garchOrder=c(1,1)))\nbac_fit <- ugarchfit(bac_spec, data = bac)\n\n# Standardized Residuals\n\nspxl_res <- as.vector(residuals(spxl_fit, standardize = T))\nbac_res <- as.vector(residuals(bac_fit, standardize = T))\n\n# Fitting t-distribution\nspxl_res_t <- fitdistr(spxl_res, \"t\")\nbac_res_t <- fitdistr(bac_res, \"t\")\n\n# Fitting t-density plots\npar(mfrow = c(1,2))\nn <- length(spxl_res)\n\nx1 <- qt((1:n)/(n+1), df = 5)\nx2 <- qt((1:n)/(n+1), df = 5)\n\nqqplot(sort(spxl_res), x1, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"SPXL Residual t-plot, DF = 5\")\n\nqqplot(sort(bac_res), x2, xlab=\"Standardized Residuals\",\n       ylab=\"tquantiles\",\n       main = \"BAC Residual t-plot, DF = 5\")\n\n# Transform residues into uniform distribution\n\nspxl_uniform <- pt(spxl_res, df = spxl_res_t$estimate[3])\nbac_uniform <- pt(bac_res, df = bac_res_t$estimate[3])\n\nU_hat <- cbind(spxl_uniform, bac_uniform)\ncolnames(U_hat) <- c(\"SPXL_U\", \"BAC_U\")\n\ntau <- as.numeric(cor.test(spxl_uniform, bac_uniform, \n                           method=\"kendall\")$estimate)\nomega <- sin(tau * pi/2)\n\n\n# Fit t-Copula\nCt <- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n# Gaussian Copula\nCgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                              copula = normalCopula(dim = 2))\n\naic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n# Gumbel Copula\nC_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n# Clayton Copula\n\nC_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                       method = \"ml\")\nlog_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n# Frank Copula\n\nCfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\nspxl_res_acf <- ggacf(spxl_res, main = \"SPXL Std. Residuals\")\nspxl_res_acf2 <- ggacf(spxl_res ** 2, main = \"Squared SPXL Std. Residuals\")\nbac_res_acf <- ggacf(bac_res, main = \"BAC Std. Residuals\")\nbac_res_acf2 <- ggacf(bac_res ** 2, main = \"Squared BAC Std. Residuals\")\n\n(spxl_res_acf + spxl_res_acf2) / (bac_res_acf + bac_res_acf2)\n\nshow(spxl_fit)\n\nshow(bac_fit)\n\n# Copula Simulation \n\nb <- 10000\nrho = Ct@estimate[1]\ndf = Ct@estimate[2]\nsimulate <- rCopula(b, tCopula(dim = 2, rho, df = df))\nspxl_marginal <- qt(simulate[, 1], df = spxl_res_t$estimate[3])\nbac_marginal <- qt(simulate[, 2], df = bac_res_t$estimate[3])\n\n# dataframe with transformed marginals\nsim <- cbind(spxl_marginal, bac_marginal)\n\n# Forecasting sigmas\n\nspxl_pred <- ugarchforecast(spxl_fit, n.ahead = 1)\nbac_pred <- ugarchforecast(bac_fit, n.ahead = 1)\nspxl_sigma <- sigma(spxl_pred) # sigma_{t + 1}\nbac_sigma <- sigma(bac_pred) # sigma_{t + 1}\n\n# Vectorize parameters\ns_mu <- rep(as.numeric(spxl_fit@fit$coef[1]), b)\ns_ar <- rep(as.numeric(spxl_fit@fit$coef[2]), b)\ns_last <- rep(spxl[length(spxl)], b)\ns_sd <- rep(spxl_sigma, b)\n\nb_mu <- rep(as.numeric(bac_fit@fit$coef[1]), b)\nb_ar <- rep(as.numeric(bac_fit@fit$coef[2]), b)\nb_last <- rep(bac[length(bac)], b)\nb_sd <- rep(bac_sigma, b)\n\n\ns <- s_mu + (s_ar * s_last) + (s_sd * sim[, 1])\nba <- b_mu + (b_ar * b_last) + (b_sd * sim[, 2])\npred_sample <- cbind(s, ba)\nrhos <- seq(0.1, 0.9, by = 0.1)\nvar_sample <- data.frame()\n\nalpha <-  0.01 # 99th percentile\n\nportfolio_value <- 1000000\n\nfor (i in rhos){\n  \n  s_rho <- s * rep(i, b)\n  b_rho <- ba * rep(1 - i, b)\n  \n  sample <-  s_rho + b_rho\n  q <- as.numeric(quantile(sample, alpha))\n  VaR <- - portfolio_value * q\n  var_sample <- rbind(var_sample, VaR)\n}\ncolnames(var_sample) <- c(\"VaR ($)\")\nrow.names(var_sample) <- c(\"p = 0.1\", \"p = 0.2\", \"p = 0.3\", \"p = 0.4\", \n                           \"p = 0.5\", \"p = 0.6\", \"p = 0.7\", \"p = 0.8\", \n                           \"p = 0.9\")\nvar_sample"
  },
  {
    "objectID": "projects/smartcard/index.html#program-theory-and-impact-theory-graph",
    "href": "projects/smartcard/index.html#program-theory-and-impact-theory-graph",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Program Theory and Impact Theory Graph",
    "text": "Program Theory and Impact Theory Graph\nThe program theory is that biometric authentication and secure payment systems can improve governance by reducing leakages in the delivery systems and reducing payment delays without adversely affecting program access.\nThe theory, based on previous research (World Bank 2003; Pritchett 2010, Reinikka and Svensson 2004; Programme Evaluation Organisation 2005; Niehaus and Sukhtankar 2013b) assumes that corruption and leakages are prevalent in government services in developing countries, leading to ineﬀiciencies in assistance delivery systems. By implementing biometric authentication and secure payment systems, the government can ensure that the correct individuals receive the correct payments and services. The Smartcards intervention has a two-fold effect. First, it changes the institutions responsible for managing the funds by eliminating a few intermediaries and moves the point of payment closer to the recipient. Second, it introduces biometric authentication which facilities transparency in the system by providing a secure, tamperproof record of who has received services and payments. This can help to reduce corruption and increase accountability.\nSimilar to Muralidharan et al. 2016, we consider two main dimensions of impact: payment logistics, and leakages. We also consider multiple scenarios surrounding each dimension. First, the payment logistics could improve or worsen. It could naturally improve the system by moving the point of payment closer to the village, or the program could slow down the process due to technical problems like malfunctioning authentication devices or absence of technical staff. Improving payment logistics will affect the wait times of beneficiaries to access the funds. Secondly, the program could reduce or fail to reduce leakages. Theoretically, smartcards should reduce payments to ghost recipients and forbid oﬀicials to collect payments in the name of real beneficiaries as the beneficiaries must be present to pass the biometric test. But there could be technical loopholes in the payment architecture that could disprove the claim of an absolute secure system. Moreover, the assistance programs’ access could improve or suffer. In case of NREGS, local oﬀicers decide projects and generate employment. Eliminating opportunities to seek rent may disincentivize oﬀicials and they might reduce access. On the other hand, the NREGS funds would directly go towards to the development projects that were originally intended thus creating rural assets and generating more employment.\n\n\n\nSmartcard Program Impact Theory Graph"
  },
  {
    "objectID": "projects/smartcard/index.html#logic-model",
    "href": "projects/smartcard/index.html#logic-model",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Logic model",
    "text": "Logic model\n\n\n\nSmartcard Program Logic Model\n\n\n\nInputs:\n\n\nFederal and state assistance funds: These funds are required to finance the implementation of the Smartcard intervention as well as NGREGS transfer payments.\nBiometric authentication technology: This technology is required to verify the identity of beneficiaries and ensure that payments are disbursed to the correct individuals.\nBanks: Banks are required to help manage the payment system and ensure that payments are securely and efficiently disbursed to beneficiaries.\nSecure payment systems: Secure payment systems are required to ensure that payments are not lost or stolen during the disbursement process.\nSmartcards: Smartcards are required to store the payment information of beneficiaries and facilitate the payment process.\nTechnical staff: Trained staff are required to collect and manage beneficiary information, conduct stress tests, and implement the payment system effectively and efficiently.\nSurvey support staff: Survey staff are required to conduct randomized promotion and conduct weekly surveys to collect information on variables of interest.\n\n\nActivities:\n\n\nTraining staff for data collection and management of payment systems effectively and efficiently\nCollecting beneficiary personal and biometric information to verify individuals.\nConducting stress tests on the digital payment system to ensure its smooth functioning.\nImplementation of secure payment systems to disburse payments to the correct individuals:\nConducting door-to-door random promotions and provide information about the program.\nConducting weekly surveys to collect data.\n\n\nOutputs:\n\n\nNumber of people using Smartcard technology:\nPercentage change in average wait times at payment centers.\nAmount of money credited for every beneficiary and payment type.\nAmount of money saved: The amount of money saved is a measure of the program’s efficiency and cost-effectiveness.\nChange in leakages and corruption in the system:\n\n\nOutcomes:\n\n\nReduced wait times: The Smartcard intervention is designed to reduce wait times at payment centers and payment delays which can save time and reduce the burden on beneficiaries.\nIncreased earnings for the beneficiaries: The Smartcard intervention is designed to improve the delivery of funds for welfare programs, which can increase the earnings of beneficiaries.\nIncreased efficiency: The Smartcard program will deliver reduce leakage, ensuring beneficiaries receive more funds in less time without allocating more budget to NREGS.\nImproved trust in government services: The Smartcard intervention is designed to improve the delivery of funds for welfare programs and ensure transparency in the payment system, which can improve trust in government."
  },
  {
    "objectID": "projects/smartcard/index.html#main-outcome",
    "href": "projects/smartcard/index.html#main-outcome",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Main outcome",
    "text": "Main outcome\nThe goal of this paper is to evaluate the smartcard program’s effect on governance. Good governance is critical to achieving the goals of development. Moreover, strong governance ensures the efficiency and smoothness of governmental processes in achieving goals directed towards public welfare. It means that the government can deliver what they promised to the public in a fair and efficient way. We expect the smartcard intervention to reduce leakages, corruption and wait times for the beneficiaries. This will lead to improved efficiency, increased earnings and greater trust in government welfare measures."
  },
  {
    "objectID": "projects/smartcard/index.html#measurement",
    "href": "projects/smartcard/index.html#measurement",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Measurement",
    "text": "Measurement\nOutcome: Better Governance\n\nImproved wait times\nReduction in corruption and abuse of power\nReduction in leakages in the payment system\nBetter implementation of governmental plans or improved effectiveness\nIncreased transparency\nBudget management\n\nWe think that the most important attributes of better governance are reduced wait times to access promised goods or services, reduction in leakages, and reduction in corruption and abuse of power. These aspects can be best quantified by observational and survey data. Leakages and delays in payments, crucial aspects of governance, should be the focus of the evaluation study. Corruption is also an important aspect of good governance, but it is our opinion that study participants might not be completely transparent about corruption related experiences. Budget management is subject to political ideology and economic situation. On the other hand, transparency can be diﬀicult to measure.\nAttribute 1: Reduction in Leakages\n\nMeasurable definition: Change in money value allocated and the money value actually received per person. Money value here is the rupee (or dollar) value actually received by the beneficiary. In other projects, money value can be the dollar value of the services received.\nIdeal measurement: Difference between the actual official amount of money allocated and the actual money received by the beneficiary.\nFeasible measurement: Difference between the muster-roll recorded amount of money allocated to the beneficiary and the money received by beneficiary based on surveys. Officials can manipulate muster rolls to under record work and pocket the difference. In an ideal world, we cannot know the true allocated value.\nMeasurement of program effect: If the promotion of the smartcard program is randomized, the leakage amount will reflect the local average treatment effect of the program on the compliers.\n\nAttribute 2: Reduction in Wait Time\n\nMeasurable definition: Number of days between completing and work and receiving the payment. Moreover, number of minutes spent at the point of service (PoS) to collect the amount.\nIdeal measurement: In an ideal setting, conduct a randomized trial on the entire population. Post treatment, survey all the people and measure the wait times people in each group experienced before receiving their full promised amount of money.\nFeasible measurement: Conduct baseline surveys before rolling out the smartcard program and randomizing promotions and conducting endline surveys after the intervention to record the wait times of people.\nMeasurement of program effect: Similar leakages, if the promotion is random, wait times would be the local average treatment effect of the smartcard program.\n\nAttribute 3: Reduction in Corruption and Abuse of Power\n\nMeasurable definition: Change in number of corruption incidences and average money value of corruption.\nIdeal measurement: In an ideal world, conduct a randomized trial on the entire population and measure the corruption incidences and per capita corruption amount for both the groups. The difference in these two parameters would be the program effect.\nFeasible measurement: Conduct a small-scale randomized promotion of the smartcard program and measure the number of corruption incidences and average corruption value.\nMeasurement of program effect: As the randomized promotion is a valid instrument, the feasible measures would reflect the local average treatment effect of the smartcard program."
  },
  {
    "objectID": "projects/smartcard/index.html#causal-theory",
    "href": "projects/smartcard/index.html#causal-theory",
    "title": "PMAP 8521: Final Project",
    "section": "Causal Theory",
    "text": "Causal Theory\nWe make causal claims that the Smartcard intervention will lead to reduced leakages, wait times, and corruption for beneficiary households. In earlier cash transfer mechanisms, the funds were released by the government and flowed through multiple intermediaries to reach the beneficiary. The system was prone to leakages and corruption. But in the new smartcard system, the worker can directly receive the funds with just a single third-party processing all the transactions. This leads to less leakages in the system as people cannot game it by withdrawing money in the name of ghost recipients. Moreover, oﬀicers have fewer chances to engage in corruption as funds are digitally transferred to the end user’s bank account. Moreover, the smartcard intervention will improve the payment logistics by reducing wait times and delays in payment collection. Smartcard holders will not be bothered by cash shortages at the PoS as money would be directly deposited into their accounts. Additionally, the biometric cards would aid in quicker verifications at the PoS and reduce wait times in queue.\nA host of household level characteristics influence enrollment into the smartcard program, reduction in leakages, wait times and corruption events. Some of the observable covariates include location, number of children in the family, household literacy rate, household religion, assets like farmland and others. Controlling for these covariates is a possible solution, but it does not paint the full causal picture. As the smartcard program is universally accessible, it leads to self-selection into the program. Moreover, the unobserved household and governmental characteristics lead to omitted variable bias. To tackle these challenges, we introduce the instrumental variable design by using randomized promotion as an instrument. We prove the validity of the instrument in the subsequent section. Randomized promotion design closes all the backdoors and provides a local average treatment effect for the compliers that enroll into the smartcard program after receiving the promotion."
  },
  {
    "objectID": "projects/smartcard/index.html#hypotheses",
    "href": "projects/smartcard/index.html#hypotheses",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Hypotheses",
    "text": "Hypotheses\nWe formulate the following hypothesis and test it in the subsequent sections:\n\\(\\textbf{H1}\\): The Smartcard program causes leakages to decrease.\n\\(\\textbf{H2}\\): The Smartcard program causes payment delays (in number of days to receive payment) to decrease.\n\\(\\textbf{H3}\\): The Smartcard program causes waiting time in queue (in minutes) to decrease.\n\\(\\textbf{H4}\\): The Smartcard program reduces incidences of corruption."
  },
  {
    "objectID": "projects/smartcard/index.html#identification-strategy",
    "href": "projects/smartcard/index.html#identification-strategy",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Identification Strategy",
    "text": "Identification Strategy\nSimilar to Muralidharan et al. 2016, we assert causal claims that the Smartcard intervention will lead to reduced leakages, wait times, and corruption for beneficiary households. In earlier leakage-prone systems, the funds were released by the government and flowed through multiple intermediaries to reach the beneficiary. But in the new Smartcard system, the worker can directly receive funds with just a single third-party processing all the transactions. This leads to fewer leakages in the system as people cannot game it by withdrawing money in the name of ghost recipients. Moreover, oﬀicers have fewer chances to engage in corruption as funds are digitally transferred to the end user’s bank account. Further, the Smartcard intervention will improve payment logistics by reducing wait times and delays in payment collection. Smartcard holders will not be bothered by cash shortages at the PoS as money would be directly deposited into their accounts. Finally, the biometric cards would aid in quicker verifications at the PoS and reduce wait times in queues.\nA host of household-level characteristics influence enrollment into the Smartcard program, reduction in leakages, wait times and corruption events. Some of the observable covariates include location, family size, household literacy rate, religion, assets, and others. Controlling for these covariates is a possible solution, but it does not paint the full causal picture. The NREGS program has a universal access and hence it is fair to provide universal access to the Smartcard program that facilitates the NREGS welfare payments. In this scenario, we suspect that households that enroll into the Smartcard intervention are fundamentally different than the households that do not enroll. For instance, poor households with fewer job opportunities or literate households would be more likely to enroll into the program. There exists a host of household and geographic characteristics that influence enrollment into the program and the outcomes. The universal nature of the program leads to self-selection bias. Additionally, the unobserved household and governmental characteristics lead to omitted variable bias. To tackle these challenges, we introduce the instrumental variable design by using randomized promotion as an instrument. Randomized promotion design closes all the backdoors and provides a local average treatment effect for the compliers that enroll in the Smartcard program after receiving the promotion.\nThe instrumental variable approach isolates the causal effect of the program on the outcome by using the variation in the treatment induced by the instrument. By comparing the outcomes of households who received treatment through the instrument with those who did not receive treatment through the instrument, we can estimate the causal effect of the program on the outcomes of choice.\nSpecifically, the randomized promotion ensures that the distribution of observable and unobservable characteristics between the treatment and control groups is balanced, reducing the potential for selection bias. Furthermore, the instrumental variable approach addresses endogeneity by focusing on the exogenous variation in the randomized promotion, rather than the endogenous relationship between the program and outcomes, thus isolating the causal effect of the program on the outcomes of interest. This approach allows us to obtain an unbiased estimate of the causal effect of the program, even in the presence of selection bias and endogeneity.\nOur identification strategy takes the form of two-staged least squares (2SLS) instrumental variable. In the first stage, we estimate the effect of randomized promotion on the endogenous smartcard enrollment. Hence, we estimate:\n\\[\n\\text{Smartcard}= \\beta_{0} + \\beta \\text{Promotion} + \\gamma X + u\n\\] where \\(\\text{Smartcard}\\) is a binary indicator of whether a household enrolled into the program, and \\(X\\) is a vector of covariates.\nThe general form of the second stage least squares is as follows: \\[\n\\text{Outcome}_i =  \\beta_{0_i} + \\beta_i \\hat{\\text{Smartcard}} + \\gamma_i X + \\epsilon_i\\]\nWhere \\[\\text{Outcome }_i = \\{\\text{Leakage}, \\text{Queue Wait Time }, \\text{Payment Delay}, \\text{Corruption} \\}\\]\nOverall, the instrumental variable approach using randomized promotion provides a rigorous and credible method for estimating the causal effects of the smartcard program on the outcomes of interest, while also addressing potential issues of selection bias and endogeneity.\nA DAG for the outcome is as follows:\n\n\nCode\n# Outcome_i DAG\n\niv_dag <- dagitty('dag {\nbb=\"-1.129,-1.666,1.129,1.666\"\n\"# Children\" [pos=\"-0.100,-0.416\"]\n\"# Female Members\" [pos=\"-0.295,-0.335\"]\n\"Household Has Farm\" [pos=\"0.258,-0.469\"]\n\"Household Head (HH) Age\" [pos=\"0.440,1.076\"]\n\"Household Head (HH) Gender\" [pos=\"0.690,0.902\"]\n\"Household Literacy Rate\" [pos=\"0.805,-0.106\"]\n\"Random Promotion (IV)\" [pos=\"-1.005,0.524\"]\nHindu [pos=\"-0.187,0.991\"]\nOutcome_i [outcome,pos=\"0.896,0.466\"]\nLocation [pos=\"-0.483,-0.189\"]\nMuslim [pos=\"0.130,1.169\"]\nSmartcard [exposure,pos=\"-0.689,0.514\"]\n\"# Children\" -> Outcome_i\n\"# Children\" -> Smartcard\n\"# Female Members\" -> Outcome_i\n\"# Female Members\" -> Smartcard\n\"Household Has Farm\" -> Smartcard\n\"Household Head (HH) Age\" -> Outcome_i\n\"Household Head (HH) Age\" -> Smartcard\n\"Household Head (HH) Gender\" -> Outcome_i\n\"Household Head (HH) Gender\" -> Smartcard\n\"Household Literacy Rate\" -> Outcome_i\n\"Household Literacy Rate\" -> Smartcard\n\"Random Promotion (IV)\" -> Smartcard\nHindu -> Outcome_i\nHindu -> Smartcard\nLocation -> Outcome_i\nLocation -> Smartcard\nMuslim -> Outcome_i\nMuslim -> Smartcard\nSmartcard -> Outcome_i\n}\n\n')\n\nggdag_status(iv_dag,  text = FALSE, use_labels = \"name\") +\n  guides(fill = \"none\", color = \"none\") +\n  theme_dag()"
  },
  {
    "objectID": "projects/smartcard/index.html#data",
    "href": "projects/smartcard/index.html#data",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Data",
    "text": "Data\nWe propose to use a mix of administrative and survey data to estimate the causal link between the program and the outcomes of interest. We plan to conduct weekly survey for a 4-week period to measure the household level weekly payments received, time spent (in minutes) at the PoS, days between work is completed and payment is received, and corruption incidences. We will compare the weekly payments received to the official disbursement data to estimate the household level leakage in the system, the amount lost in inefficiencies or corruptions. Alongside, we will collect data on household characteristics like number of children, number of females in the household, household literacy rate, religion, assets like farmland, and household head age and gender.\n\n\nCode\nset.seed(1234)\n\nn_household <- 100000\nn_panchayats <- 500\niv_data <- tibble(\n  household_ID = factor(1:n_household),\n  panchayat_ID = factor(sample(1:n_panchayats, n_household, replace = TRUE)),\n  state_ID = factor(sample(1:3, n_household, replace = TRUE))) %>%\n  mutate(\n  off_week_1 = rlnormTrunc(n_household, meanlog = log(500), \n                              sdlog =0.6, min = 0, max = 4000),\n  off_week_2 = rlnormTrunc(n_household, meanlog = log(450), \n                              sdlog =0.6, min = 0, max = 4000),\n  off_week_3 = rlnormTrunc(n_household, meanlog = log(450), \n                              sdlog =0.6, min = 0, max = 4000),\n  off_week_4 = rlnormTrunc(n_household, meanlog = log(400), \n                              sdlog =0.6, min = 0, max = 4000),\n  week_1 = rlnormTrunc(n_household, meanlog = log(450), \n                          sdlog =0.6, min = 0, max = 3000),\n  week_2 = rlnormTrunc(n_household, meanlog = log(400), \n                          sdlog =0.6, min = 0, max = 3400),\n  week_3 = rlnormTrunc(n_household, meanlog = log(375), \n                          sdlog =0.6, min = 0, max = 3000),\n  week_4 = rlnormTrunc(n_household, meanlog = log(350),\n                          sdlog =0.6, min = 0, max = 3000)) %>% \n  \n  mutate(off_total_amount = off_week_1 + off_week_2 + off_week_3 + off_week_4 +\n           rnorm(n_household, 0, 10),\n         total_amount = week_1 + week_2 + week_3 + week_4 + rnorm(n_household, 0, 10),\n         leakage_base = off_total_amount - total_amount) \n  \npromotion_panchayats <- sample(1:n_panchayats, round(n_panchayats / 2), replace = FALSE)\n\niv_data <- iv_data %>% \n  # Randomized Promotion\n  mutate(promotion = ifelse(panchayat_ID %in% promotion_panchayats, TRUE, FALSE)) %>% \n  # Potential Confounders\n  mutate(location = panchayat_ID,\n         num_members = round(rtruncnorm(n_household, a = 0, b = 20, \n                                  mean = 4, sd = 1)),\n         farm = rbinom(n_household, 1, 0.1),\n         children = round(rtruncnorm(n_household, a = 0, b = 10, \n                                  mean = 2, sd = 1)),\n         household_literacy_rate = rtruncnorm(n_household, a = 0, b = 1, \n                                              mean = 0.5, sd = 0.2),\n         female_member = round(rtruncnorm(n_household, a = 0, b = 10, \n                                  mean = 1, sd = 1)),\n         hindu = rbinom(n_household, 1, 0.8),\n         muslim = ifelse(hindu == 1, 0, rbinom(1, 1, 0.1)),\n         other_religion = ifelse(hindu == 1 | muslim == 1, 0, 1),\n         hh_age = round(rtruncnorm(n_household, a = 18, b = 80, \n                                  mean = 35, sd = 10)),\n         hh_male = rbinom(n_household, 1, 0.9)\n         ) %>% \n  mutate(promotion_effect = promotion * 6,\n         # Agricultural Locations\n         farm_effect = -0.09 * farm,\n         children_effect = 0.03 * children,\n         literacy_effect = - 0.01 * household_literacy_rate,\n         female_effect = 0.02 * female_member,\n         hindu_effect = 0.03 * hindu,\n         muslim_effect = ifelse(location %in% c(2, 6, 7), 0.005 * muslim, -0.01 * muslim),\n         other_effect = - 0.02 * other_religion,\n         hh_age_effect = 0.02 * hh_age,\n         hh_male_effect = 0.04 * hh_male,\n         smartcard_score = 0 + promotion_effect + \n           farm_effect + children_effect +\n           literacy_effect + female_effect + hindu_effect + \n           muslim_effect + other_effect + hh_age_effect + hh_male_effect +\n           rnorm(n_household, 0, 0.2),\n         smartcard_probability = scales::rescale(smartcard_score, to = c(0.1, 1)),\n         smartcard = rbinom(n_household, 1, smartcard_probability)) %>% \n  mutate(smartcard_leakage_effect = - 60 * smartcard)\n\niv_data <- iv_data %>% \n  mutate(leakage = leakage_base + smartcard_leakage_effect + + (100 * children_effect) +\n           + (100 * farm_effect) +\n           (100 *literacy_effect) + (100 * female_effect) + \n           (100 * hindu_effect) + (100 * muslim_effect) + (100 * other_effect) + \n           (100 * hh_age_effect) + (hh_male_effect) +\n           rnorm(n_household, 0, 20)) %>% \n  mutate(leakage_difference = leakage - leakage_base) %>% \n  mutate(leakage_percetage = rescale(leakage/off_total_amount, to = c(0, 1)))\n\n\n# Time to Payment\n\niv_data <- iv_data %>% \n  mutate(\n    \n    week_1_ttp = ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                          round(rnorm(n_household, 0, 5)),\n    week_2_ttp = ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                          round(rnorm(n_household, 0, 5)),\n    week_3_ttp =ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                            round(rnorm(n_household, 0, 5)),\n    week_4_ttp = ifelse(smartcard == 1, round(rbeta(n_household, 6, 4) * 100),\n                           round(rbeta(n_household, 7, 3) * 100)) +\n                          round(rnorm(n_household, 0, 5)),\n    avg_ttp = (week_1_ttp + week_2_ttp + week_3_ttp + week_4_ttp)/ 4\n  )\n\niv_data <- iv_data%>% \n  mutate(\n    week_1_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    week_2_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    week_3_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    week_4_dbp = ifelse(smartcard == 1, round(rnorm(n_household, 10, 2)),\n                          round(rnorm(n_household, 13, 2))),\n    avg_dbp = (week_1_dbp+ week_2_dbp+ week_3_dbp+ week_4_dbp) / 4\n  )\n\n# Corruption\n\n# Did you pay bribe? Yes/No\n\niv_data <- iv_data %>% \n  mutate(\n    smartcard_bribe_effect = -0.2 * smartcard,\n    bribe_score = rbeta(n_household, 4, 6),\n    bribe_probability = scales::rescale(bribe_score + smartcard_bribe_effect + \n                                  rnorm(n_household, mean = 0, sd = 0.01), to = c(0, 1)),\n    week_1_bribe = rbinom(n_household, 1, bribe_probability),\n    week_2_bribe = rbinom(n_household, 1, bribe_probability),\n    week_3_bribe = rbinom(n_household, 1, bribe_probability),\n    week_4_bribe = rbinom(n_household, 1, bribe_probability),\n    bribe_incidence = (week_1_bribe + week_2_bribe +week_3_bribe + week_4_bribe)\n  )\n\n\niv_data_csv <- iv_data %>% \n  dplyr::select(household_ID, panchayat_ID, state_ID, promotion, num_members, farm, children, female_member, household_literacy_rate, hh_age, hh_male, hindu, muslim, other_religion,  off_week_1, off_week_2, off_week_3, off_week_4, off_total_amount, week_1, week_2, week_3, week_4, total_amount, leakage, week_1_ttp, week_2_ttp,week_3_ttp,week_4_ttp, avg_ttp, week_1_dbp,\n                week_2_dbp, week_3_dbp, week_4_dbp, avg_dbp,\n                week_1_bribe, week_2_bribe, week_3_bribe, week_4_bribe, bribe_incidence)\n\nwrite_csv(iv_data_csv, \"./data/smartcard_data.csv\")"
  },
  {
    "objectID": "projects/smartcard/index.html#instrument-validity",
    "href": "projects/smartcard/index.html#instrument-validity",
    "title": "PMAP 8521: Final Project",
    "section": "Instrument Validity",
    "text": "Instrument Validity\n\n\nCode\nrelevance_check <- lm(smartcard ~  promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                      data = iv_data)\nmodelsummary(list(\"First Stage IV\" = relevance_check),\n             stars = T)\n\n\n\n\n\n\nFirst Stage IV\n\n\n\n\n(Intercept)\n0.118***\n\n\n\n(0.008)\n\n\npromotionTRUE\n0.635***\n\n\n\n(0.002)\n\n\nfarm\n-0.007\n\n\n\n(0.004)\n\n\nchildren\n0.003*\n\n\n\n(0.001)\n\n\nhousehold_literacy_rate\n0.002\n\n\n\n(0.006)\n\n\nfemale_member\n0.003+\n\n\n\n(0.001)\n\n\nhindu\n0.004\n\n\n\n(0.003)\n\n\nhh_age\n0.002***\n\n\n\n(0.000)\n\n\nhh_male\n0.005\n\n\n\n(0.004)\n\n\nNum.Obs.\n100000\n\n\nR2\n0.408\n\n\nR2 Adj.\n0.408\n\n\nAIC\n92273.1\n\n\nBIC\n92368.2\n\n\nLog.Lik.\n-46126.530\n\n\nRMSE\n0.38\n\n\n\nNote: ^^ + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\nBased on the first stage model, we see that there is a strong relationship between instrument (randomized promotion) and smartcard enrollment. The randomized promotion is positively correlated with smartcard enrollment. Moreover, the joint F-statistic of the model is 8608 with a p-value less than 0.01. We can reject the hypothesis that the instrument is weak. We can conclude that the instrument is relevant. Since the promotion panchayats are selected through a randomized process, the instrument is not correlated with any observable or unobservable characteristics of households or location. The instrument satisfies exogeneity. Lastly, there is a clear causal mechanism between the randomized promotion and the outcomes of interest. A promotion can only cause leakages, wait times, and corruption to increase or decrease only if a household enrolls into the smartcard program after receiving the promotion. Hence, the instrument satisfies exclusion."
  },
  {
    "objectID": "projects/smartcard/index.html#sls-estimation",
    "href": "projects/smartcard/index.html#sls-estimation",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "2SLS Estimation",
    "text": "2SLS Estimation\n\n\nCode\nleakage_iv_c <- iv_robust(leakage ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male| \n                        promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                        data = iv_data,\n                        diagnostics = TRUE)\n\nttp_iv_c <- iv_robust(avg_ttp ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male\n                      | promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male , \n                      data = iv_data,\n                      diagnostics = TRUE)\n\ndbp_iv_c <- iv_robust(avg_dbp ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male| \n                        promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                      data = iv_data,\n                      diagnostics = TRUE)\n\nbribe_iv_c <- iv_robust(bribe_incidence ~ smartcard + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male| \n                          promotion+ farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                        data = iv_data,\n                        diagnostics = TRUE)\n\n\n\n\nCode\n# Robustness Checks\n\nendo_row <- tibble(\"Wu-Hausman Test\", leakage_iv_c$diagnostic_endogeneity_test[\"value\"], ttp_iv_c$diagnostic_endogeneity_test[\"value\"], dbp_iv_c$diagnostic_endogeneity_test[\"value\"], bribe_iv_c$diagnostic_endogeneity_test[\"value\"])\n\n\nrelevance_row <- tibble(\"Weak Instruments Test\", leakage_iv_c$diagnostic_first_stage_fstatistic[\"value\"], ttp_iv_c$diagnostic_first_stage_fstatistic[\"value\"], dbp_iv_c$diagnostic_first_stage_fstatistic[\"value\"], bribe_iv_c$diagnostic_first_stage_fstatistic[\"value\"])\n\ncolnames(relevance_row) <- c(\"Test\", \"a\", \"b\", \"c\", \"d\")\ncolnames(endo_row) <- c(\"Test\", \"a\", \"b\", \"c\", \"d\")\ndiagnostics_tibble <- bind_rows(data.frame(relevance_row), data.frame(endo_row))\n\n\n# Final Table with Robustness Checks\nmodelsummary(list(\"Leakage\" = leakage_iv_c, \"Wait Time (Queue)\" = ttp_iv_c, \n                  \"Payment Delay\" = dbp_iv_c, \"Corruption\" = bribe_iv_c),\n             stars = TRUE,\n             vcov = vcov,\n             metrics = \"all\",\n             add_rows = diagnostics_tibble,\n             title = \"Instrumental Variables Estimation\")\n\n\n\n\nInstrumental Variables Estimation\n \n  \n      \n    Leakage \n    Wait Time (Queue) \n    Payment Delay \n    Corruption \n  \n \n\n  \n    smartcard \n    −45.308*** \n    −10.123*** \n    −2.995*** \n    −0.725*** \n  \n  \n     \n    (9.303) \n    (0.075) \n    (0.010) \n    (0.011) \n  \n  \n    farm \n    −2.225 \n    0.038 \n    −0.012 \n    0.026* \n  \n  \n     \n    (9.970) \n    (0.081) \n    (0.011) \n    (0.011) \n  \n  \n    children \n    6.175* \n    −0.006 \n    −0.002 \n    0.001 \n  \n  \n     \n    (2.981) \n    (0.024) \n    (0.003) \n    (0.003) \n  \n  \n    household_literacy_rate \n    −15.608 \n    −0.054 \n    −0.023 \n    −0.007 \n  \n  \n     \n    (15.490) \n    (0.125) \n    (0.017) \n    (0.018) \n  \n  \n    female_member \n    1.402 \n    0.006 \n    0.004 \n    0.006 \n  \n  \n     \n    (3.432) \n    (0.028) \n    (0.004) \n    (0.004) \n  \n  \n    hindu \n    242.714*** \n    69.991*** \n    13.006*** \n    2.118*** \n  \n  \n     \n    (18.852) \n    (0.153) \n    (0.020) \n    (0.022) \n  \n  \n    other_religion \n    235.559*** \n    70.036*** \n    12.994*** \n    2.119*** \n  \n  \n     \n    (19.722) \n    (0.160) \n    (0.021) \n    (0.023) \n  \n  \n    hh_age \n    2.249*** \n    0.001 \n    0.000 \n    0.000 \n  \n  \n     \n    (0.325) \n    (0.003) \n    (0.000) \n    (0.000) \n  \n  \n    hh_male \n    20.088* \n    0.084 \n    0.002 \n    0.002 \n  \n  \n     \n    (9.869) \n    (0.080) \n    (0.011) \n    (0.011) \n  \n  \n    Num.Obs. \n    100000 \n    100000 \n    100000 \n    100000 \n  \n  \n    R2 \n    0.001 \n    0.307 \n    0.686 \n    0.103 \n  \n  \n    R2 Adj. \n    0.001 \n    0.307 \n    0.686 \n    0.103 \n  \n  \n    AIC \n    1651825.0 \n    688573.5 \n    285975.8 \n    298953.8 \n  \n  \n    BIC \n    1651920.1 \n    688668.6 \n    286071.0 \n    299048.9 \n  \n  \n    RMSE \n    934.57 \n    7.57 \n    1.01 \n    1.08 \n  \n  \n    Std.Errors \n    Custom \n    Custom \n    Custom \n    Custom \n  \n  \n    Weak Instruments Test \n    24846.035 \n    24846.035 \n    24846.035 \n    24846.035 \n  \n  \n    Wu-Hausman Test \n    1.489 \n    0.341 \n    0.268 \n    0.968 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\nWe can see that the smartcard intervention causes a decrease in leakages by 45.3 rupees for compliers. Moreover, wait time in queue is reduced by 10 minutes. Complier households received payment 3 days earlier, on average, as compared to non-compliers. Lastly, corruption decreased by 0.725 incidences. All estimates are statistically significant at 1% level. These results reinforce our expectations that the smartcard intervention would negatively affect leakage, wait times and corruption faced by the rural workers."
  },
  {
    "objectID": "projects/smartcard/index.html#robustness-checks",
    "href": "projects/smartcard/index.html#robustness-checks",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "Robustness Checks",
    "text": "Robustness Checks\n\nInstrument Validity\nBased on the first stage model, we see that there is a strong relationship between the instrument (randomized promotion) and Smartcard enrollment. The randomized promotion is positively correlated with Smartcard enrollment. Moreover, the joint F-statistic of the model is 8608 with a p-value less than 0.01. We can reject the hypothesis that the instrument is weak. We can conclude that the instrument is relevant. Since the promotion panchayats are selected through a randomized process, the instrument is not correlated with any observable or unobservable characteristics of households or location. The instrument satisfies exogeneity. Lastly, there is a clear causal mechanism between the randomized promotion and the outcomes of interest. A promotion can only cause leakages, wait times, and corruption to increase or decrease only if a household enrolls into the Smartcard program after receiving the promotion. Hence, the instrument satisfies exclusion.\n\n\nCode\nrelevance_check <- lm(smartcard ~  promotion + farm + children +\n                        household_literacy_rate + female_member + hindu + \n                        muslim + other_religion + hh_age + hh_male, \n                      data = iv_data)\nmodelsummary(list(\"First Stage IV\" = relevance_check),\n             stars = T)\n\n\n\n\n \n  \n      \n     First Stage IV \n  \n \n\n  \n    (Intercept) \n    0.118*** \n  \n  \n     \n    (0.008) \n  \n  \n    promotionTRUE \n    0.635*** \n  \n  \n     \n    (0.002) \n  \n  \n    farm \n    −0.007 \n  \n  \n     \n    (0.004) \n  \n  \n    children \n    0.003* \n  \n  \n     \n    (0.001) \n  \n  \n    household_literacy_rate \n    0.002 \n  \n  \n     \n    (0.006) \n  \n  \n    female_member \n    0.003+ \n  \n  \n     \n    (0.001) \n  \n  \n    hindu \n    0.004 \n  \n  \n     \n    (0.003) \n  \n  \n    hh_age \n    0.002*** \n  \n  \n     \n    (0.000) \n  \n  \n    hh_male \n    0.005 \n  \n  \n     \n    (0.004) \n  \n  \n    Num.Obs. \n    100000 \n  \n  \n    R2 \n    0.408 \n  \n  \n    R2 Adj. \n    0.408 \n  \n  \n    AIC \n    92273.1 \n  \n  \n    BIC \n    92368.2 \n  \n  \n    Log.Lik. \n    −46126.530 \n  \n  \n    RMSE \n    0.38 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001"
  },
  {
    "objectID": "projects/smartcard/index.html",
    "href": "projects/smartcard/index.html",
    "title": "Impact Evaulation of the Smartcard Program using Synthetic Data",
    "section": "",
    "text": "Developing countries allocate a significant portion of their annual budget to social assistance programs, but most of these funds fail to reach their intended recipients due to inefficiencies resulting from leakages and corruption. In 2016, India allocated $70 billion to social assistance programs (Govt. of India 2017), but historical estimates suggest that around 51% of these funds were lost due to inadequate delivery systems (Imbert, Clement, and John Papp. 2016; Govt of India 2005). One potential solution to improve assistance delivery is the implementation of digital payment systems that directly deposit government funds into the beneficiaries’ bank accounts, thereby reducing bureaucracy. To enhance the system’s security, biometric authentication can be employed.\nIn 2006, the state government of Andhra Pradesh (GoAP) introduced the Smartcard system, a large-scale biometric payments system, to enhance the delivery of two major social welfare programs: the National Rural Employment Guarantee Scheme (NREGS) and Social Security Pensions (SSP). NREGS guarantees 100 days of wage employment per year to every rural household, while SSP provides payments to elderly and disabled individuals below the poverty line. However, due to implementation issues, the program was initially shut down. In 2010, J-PAL South Asia partnered with GoAP and relaunched the program in eight districts of Andhra Pradesh. The objective was to evaluate the effectiveness of the Smartcard system by running a large-scale randomized evaluation involving 8,000 households. The program aimed to improve governance by reducing corruption and enhancing efficiency using biometric authentication and secure digital payment systems. This initiative increased access to governmental benefits for citizens, and reduced costs associated with manual processes and fraud.\nBuilding upon this previous work, our pre-registered study proposes to expand the biometric smartcard program introduced by Andhra Pradesh to three other Indian states using synthetic data. The expanded program aims to fully implement the biometric smartcard payment system in these states, specifically targeting the NREGS welfare program. Our plan involves equipping all panchayats (village councils) in the three states with payment technology and converting local Point of Service (PoS) locations into Smartcard facilities where all payments will be processed through the Smartcard system. If beneficiaries are not enrolled in the program, they will continue to receive payments in cash, similar to the previous system. Workers can enroll in the program, register their biometric information, and receive new Smartcards at designated stations. The PoS devices will store the biometric information of all enrolled workers locally, facilitating transaction verification without internet access.\nThe primary goal of this paper is to evaluate the impact of the Smartcard program on governance. Good governance ensures the efficient and equitable delivery of governmental processes, promoting public welfare. We anticipate a reduction in leakages, corruption, and wait times for the program’s beneficiaries. These improvements are expected to enhance overall efficiency, increase earnings, and foster greater trust in government welfare measures.\nTo evaluate the program’s impact on system leakages, wait times, and corruption, we employ an instrumental variable design. Our study involves randomly selecting 500 panchayats from the three states as a representative sample. Following program implementation, we randomly select 250 out of the 500 sample panchayats as the promotion locations. All households previously enrolled in the NREGS program will receive a promotion and information on enrolling in the Smartcard program. Subsequently, we conduct weekly surveys in all 500 panchayats over four weeks, gathering data on personal characteristics, leakages, wait times, and corruption events. By utilizing randomized promotion as a valid instrument, we find that the Smartcard intervention leads to a reduction in leakages, wait times and corruption.\n\n\nCode\n# import packages\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(fitdistrplus)\nlibrary(cascsim) #truncated gamma distribution\nlibrary(EnvStats) #truncated log-normal distribution\nlibrary(psych) \nlibrary(truncnorm)\nlibrary(scales)\nlibrary(magrittr)\nlibrary(estimatr)\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(modelsummary)\nlibrary(fixest)\nlibrary(lfe)"
  },
  {
    "objectID": "projects/garch-var/index.html#introduction",
    "href": "projects/garch-var/index.html#introduction",
    "title": "Balancing Risk and Reward: Copula-GARCH framework for Portfolio Optimiziation",
    "section": "Introduction",
    "text": "Introduction\nIn today’s volatile financial landscape, effective portfolio management has become an indispensable tool for mitigating investment risks that simply cannot be overlooked. The role of a portfolio manager is paramount, as they navigate this complex terrain with the overarching objective of minimizing risks while maximizing returns for their clients.\nThe goal of this project is to conduct a comprehensive analysis of two distinct financial instruments: the Direxion Daily S&P 500 Bull 3X Shares (SPXL) Exchange-Traded Fund (ETF) and Bank of America Corp (NYSE: BAC) stock. Our primary goal is to craft an optimal portfolio of these two assets that not only forecasts Value-at-Risk (VaR) but also actively reduces it.\nThe Direxion Daily S&P 500 Bull 3X Shares ETF, a 300% leveraged vehicle based on the performance of the S&P 500 index, offers 3x amplified returns as compared to the benchmark return on a daily basis. In contrast, Bank of America Corp (BAC), a steadfast and established financial institution, has a history of providing stable returns. The fusion of these two financial assets into an optimal portfolio promises a potential of handsome returns while managing VaR. We collect historical ten year daily OHLC data for both the instruments from Yahoo Finance using quantmod::getSymbols() in R.\nSubsequently, we perform the following analysis to construct an optimal portfolio:\n\nExploring the Data: We conduct a rigorous examination of the data to unveil any time-dependent patterns and potential volatility clustering, that provide insights into the behavior of these financial instruments.\nTime Series Modeling: Employing a \\(AR(1) + GARCH(1,1)\\) model, we seek to model the logarithmic returns of each product, providing us a deeper understanding of their inherent dynamics.\nCopula Analysis: We fit t-distributions to the standardized errors of both models. To study the dependence structure of both the residues, we explore various copulas such as the t-Copula, Gaussian Copula, Gumbel Copula, Clayton Copula, and Frank Copula. The selection of the most suitable copula is based on minimizing the Akaike Information Criterion (AIC).\nResidual Analysis: Rigorous evaluation of the standardized residues of our model is carried out, aiming to detect any potential serial correlation. This step involves the utilization of autocorrelation plots and weighted versions of the Ljung-Box test, ultimately confirming the appropriateness of the \\(AR(1) + GARCH(1,1)\\) model for our dataset.\nVaR Forecasting: Employing advanced numerical methods, we forecast Value-at-Risk for various portfolios comprising of SPXL and BAC. Our findings reveal that VaR is responsive to varying proportions of SPXL within the portfolio. Notably, we uncover that VaR tends to rise as the allocation to SPXL increases.\n\nIn essence, our project serves as a comprehensive exploration of the dynamic interplay between these financial assets, offering valuable insights and strategies for optimizing portfolio performance while diligently managing risks in an ever-evolving financial landscape.\n\n\nCode\n# Importing Packages\n\nlibrary(quantmod)\nlibrary(xts)\nlibrary(gridExtra)\nlibrary(MASS)\nlibrary(fGarch)\nlibrary(sn)\nlibrary(copula)\nlibrary(ks)\nlibrary(stargazer)\nlibrary(patchwork)\nlibrary(rugarch)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(tseries)\nlibrary(modelsummary)"
  },
  {
    "objectID": "projects/garch-var/index.html#time-series-graphs-and-distribution-plots",
    "href": "projects/garch-var/index.html#time-series-graphs-and-distribution-plots",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Time series graphs and distribution plots",
    "text": "Time series graphs and distribution plots\n\n\nCode\nplot1 <- ggplot(spxl_df, aes(x = Index, y = SPXL.Adjusted))+\n  geom_line(color = \"steelblue\") + \n  ggtitle(\"SPXL Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") +\n  theme_bw()\n\nplot2 <- ggplot(bac_df, aes(x = Index, y = BAC.Adjusted))+\n  geom_line(color = \"darkorange2\") + \n  ggtitle(\"BAC Daily Price\") +   \n  xlab(\"Date\") + \n  ylab(\"Price($)\") + \n  theme(plot.title = element_text(hjust = 0.5)) + \n  scale_x_date(date_labels = \"%y-%m\", date_breaks = \"1 year\") + \n  theme_bw()\n\nplot1 / plot2"
  },
  {
    "objectID": "projects/garch-var/index.html#ar1-garch11-model",
    "href": "projects/garch-var/index.html#ar1-garch11-model",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "AR(1)-GARCH(1,1) Model",
    "text": "AR(1)-GARCH(1,1) Model\nIn order to model autocorelation and volatility clustering, we combine an AR(1) model that has a nonconstant conditional mean but a constant conditional variance with a GARCH(1,1) model that has conditional mean and the variance of data depends on the past. Additionally, in a GARCH model, conditional standard deviation exhibits more persistent periods of low and high volatility.\nThe \\(AR(1) - GARCH(1,1)\\) is model is as follows:\n\\[ X_t = \\mu + \\phi X_{t-1} + e_t, \\quad e_t =  \\sigma_{t}\\varepsilon_{t}, \\quad \\sigma^2_t = \\omega + \\alpha e^2_{t-1} + \\beta \\sigma^2_{t-1} \\] \\[ \\tilde X_t = \\tilde \\mu + \\tilde \\phi \\tilde X_{t-1} + \\tilde e_t, \\quad \\tilde e_t =  \\tilde \\sigma_{t}\\tilde \\varepsilon_{t}, \\quad \\tilde \\sigma^2_t = \\tilde \\omega + \\tilde \\alpha \\tilde e^2_{t-1} + \\tilde \\beta \\tilde \\sigma^2_{t-1} \\]\nwhere, \\(X_t\\) is the daily log returns of SPXL and \\(\\tilde X_t\\) is the daily log returns of BAC.\n\n\nCode\nspxl <- as.vector(daily_df$SPXL_r)\nbac <- as.vector(daily_df$BAC_r)\n\n# Fitting an AR(1) + GARCH(1,1) Model\nspxl_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nspxl_fit <- ugarchfit(spxl_spec, data = spxl)\n\nbac_spec <- ugarchspec(mean.model=list(armaOrder=c(1,0)),\n                        variance.model=list(garchOrder=c(1,1)))\nbac_fit <- ugarchfit(bac_spec, data = bac)"
  },
  {
    "objectID": "projects/garch-var/index.html#residual-copula-modelling",
    "href": "projects/garch-var/index.html#residual-copula-modelling",
    "title": "Balancing Risk and Reward: Copula-GARCH Framework for Portfolio Optimiziation",
    "section": "Residual Copula Modelling",
    "text": "Residual Copula Modelling\nIn order to work with a portfolio of Direxion Daily S&P 500 Bull 3X Shares (SPXL) and Bank of America Corp (BAC), we need to model both the series as a copula to extract the dependence structure between the two financial instruments. In the project, we particulary study the dependent structure of the standardized residuals of an \\(AR(1) - GARCH(1,1)\\) model fitted to both the series.\nFirst, we fit a Student t distribution to both \\(\\varepsilon_t\\) and \\(\\tilde \\varepsilon_t\\) using the fitdistr() function from the MASS library.\n\n\nCode\n# Standardized Residuals\n\nspxl_res <- as.vector(residuals(spxl_fit, standardize = T))\nbac_res <- as.vector(residuals(bac_fit, standardize = T))\n\n# Fitting t-distribution\nspxl_res_t <- fitdistr(spxl_res, \"t\")\nbac_res_t <- fitdistr(bac_res, \"t\")\n\n\n\n\nCode\n# Fitting t-density plots\npar(mfrow = c(1,2))\nn <- length(spxl_res)\n\nx1 <- qt((1:n)/(n+1), df = 5)\nx2 <- qt((1:n)/(n+1), df = 5)\n\nqqplot(sort(spxl_res), x1, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"SPXL Residual t-plot, DF = 5\")\n\nqqplot(sort(bac_res), x2, xlab=\"Standardized Residuals\",\n       ylab=\"t-quantiles\",\n       main = \"BAC Residual t-plot, DF = 5\")\n\n\n\n\n\nA t-distribution with df = 5 fits well to the SPXL and BAC residuals. The qqplot for both the instruments is a straight line except for a few outliers. Given that the total number of observations (n = 2517) is high enough, the outliers are a small fraction of the sample.\nSecond, we transform the residuals into marginal t - distributions and fit t-copula, Gaussian copula, Gumbel Copula, Clayton and Frank copulas. We select the best fit copula by minimizing AIC. We select t-copula as it minimizes AIC.\n\n\nCode\n# Transform residues into uniform distribution\n\nspxl_uniform <- pt(spxl_res, df = spxl_res_t$estimate[3])\nbac_uniform <- pt(bac_res, df = bac_res_t$estimate[3])\n\nU_hat <- cbind(spxl_uniform, bac_uniform)\ncolnames(U_hat) <- c(\"SPXL_U\", \"BAC_U\")\n\ntau <- as.numeric(cor.test(spxl_uniform, bac_uniform, \n                           method=\"kendall\")$estimate)\nomega <- sin(tau * pi/2)\n\n\n\n\nCode\n# Fit t-Copula\nCt <- fitCopula(copula = tCopula(dim = 2), data = U_hat,\n                method = \"ml\", start = c(omega, 10))\n\n# Log-Likelihood\nlog_lik_t <- loglikCopula(param = Ct@estimate, u = U_hat, \n                          copula = tCopula(dim = 2))\naic_t <- (2 * length(Ct@estimate)) - (2 * abs(log_lik_t))\naic_t\n\n\n[1] -1516.131\n\n\nCode\n# Gaussian Copula\nCgauss <- fitCopula(copula = normalCopula(dim = 2), data = U_hat, \n                    method = \"ml\", start=c(omega))\nlog_lik_gauss <- loglikCopula(param = Cgauss@estimate, u = U_hat, \n                          copula = normalCopula(dim = 2))\n\naic_gauss <- (2 * length(Cgauss@estimate)) - (2 *abs(log_lik_gauss))\naic_gauss\n\n\n[1] -1411.64\n\n\nCode\n# Gumbel Copula\nC_gumbel <- fitCopula(copula = gumbelCopula(dim = 2), data =U_hat,\n                      method = \"ml\")\nlog_lik_gumbel <- loglikCopula(param = C_gumbel@estimate, u = U_hat,\n                               copula = gumbelCopula(dim = 2))\naic_gumbel <- (2 * length(C_gumbel@estimate)) -(2*abs(log_lik_gumbel))\naic_gumbel\n\n\n[1] -1388.308\n\n\nCode\n# Clayton Copula\n\nC_clayton <- fitCopula(copula = claytonCopula(dim = 2), data = U_hat,\n                      method = \"ml\")\nlog_lik_clayton <- loglikCopula(param = C_clayton@estimate, u = U_hat,\n                                copula = claytonCopula(dim = 2))\naic_clayton <- (2 * length(C_clayton@estimate)) -(2*abs(log_lik_clayton))\naic_clayton\n\n\n[1] -1213.358\n\n\nCode\n# Frank Copula\n\nCfrank <- fitCopula(copula = frankCopula(1, dim = 2), data = U_hat,\n                    method = \"ml\")\n\nlog_lik_frank <- loglikCopula(param = Cfrank@estimate, u = U_hat,\n                              copula = frankCopula(dim = 2))\naic_frank <- (2 * length(Cfrank@estimate)) - (2 *abs(log_lik_frank))\naic_frank\n\n\n[1] -1395.342"
  }
]